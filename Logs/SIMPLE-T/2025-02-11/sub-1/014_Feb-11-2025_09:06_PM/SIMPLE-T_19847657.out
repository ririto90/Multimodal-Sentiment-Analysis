SLURM Job ID: 19847657
Number of GPUs available: 1
Python PATH: ['/home/rgg2706/Multimodal-Sentiment-Analysis/Models/SIMPLE-T/src', '/home/rgg2706/Multimodal-Sentiment-Analysis', '/Models/SIMPLE-T/src', '/.autofs/tools/spack/var/spack/environments/default-nlp-x86_64-24072401/.spack-env/._view/wtxwc3mlkmzy7fbaxlum2674jarpitc2/lib/python311.zip', '/.autofs/tools/spack/var/spack/environments/default-nlp-x86_64-24072401/.spack-env/._view/wtxwc3mlkmzy7fbaxlum2674jarpitc2/lib/python3.11', '/.autofs/tools/spack/var/spack/environments/default-nlp-x86_64-24072401/.spack-env/._view/wtxwc3mlkmzy7fbaxlum2674jarpitc2/lib/python3.11/lib-dynload', '/.autofs/tools/spack/var/spack/environments/default-nlp-x86_64-24072401/.spack-env/._view/wtxwc3mlkmzy7fbaxlum2674jarpitc2/lib/python3.11/site-packages']
Logs directory: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/SIMPLE-T/2025-02-11/sub-1/014_Feb-11-2025_09:06_PM
> training arguments:
>>> rand_seed: 8
>>> model_fusion: simpletextatt
>>> dataset: mvsa-mts-v3
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> initializer: <function xavier_uniform_ at 0x7fe357011440>
>>> learning_rate: 0.001
>>> dropout_rate: 0.5
>>> weight_decay: 0.0
>>> num_layers: 3
>>> num_epoch: 100
>>> batch_size: 64
>>> log_step: 60
>>> max_seq_len: 20
>>> polarities_dim: 3
>>> clip_grad: 5.0
>>> path_image: ./images
>>> crop_size: 224
>>> n_head: 8
>>> hidden_dim: 256
>>> num_classes: 3
>>> log_dir: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/SIMPLE-T/2025-02-11/sub-1/014_Feb-11-2025_09:06_PM
>>> counter: 0
>>> model_class: <class 'models.simpletextatt.SimpleTextAtt'>
Loading dataset 'mvsa-mts-v3':
  Train path: Datasets/MVSA-MTS/mvsa-mts-v3/train.tsv
	Validation path: Datasets/MVSA-MTS/mvsa-mts-v3/val.tsv
  Test path: Datasets//MVSA-MTS/mvsa-mts-v3/test.tsv
loading word vectors...
building embedding_matrix: 200_glove_embedding_matrix.dat
-------------- Loading Datasets/MVSA-MTS/mvsa-mts-v3/train.tsv ---------------
[DEBUG] index: 706
[DEBUG] raw_text: PC Party, #Youth, #Education,Opportunity,#RenewableResources, Proudly #Canada's PCs #elxn42 http://t.co/NSwTddCHS8
[DEBUG] processed_str: pc party opportunity proudly pcs
[DEBUG] text_indices: [2 3 4 5 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[DEBUG] polarity: 2
[DEBUG] index: 2699
[DEBUG] raw_text: Running through the 6 wit my woes #ComeTogether #BlueJays #inthe6
[DEBUG] processed_str: running through the wit my woes
[DEBUG] text_indices: [ 7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
[DEBUG] polarity: 1
[DEBUG] index: 15657
[DEBUG] raw_text: #TruckTuesday | | support@innovativeautoworx.com | 403.242.2767 | #Trucks #YYC #Calgary | http://t.co/ruwEqCd3LT
[DEBUG] processed_str: support
[DEBUG] text_indices: [13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
[DEBUG] polarity: 0
[DEBUG] index: 13219
[DEBUG] raw_text: I dont even care how ridiculous this looks #OTRAToronto is officially tomorrow and I am more than ready @onedirection
[DEBUG] processed_str: i dont even care how ridiculous this looks is officially tomorrow and i am more than ready onedirection
[DEBUG] text_indices: [14 15 16 17 18 19 20 21 22 23 24 25 14 26 27 28 29 30  0  0]
[DEBUG] polarity: 0
Time taken to load Datasets/MVSA-MTS/mvsa-mts-v3/train.tsv: 2.76 seconds(0.05 minutes)
Train classes: [0, 1, 2], count=3
[DEBUG] Train label distribution:
{0: 3522, 1: 3468, 2: 6631}
-------------- Loading Datasets/MVSA-MTS/mvsa-mts-v3/val.tsv ---------------
[DEBUG] index: 18681
[DEBUG] raw_text: ***Steven thinking about the life he just left behind with his beloved, Sam. Should he have stayed?...to be continued
[DEBUG] processed_str: steven thinking about the life he just left behind with his beloved sam should he have stayed to be continued
[DEBUG] text_indices: [ 5180  1615   777     9   825   594   339  1227   652   143   220  9092
  5368   562   594   428 15160    90    50 14601]
[DEBUG] polarity: 2
[DEBUG] index: 16242
[DEBUG] raw_text: Thanks for an amazing summer #yyc,53 organizations engaged youth in 350 projects to contribute 20000 volunteer hours!
[DEBUG] processed_str: thanks for an amazing summer organizations engaged youth in projects to contribute volunteer hours
[DEBUG] text_indices: [  567    39   498    80   655 10273 14072  2198    42  3274    90 16735
  5282  1135     0     0     0     0     0     0]
[DEBUG] polarity: 2
[DEBUG] index: 9628
[DEBUG] raw_text: HSR fares go up on Tuesday. Tickets (new issue) are $2.15. Don't be overcharged! #HamOnt #HSR https://t.co/zBxyTmcy1o
[DEBUG] processed_str: hsr fares go up on tuesday tickets new issue are do be overcharged
[DEBUG] text_indices: [ 9340 14360   413   197    69   656   528    76  1129   582   489    50
 16736     0     0     0     0     0     0     0]
[DEBUG] polarity: 1
[DEBUG] index: 6350
[DEBUG] raw_text: @Calum5SOS just saw this on my Instagram feed and instantly thought of you #JetBlackHeart #ShesKindaHotVMA
[DEBUG] processed_str: just saw this on my instagram feed and instantly thought of you
[DEBUG] text_indices: [  339  1395    20    69    11  1661  3485    25 16017   385   102   283
     0     0     0     0     0     0     0     0]
[DEBUG] polarity: 0
Time taken to load Datasets/MVSA-MTS/mvsa-mts-v3/val.tsv: 0.35 seconds(0.01 minutes)
Val classes: [0, 1, 2], count=3
[DEBUG] Train label distribution:
{0: 436, 1: 442, 2: 825}
[DEBUG] Computed class_weights = [1.2891349792480469, 1.3092080354690552, 0.6847132444381714]
-------------- Loading Datasets//MVSA-MTS/mvsa-mts-v3/test.tsv ---------------
[DEBUG] index: 14949
[DEBUG] raw_text: Candid shot at #Montreal @FetishWeekend. #smile latex: @HWD_Latex #iLoveBiancaMondays http://t.co/eDaoHprlRP
[DEBUG] processed_str: candid shot at fetishweekend latex
[DEBUG] text_indices: [17897   297    75  7826 17898     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0]
[DEBUG] polarity: 2
[DEBUG] index: 9542
[DEBUG] raw_text: #hamont stop by and help out Lynwood Hall raise some founds with a car wash! #lynwoodhallcarwash #machealth
[DEBUG] processed_str: stop by and help out lynwood hall raise some founds with a car wash
[DEBUG] text_indices: [  394   243    25  1455    54 17899   723  2788    85 17900   143   139
   933  5787     0     0     0     0     0     0]
[DEBUG] polarity: 0
[DEBUG] index: 6309
[DEBUG] raw_text: EVEN MY NEICE WANTS TO VOTE #ShesKindaHotVMA
[DEBUG] processed_str: even my neice wants to vote
[DEBUG] text_indices: [   16    11 17901  3469    90   752     0     0     0     0     0     0
     0     0     0     0     0     0     0     0]
[DEBUG] polarity: 2
[DEBUG] index: 17974
[DEBUG] raw_text: Looks like I'm going alone ????
[DEBUG] processed_str: looks like i going alone
[DEBUG] text_indices: [  21  138   14  375 1954    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0]
[DEBUG] polarity: 2
Time taken to load Datasets//MVSA-MTS/mvsa-mts-v3/test.tsv: 0.35 seconds(0.01 minutes)
Test classes: [0, 1, 2], count=3
[DEBUG] Train label distribution:
{0: 450, 1: 431, 2: 822}
[DEBUG] 95th percentile sequence length across all splits: 17.00
Total Training Samples: 17027
Number of Training Samples: 13621
Number of Validation Samples: 1703
Number of Test Samples: 1703
Number of unique sentiment classes: 3
Building model
1
n_trainable_params: 7539971, n_nontrainable_params: 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
[DEBUG] text_indices.shape: torch.Size([64, 20])
[DEBUG] embedded_text.shape: torch.Size([64, 20, 200])
[DEBUG] lstm_output.shape: torch.Size([64, 20, 1536])
[DEBUG] h_n.shape: torch.Size([6, 64, 768])
[DEBUG] c_n.shape: torch.Size([6, 64, 768])
[DEBUG] Sample predictions in evaluate:  tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')
[DEBUG] outputs.shape: torch.Size([64, 3])
[DEBUG] Sample of raw logits (first 5):
tensor([[-1.6029, -0.0272, -2.2284],
        [-1.5216, -0.3506, -0.4008],
        [-1.9400,  0.1780, -0.7932],
        [-0.6588,  0.6627, -1.9626],
        [-0.5506, -1.9561, -1.1064]], device='cuda:0',
       grad_fn=<SliceBackward0>)
[DEBUG] Sample of predicted probabilities (first 5):
tensor([[0.1570, 0.7590, 0.0840],
        [0.1371, 0.4423, 0.4206],
        [0.0802, 0.6672, 0.2526],
        [0.1992, 0.7467, 0.0541],
        [0.5498, 0.1348, 0.3154]], device='cuda:0', grad_fn=<SliceBackward0>)
Batch 0 completed in 0.57 seconds (0.01 minutes)
New best val_f1: 0.247163 (previous best: 0.000000)
loss: 1.230950, val_acc: 47.74% (0.477393), val_f1: 24.72% (0.247163), test_acc: 47.56% (0.475631), test_f1: 24.98% (0.249763)
Batch 60 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.353547 (previous best: 0.247163)
loss: 1.140229, val_acc: 42.45% (0.424545), val_f1: 35.35% (0.353547), test_acc: 43.92% (0.439225), test_f1: 35.86% (0.358629)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.097333, val_acc: 47.21% (0.472108), val_f1: 26.48% (0.264848), test_acc: 47.39% (0.473870), test_f1: 26.97% (0.269660)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.136278, val_acc: 28.71% (0.287140), val_f1: 18.96% (0.189647), test_acc: 28.30% (0.283030), test_f1: 18.05% (0.180540)
Epoch 0 completed in 4.88 seconds (0.08 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.853492, val_acc: 30.89% (0.308867), val_f1: 22.41% (0.224073), test_acc: 31.59% (0.315913), test_f1: 22.98% (0.229766)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.084513, val_acc: 35.47% (0.354668), val_f1: 34.80% (0.347973), test_acc: 37.17% (0.371697), test_f1: 36.38% (0.363768)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.071823, val_acc: 48.44% (0.484439), val_f1: 21.76% (0.217563), test_acc: 48.27% (0.482678), test_f1: 21.70% (0.217030)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.081605, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Epoch 1 completed in 4.23 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.861680, val_acc: 34.35% (0.343511), val_f1: 31.31% (0.313139), test_acc: 34.59% (0.345860), test_f1: 31.76% (0.317555)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.127190, val_acc: 33.94% (0.339401), val_f1: 25.75% (0.257531), test_acc: 33.76% (0.337639), test_f1: 25.49% (0.254912)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.115299, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.089268, val_acc: 48.62% (0.486201), val_f1: 30.55% (0.305477), test_acc: 49.79% (0.497945), test_f1: 31.35% (0.313481)
Epoch 2 completed in 4.23 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.723585, val_acc: 48.44% (0.484439), val_f1: 21.76% (0.217563), test_acc: 48.27% (0.482678), test_f1: 21.70% (0.217030)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.069285, val_acc: 39.46% (0.394598), val_f1: 32.26% (0.322601), test_acc: 42.40% (0.423958), test_f1: 34.10% (0.340973)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.067102, val_acc: 29.83% (0.298297), val_f1: 23.04% (0.230436), test_acc: 30.71% (0.307105), test_f1: 23.74% (0.237377)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.104779, val_acc: 25.95% (0.259542), val_f1: 14.84% (0.148402), test_acc: 26.72% (0.267176), test_f1: 15.43% (0.154281)
Epoch 3 completed in 4.23 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.209911, val_acc: 42.63% (0.426307), val_f1: 31.82% (0.318190), test_acc: 45.16% (0.451556), test_f1: 33.61% (0.336144)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.092429, val_acc: 48.44% (0.484439), val_f1: 28.41% (0.284109), test_acc: 50.44% (0.504404), test_f1: 30.74% (0.307432)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.094066, val_acc: 44.16% (0.441574), val_f1: 33.05% (0.330468), test_acc: 45.74% (0.457428), test_f1: 34.06% (0.340586)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.324588, val_acc: 29.83% (0.298297), val_f1: 26.26% (0.262583), test_acc: 29.59% (0.295948), test_f1: 25.69% (0.256948)
Epoch 4 completed in 4.24 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.243601, val_acc: 46.27% (0.462713), val_f1: 27.09% (0.270900), test_acc: 46.21% (0.462126), test_f1: 27.24% (0.272416)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.095135, val_acc: 29.36% (0.293600), val_f1: 19.42% (0.194248), test_acc: 29.48% (0.294774), test_f1: 20.08% (0.200822)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.067228, val_acc: 29.13% (0.291251), val_f1: 18.72% (0.187152), test_acc: 30.89% (0.308867), test_f1: 20.39% (0.203909)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.100373, val_acc: 30.59% (0.305931), val_f1: 20.70% (0.206952), test_acc: 33.06% (0.330593), test_f1: 23.10% (0.230983)
Epoch 5 completed in 4.24 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
Batch 0 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.363350 (previous best: 0.353547)
loss: 2.847638, val_acc: 40.40% (0.403993), val_f1: 36.33% (0.363350), test_acc: 42.10% (0.421022), test_f1: 38.03% (0.380289)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.092922, val_acc: 31.59% (0.315913), val_f1: 21.97% (0.219650), test_acc: 33.06% (0.330593), test_f1: 23.29% (0.232853)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.064798, val_acc: 38.76% (0.387551), val_f1: 29.61% (0.296131), test_acc: 40.75% (0.407516), test_f1: 31.17% (0.311688)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.035941, val_acc: 46.21% (0.462126), val_f1: 30.51% (0.305149), test_acc: 47.74% (0.477393), test_f1: 32.19% (0.321903)
Epoch 6 completed in 4.24 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.219967, val_acc: 39.93% (0.399295), val_f1: 28.21% (0.282102), test_acc: 39.22% (0.392249), test_f1: 29.91% (0.299101)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.060322, val_acc: 26.37% (0.263652), val_f1: 14.73% (0.147271), test_acc: 27.01% (0.270112), test_f1: 14.81% (0.148115)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.079677, val_acc: 44.69% (0.446858), val_f1: 34.38% (0.343828), test_acc: 46.62% (0.466236), test_f1: 35.48% (0.354816)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.088964, val_acc: 41.69% (0.416911), val_f1: 32.04% (0.320431), test_acc: 42.98% (0.429830), test_f1: 32.87% (0.328743)
Epoch 7 completed in 4.25 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.461700, val_acc: 46.56% (0.465649), val_f1: 35.09% (0.350876), test_acc: 48.85% (0.488550), test_f1: 37.12% (0.371246)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.060888, val_acc: 42.28% (0.422783), val_f1: 32.47% (0.324748), test_acc: 44.27% (0.442748), test_f1: 33.89% (0.338885)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.998855, val_acc: 43.63% (0.436289), val_f1: 33.54% (0.335364), test_acc: 45.16% (0.451556), test_f1: 34.37% (0.343724)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.111211, val_acc: 41.81% (0.418086), val_f1: 35.96% (0.359553), test_acc: 44.74% (0.447446), test_f1: 38.76% (0.387556)
Epoch 8 completed in 4.24 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.558641, val_acc: 38.11% (0.381092), val_f1: 31.62% (0.316170), test_acc: 40.11% (0.401057), test_f1: 33.23% (0.332273)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.077582, val_acc: 44.92% (0.449207), val_f1: 34.11% (0.341131), test_acc: 47.03% (0.470346), test_f1: 35.57% (0.355710)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.067761, val_acc: 41.46% (0.414563), val_f1: 35.02% (0.350188), test_acc: 42.75% (0.427481), test_f1: 35.89% (0.358938)
Batch 180 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.378939 (previous best: 0.363350)
loss: 1.033582, val_acc: 45.21% (0.452143), val_f1: 37.89% (0.378939), test_acc: 46.39% (0.463887), test_f1: 38.73% (0.387252)
Epoch 9 completed in 4.25 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.450468, val_acc: 49.44% (0.494422), val_f1: 29.91% (0.299121), test_acc: 50.15% (0.501468), test_f1: 31.09% (0.310900)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.043574, val_acc: 39.40% (0.394011), val_f1: 36.02% (0.360234), test_acc: 39.22% (0.392249), test_f1: 36.70% (0.366982)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.052079, val_acc: 47.21% (0.472108), val_f1: 36.14% (0.361407), test_acc: 49.32% (0.493247), test_f1: 37.84% (0.378376)
Batch 180 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.382102 (previous best: 0.378939)
loss: 1.062765, val_acc: 46.27% (0.462713), val_f1: 38.21% (0.382102), test_acc: 48.39% (0.483852), test_f1: 39.69% (0.396889)
Epoch 10 completed in 4.25 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.486449, val_acc: 43.75% (0.437463), val_f1: 37.93% (0.379341), test_acc: 45.51% (0.455079), test_f1: 38.92% (0.389203)
Batch 60 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.394947 (previous best: 0.382102)
loss: 1.055554, val_acc: 44.27% (0.442748), val_f1: 39.49% (0.394947), test_acc: 46.21% (0.462126), test_f1: 41.28% (0.412769)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.012387, val_acc: 45.98% (0.459777), val_f1: 37.25% (0.372481), test_acc: 47.80% (0.477980), test_f1: 39.15% (0.391508)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.055231, val_acc: 45.33% (0.453318), val_f1: 39.33% (0.393294), test_acc: 47.68% (0.476806), test_f1: 41.56% (0.415630)
Epoch 11 completed in 4.26 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
Batch 0 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.417270 (previous best: 0.394947)
loss: 1.515837, val_acc: 44.57% (0.445684), val_f1: 41.73% (0.417270), test_acc: 45.39% (0.453905), test_f1: 42.07% (0.420691)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.063219, val_acc: 45.21% (0.452143), val_f1: 38.75% (0.387463), test_acc: 47.15% (0.471521), test_f1: 40.20% (0.401950)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.043072, val_acc: 39.40% (0.394011), val_f1: 39.33% (0.393318), test_acc: 40.58% (0.405755), test_f1: 40.38% (0.403791)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.083411, val_acc: 46.15% (0.461538), val_f1: 38.47% (0.384663), test_acc: 47.62% (0.476218), test_f1: 39.19% (0.391910)
Epoch 12 completed in 4.26 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 13
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.369891, val_acc: 42.63% (0.426307), val_f1: 36.02% (0.360158), test_acc: 42.69% (0.426894), test_f1: 35.16% (0.351621)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.049890, val_acc: 39.34% (0.393423), val_f1: 39.34% (0.393356), test_acc: 41.75% (0.417499), test_f1: 41.94% (0.419429)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.135568, val_acc: 41.69% (0.416911), val_f1: 40.58% (0.405816), test_acc: 44.04% (0.440399), test_f1: 43.09% (0.430938)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.089284, val_acc: 27.77% (0.277745), val_f1: 23.24% (0.232384), test_acc: 27.77% (0.277745), test_f1: 23.70% (0.237037)
Epoch 13 completed in 4.26 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 14
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.131493, val_acc: 30.06% (0.300646), val_f1: 26.32% (0.263157), test_acc: 30.42% (0.304169), test_f1: 26.63% (0.266292)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.020573, val_acc: 46.39% (0.463887), val_f1: 37.58% (0.375801), test_acc: 49.27% (0.492660), test_f1: 40.83% (0.408308)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.027843, val_acc: 44.86% (0.448620), val_f1: 40.87% (0.408739), test_acc: 46.51% (0.465062), test_f1: 42.61% (0.426094)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.055920, val_acc: 47.56% (0.475631), val_f1: 36.83% (0.368312), test_acc: 50.26% (0.502642), test_f1: 38.68% (0.386775)
Epoch 14 completed in 4.27 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 15
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.489575, val_acc: 39.99% (0.399883), val_f1: 33.79% (0.337856), test_acc: 41.81% (0.418086), test_f1: 35.20% (0.352029)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.127348, val_acc: 46.98% (0.469759), val_f1: 39.73% (0.397295), test_acc: 48.44% (0.484439), test_f1: 41.51% (0.415073)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.040208, val_acc: 32.53% (0.325308), val_f1: 26.34% (0.263435), test_acc: 34.88% (0.348796), test_f1: 28.30% (0.283008)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.057644, val_acc: 41.87% (0.418673), val_f1: 40.49% (0.404888), test_acc: 44.10% (0.440986), test_f1: 43.05% (0.430496)
Epoch 15 completed in 4.27 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 16
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.696178, val_acc: 39.64% (0.396359), val_f1: 38.68% (0.386801), test_acc: 41.98% (0.419847), test_f1: 40.89% (0.408937)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.025343, val_acc: 36.47% (0.364651), val_f1: 31.61% (0.316136), test_acc: 39.22% (0.392249), test_f1: 34.37% (0.343744)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.036744, val_acc: 46.45% (0.464474), val_f1: 40.21% (0.402144), test_acc: 48.80% (0.487962), test_f1: 42.62% (0.426180)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.044574, val_acc: 43.16% (0.431591), val_f1: 40.38% (0.403750), test_acc: 46.21% (0.462126), test_f1: 43.15% (0.431480)
Epoch 16 completed in 4.27 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 17
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.436891, val_acc: 45.57% (0.455666), val_f1: 41.34% (0.413368), test_acc: 47.45% (0.474457), test_f1: 43.15% (0.431531)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.017639, val_acc: 46.27% (0.462713), val_f1: 40.32% (0.403156), test_acc: 47.80% (0.477980), test_f1: 41.49% (0.414929)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.000737, val_acc: 42.69% (0.426894), val_f1: 41.40% (0.414013), test_acc: 43.63% (0.436289), test_f1: 41.80% (0.418049)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.012071, val_acc: 41.05% (0.410452), val_f1: 38.99% (0.389899), test_acc: 43.28% (0.432766), test_f1: 41.19% (0.411877)
Epoch 17 completed in 4.28 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 18
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.171950, val_acc: 38.64% (0.386377), val_f1: 38.25% (0.382472), test_acc: 40.58% (0.405755), test_f1: 40.18% (0.401785)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.047558, val_acc: 43.57% (0.435702), val_f1: 41.71% (0.417126), test_acc: 45.39% (0.453905), test_f1: 43.63% (0.436325)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.084314, val_acc: 38.29% (0.382854), val_f1: 34.06% (0.340634), test_acc: 39.93% (0.399295), test_f1: 35.12% (0.351194)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.998320, val_acc: 43.63% (0.436289), val_f1: 41.22% (0.412195), test_acc: 45.21% (0.452143), test_f1: 42.45% (0.424510)
Epoch 18 completed in 4.28 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 19
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.042724, val_acc: 37.35% (0.373459), val_f1: 37.16% (0.371605), test_acc: 39.40% (0.394011), test_f1: 39.62% (0.396181)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.059545, val_acc: 40.87% (0.408691), val_f1: 40.25% (0.402512), test_acc: 41.98% (0.419847), test_f1: 41.27% (0.412715)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.067487, val_acc: 40.81% (0.408103), val_f1: 39.01% (0.390128), test_acc: 42.98% (0.429830), test_f1: 41.31% (0.413113)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.095569, val_acc: 42.34% (0.423371), val_f1: 39.16% (0.391573), test_acc: 45.57% (0.455666), test_f1: 42.50% (0.424978)
Epoch 19 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 20
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.985436, val_acc: 43.28% (0.432766), val_f1: 40.87% (0.408651), test_acc: 46.56% (0.465649), test_f1: 43.91% (0.439079)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.043802, val_acc: 42.34% (0.423371), val_f1: 40.48% (0.404802), test_acc: 46.21% (0.462126), test_f1: 44.22% (0.442183)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.054470, val_acc: 41.46% (0.414563), val_f1: 36.47% (0.364662), test_acc: 44.69% (0.446858), test_f1: 39.85% (0.398474)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.021667, val_acc: 41.93% (0.419260), val_f1: 38.17% (0.381658), test_acc: 44.51% (0.445097), test_f1: 40.24% (0.402374)
Epoch 20 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 21
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.165961, val_acc: 43.28% (0.432766), val_f1: 38.57% (0.385701), test_acc: 46.15% (0.461538), test_f1: 41.65% (0.416518)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.007540, val_acc: 38.87% (0.388726), val_f1: 36.24% (0.362372), test_acc: 39.93% (0.399295), test_f1: 36.84% (0.368353)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.022400, val_acc: 42.04% (0.420435), val_f1: 40.05% (0.400459), test_acc: 43.57% (0.435702), test_f1: 41.58% (0.415751)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.094461, val_acc: 47.15% (0.471521), val_f1: 38.94% (0.389430), test_acc: 49.32% (0.493247), test_f1: 42.10% (0.421030)
Epoch 21 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 22
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.179120, val_acc: 43.39% (0.433940), val_f1: 37.41% (0.374098), test_acc: 45.63% (0.456254), test_f1: 39.27% (0.392739)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.126816, val_acc: 41.93% (0.419260), val_f1: 38.56% (0.385606), test_acc: 43.81% (0.438050), test_f1: 39.91% (0.399060)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.002502, val_acc: 42.22% (0.422196), val_f1: 40.96% (0.409558), test_acc: 45.86% (0.458602), test_f1: 44.23% (0.442297)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.051149, val_acc: 38.58% (0.385790), val_f1: 38.17% (0.381676), test_acc: 39.40% (0.394011), test_f1: 38.85% (0.388544)
Epoch 22 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 23
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.261059, val_acc: 43.22% (0.432179), val_f1: 40.25% (0.402478), test_acc: 44.33% (0.443335), test_f1: 41.10% (0.410980)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.053849, val_acc: 46.86% (0.468585), val_f1: 40.24% (0.402434), test_acc: 48.33% (0.483265), test_f1: 42.35% (0.423542)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.106004, val_acc: 43.22% (0.432179), val_f1: 39.59% (0.395908), test_acc: 45.16% (0.451556), test_f1: 41.19% (0.411898)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.062150, val_acc: 36.41% (0.364063), val_f1: 31.48% (0.314807), test_acc: 38.46% (0.384615), test_f1: 32.92% (0.329179)
Epoch 23 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 24
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.997645, val_acc: 38.64% (0.386377), val_f1: 36.33% (0.363283), test_acc: 39.93% (0.399295), test_f1: 37.12% (0.371210)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.976470, val_acc: 43.86% (0.438638), val_f1: 39.92% (0.399233), test_acc: 46.45% (0.464474), test_f1: 42.34% (0.423366)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.039370, val_acc: 40.28% (0.402819), val_f1: 38.89% (0.388870), test_acc: 41.81% (0.418086), test_f1: 40.30% (0.403012)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.043965, val_acc: 37.35% (0.373459), val_f1: 35.51% (0.355072), test_acc: 40.34% (0.403406), test_f1: 38.18% (0.381810)
Epoch 24 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 25
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.959299, val_acc: 45.68% (0.456841), val_f1: 41.43% (0.414310), test_acc: 48.56% (0.485614), test_f1: 44.44% (0.444395)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.065421, val_acc: 39.22% (0.392249), val_f1: 39.02% (0.390205), test_acc: 38.81% (0.388139), test_f1: 38.55% (0.385476)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.126347, val_acc: 42.45% (0.424545), val_f1: 40.95% (0.409545), test_acc: 43.34% (0.433353), test_f1: 41.25% (0.412486)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.050461, val_acc: 44.92% (0.449207), val_f1: 40.51% (0.405065), test_acc: 47.62% (0.476218), test_f1: 43.15% (0.431471)
Epoch 25 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 26
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.218371, val_acc: 40.69% (0.406929), val_f1: 38.79% (0.387942), test_acc: 41.28% (0.412801), test_f1: 38.97% (0.389746)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.995512, val_acc: 40.87% (0.408691), val_f1: 36.84% (0.368444), test_acc: 41.75% (0.417499), test_f1: 36.70% (0.367044)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.023531, val_acc: 40.87% (0.408691), val_f1: 36.65% (0.366455), test_acc: 43.98% (0.439812), test_f1: 39.18% (0.391842)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.056396, val_acc: 44.04% (0.440399), val_f1: 41.23% (0.412342), test_acc: 47.15% (0.471521), test_f1: 44.33% (0.443288)
Epoch 26 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 27
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.113591, val_acc: 45.98% (0.459777), val_f1: 40.95% (0.409486), test_acc: 48.50% (0.485026), test_f1: 43.84% (0.438380)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.086222, val_acc: 44.39% (0.443922), val_f1: 40.79% (0.407918), test_acc: 47.80% (0.477980), test_f1: 44.48% (0.444783)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.065646, val_acc: 26.25% (0.262478), val_f1: 17.18% (0.171817), test_acc: 27.60% (0.275984), test_f1: 18.19% (0.181892)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.114057, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Epoch 27 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 28
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.285517, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.109814, val_acc: 43.81% (0.438050), val_f1: 36.20% (0.362010), test_acc: 43.04% (0.430417), test_f1: 35.35% (0.353491)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.026143, val_acc: 39.64% (0.396359), val_f1: 36.02% (0.360218), test_acc: 41.40% (0.413975), test_f1: 37.12% (0.371241)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.086647, val_acc: 43.75% (0.437463), val_f1: 41.02% (0.410177), test_acc: 45.21% (0.452143), test_f1: 41.83% (0.418300)
Epoch 28 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 29
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.505852, val_acc: 37.40% (0.374046), val_f1: 33.75% (0.337501), test_acc: 39.93% (0.399295), test_f1: 35.86% (0.358616)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.005893, val_acc: 46.68% (0.466823), val_f1: 35.19% (0.351932), test_acc: 48.09% (0.480916), test_f1: 36.20% (0.362000)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.094915, val_acc: 26.01% (0.260129), val_f1: 13.82% (0.138245), test_acc: 25.31% (0.253083), test_f1: 13.53% (0.135268)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.104100, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Epoch 29 completed in 4.28 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 30
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.377527, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.25% (0.252496), test_f1: 13.44% (0.134396)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.081824, val_acc: 40.16% (0.401644), val_f1: 30.70% (0.306967), test_acc: 40.11% (0.401057), test_f1: 30.66% (0.306649)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.098792, val_acc: 29.59% (0.295948), val_f1: 19.55% (0.195493), test_acc: 29.77% (0.297710), test_f1: 20.13% (0.201345)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.082143, val_acc: 31.24% (0.312390), val_f1: 21.70% (0.217015), test_acc: 31.53% (0.315326), test_f1: 22.54% (0.225427)
Epoch 30 completed in 4.27 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 31
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.050920, val_acc: 28.07% (0.280681), val_f1: 17.66% (0.176579), test_acc: 28.83% (0.288315), test_f1: 17.86% (0.178602)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.097550, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.103503, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.100092, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Epoch 31 completed in 4.28 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 32
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.377212, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.098076, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.098943, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.25% (0.252496), test_f1: 13.44% (0.134396)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.098697, val_acc: 25.66% (0.256606), val_f1: 13.68% (0.136760), test_acc: 26.60% (0.266001), test_f1: 14.20% (0.142021)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Epoch 32 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 33
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.047808, val_acc: 48.44% (0.484439), val_f1: 21.76% (0.217563), test_acc: 48.27% (0.482678), test_f1: 21.70% (0.217030)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.098853, val_acc: 26.83% (0.268350), val_f1: 15.40% (0.153967), test_acc: 27.48% (0.274809), test_f1: 15.56% (0.155571)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.029187, val_acc: 48.03% (0.480329), val_f1: 34.70% (0.347015), test_acc: 50.15% (0.501468), test_f1: 36.87% (0.368723)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.020040, val_acc: 45.45% (0.454492), val_f1: 35.35% (0.353457), test_acc: 47.33% (0.473282), test_f1: 36.59% (0.365866)
Epoch 33 completed in 4.28 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 34
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.166065, val_acc: 31.06% (0.310628), val_f1: 22.84% (0.228397), test_acc: 33.35% (0.333529), test_f1: 25.10% (0.250966)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.091154, val_acc: 46.21% (0.462126), val_f1: 39.41% (0.394060), test_acc: 47.33% (0.473282), test_f1: 40.38% (0.403768)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.042805, val_acc: 45.92% (0.459190), val_f1: 39.86% (0.398645), test_acc: 47.21% (0.472108), test_f1: 40.66% (0.406598)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.036679, val_acc: 39.69% (0.396947), val_f1: 31.83% (0.318327), test_acc: 41.51% (0.415150), test_f1: 32.78% (0.327784)
Epoch 34 completed in 4.28 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 35
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.047864, val_acc: 47.86% (0.478567), val_f1: 37.67% (0.376666), test_acc: 48.68% (0.486788), test_f1: 38.88% (0.388752)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.020127, val_acc: 43.92% (0.439225), val_f1: 36.42% (0.364160), test_acc: 45.45% (0.454492), test_f1: 37.79% (0.377883)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.068020, val_acc: 43.45% (0.434527), val_f1: 33.92% (0.339195), test_acc: 45.80% (0.458015), test_f1: 36.60% (0.365975)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.997191, val_acc: 40.28% (0.402819), val_f1: 33.98% (0.339797), test_acc: 43.04% (0.430417), test_f1: 36.04% (0.360416)
Epoch 35 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 36
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.083279, val_acc: 46.21% (0.462126), val_f1: 41.67% (0.416710), test_acc: 47.74% (0.477393), test_f1: 42.92% (0.429206)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.018922, val_acc: 42.16% (0.421609), val_f1: 35.39% (0.353906), test_acc: 45.33% (0.453318), test_f1: 37.50% (0.375002)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.028870, val_acc: 38.52% (0.385203), val_f1: 34.40% (0.343994), test_acc: 40.34% (0.403406), test_f1: 35.48% (0.354809)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.081910, val_acc: 38.46% (0.384615), val_f1: 34.31% (0.343111), test_acc: 40.11% (0.401057), test_f1: 34.75% (0.347539)
Epoch 36 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 37
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.075699, val_acc: 46.39% (0.463887), val_f1: 40.34% (0.403394), test_acc: 47.86% (0.478567), test_f1: 42.06% (0.420621)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.025992, val_acc: 42.81% (0.428068), val_f1: 35.09% (0.350942), test_acc: 44.39% (0.443922), test_f1: 35.91% (0.359095)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.028829, val_acc: 43.69% (0.436876), val_f1: 39.86% (0.398618), test_acc: 44.57% (0.445684), test_f1: 39.50% (0.395041)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.040788, val_acc: 41.16% (0.411627), val_f1: 37.78% (0.377756), test_acc: 43.39% (0.433940), test_f1: 39.22% (0.392219)
Epoch 37 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 38
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.113128, val_acc: 35.29% (0.352907), val_f1: 30.42% (0.304224), test_acc: 37.87% (0.378743), test_f1: 32.88% (0.328813)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.098403, val_acc: 44.51% (0.445097), val_f1: 41.35% (0.413514), test_acc: 46.15% (0.461538), test_f1: 41.56% (0.415569)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.013443, val_acc: 44.16% (0.441574), val_f1: 40.44% (0.404387), test_acc: 45.57% (0.455666), test_f1: 40.66% (0.406567)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.970301, val_acc: 45.57% (0.455666), val_f1: 40.19% (0.401864), test_acc: 47.27% (0.472695), test_f1: 41.41% (0.414065)
Epoch 38 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 39
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.050361, val_acc: 43.69% (0.436876), val_f1: 38.95% (0.389473), test_acc: 45.86% (0.458602), test_f1: 40.14% (0.401414)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.051406, val_acc: 38.81% (0.388139), val_f1: 34.81% (0.348055), test_acc: 41.63% (0.416324), test_f1: 36.95% (0.369500)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.998437, val_acc: 41.05% (0.410452), val_f1: 37.04% (0.370390), test_acc: 43.57% (0.435702), test_f1: 39.12% (0.391242)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.157677, val_acc: 44.33% (0.443335), val_f1: 40.73% (0.407289), test_acc: 45.45% (0.454492), test_f1: 40.89% (0.408856)
Epoch 39 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 40
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.080317, val_acc: 43.86% (0.438638), val_f1: 41.24% (0.412393), test_acc: 46.74% (0.467410), test_f1: 43.97% (0.439744)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.063497, val_acc: 43.04% (0.430417), val_f1: 41.05% (0.410452), test_acc: 44.27% (0.442748), test_f1: 41.47% (0.414709)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.042741, val_acc: 41.87% (0.418673), val_f1: 38.60% (0.386012), test_acc: 43.98% (0.439812), test_f1: 39.97% (0.399746)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.033343, val_acc: 46.62% (0.466236), val_f1: 40.41% (0.404096), test_acc: 49.09% (0.490898), test_f1: 43.81% (0.438098)
Epoch 40 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 41
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.079036, val_acc: 43.10% (0.431004), val_f1: 41.18% (0.411831), test_acc: 43.22% (0.432179), test_f1: 40.46% (0.404576)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.026994, val_acc: 46.74% (0.467410), val_f1: 35.15% (0.351494), test_acc: 47.56% (0.475631), test_f1: 35.62% (0.356237)
Batch 120 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.420158 (previous best: 0.417270)
loss: 1.012265, val_acc: 43.98% (0.439812), val_f1: 42.02% (0.420158), test_acc: 45.04% (0.450382), test_f1: 42.21% (0.422120)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.059476, val_acc: 36.70% (0.366999), val_f1: 34.21% (0.342074), test_acc: 40.40% (0.403993), test_f1: 37.76% (0.377591)
Epoch 41 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 42
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.000031, val_acc: 40.87% (0.408691), val_f1: 37.34% (0.373384), test_acc: 43.10% (0.431004), test_f1: 38.74% (0.387407)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.058686, val_acc: 45.10% (0.450969), val_f1: 40.85% (0.408501), test_acc: 47.74% (0.477393), test_f1: 43.64% (0.436403)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.040282, val_acc: 45.74% (0.457428), val_f1: 39.97% (0.399672), test_acc: 48.21% (0.482090), test_f1: 43.07% (0.430711)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.010651, val_acc: 44.39% (0.443922), val_f1: 41.54% (0.415407), test_acc: 47.27% (0.472695), test_f1: 44.36% (0.443585)
Epoch 42 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 43
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.097883, val_acc: 43.45% (0.434527), val_f1: 41.51% (0.415150), test_acc: 44.63% (0.446271), test_f1: 42.07% (0.420746)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.030602, val_acc: 37.64% (0.376395), val_f1: 35.07% (0.350651), test_acc: 39.81% (0.398121), test_f1: 37.08% (0.370775)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.068695, val_acc: 43.75% (0.437463), val_f1: 40.55% (0.405494), test_acc: 44.92% (0.449207), test_f1: 40.57% (0.405672)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.088633, val_acc: 43.04% (0.430417), val_f1: 40.20% (0.401997), test_acc: 44.33% (0.443335), test_f1: 40.31% (0.403108)
Epoch 43 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 44
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.083789, val_acc: 45.04% (0.450382), val_f1: 41.49% (0.414906), test_acc: 46.04% (0.460364), test_f1: 41.76% (0.417577)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.999245, val_acc: 41.57% (0.415737), val_f1: 38.64% (0.386443), test_acc: 43.28% (0.432766), test_f1: 38.91% (0.389066)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.062148, val_acc: 45.57% (0.455666), val_f1: 40.78% (0.407821), test_acc: 47.62% (0.476218), test_f1: 42.46% (0.424647)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.969954, val_acc: 43.04% (0.430417), val_f1: 39.57% (0.395733), test_acc: 44.86% (0.448620), test_f1: 41.08% (0.410763)
Epoch 44 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 45
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.056227, val_acc: 40.05% (0.400470), val_f1: 36.88% (0.368832), test_acc: 41.16% (0.411627), test_f1: 37.27% (0.372691)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.085358, val_acc: 44.92% (0.449207), val_f1: 40.97% (0.409651), test_acc: 46.33% (0.463300), test_f1: 41.74% (0.417394)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.045199, val_acc: 43.92% (0.439225), val_f1: 40.44% (0.404431), test_acc: 46.51% (0.465062), test_f1: 42.66% (0.426625)
Batch 180 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.420755 (previous best: 0.420158)
loss: 1.007259, val_acc: 44.63% (0.446271), val_f1: 42.08% (0.420755), test_acc: 45.39% (0.453905), test_f1: 42.60% (0.426038)
Epoch 45 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 46
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.031396, val_acc: 42.34% (0.423371), val_f1: 39.98% (0.399773), test_acc: 43.81% (0.438050), test_f1: 40.54% (0.405371)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.041266, val_acc: 45.51% (0.455079), val_f1: 41.72% (0.417173), test_acc: 46.10% (0.460951), test_f1: 41.78% (0.417807)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.068720, val_acc: 45.51% (0.455079), val_f1: 40.75% (0.407465), test_acc: 47.92% (0.479154), test_f1: 42.47% (0.424661)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.040638, val_acc: 44.69% (0.446858), val_f1: 40.79% (0.407875), test_acc: 46.74% (0.467410), test_f1: 42.51% (0.425066)
Epoch 46 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 47
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.067053, val_acc: 43.39% (0.433940), val_f1: 40.43% (0.404332), test_acc: 44.74% (0.447446), test_f1: 41.04% (0.410429)
Batch 60 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.421918 (previous best: 0.420755)
loss: 1.028434, val_acc: 44.16% (0.441574), val_f1: 42.19% (0.421918), test_acc: 44.57% (0.445684), test_f1: 42.27% (0.422680)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.058493, val_acc: 41.51% (0.415150), val_f1: 39.63% (0.396294), test_acc: 43.92% (0.439225), test_f1: 41.13% (0.411271)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.008102, val_acc: 43.28% (0.432766), val_f1: 39.41% (0.394132), test_acc: 45.63% (0.456254), test_f1: 41.40% (0.413961)
Epoch 47 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 48
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.048391, val_acc: 41.28% (0.412801), val_f1: 36.94% (0.369419), test_acc: 43.39% (0.433940), test_f1: 38.73% (0.387286)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.998990, val_acc: 46.04% (0.460364), val_f1: 40.06% (0.400622), test_acc: 48.50% (0.485026), test_f1: 43.23% (0.432275)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.043783, val_acc: 45.51% (0.455079), val_f1: 40.89% (0.408869), test_acc: 47.15% (0.471521), test_f1: 42.66% (0.426600)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.961572, val_acc: 45.16% (0.451556), val_f1: 40.37% (0.403681), test_acc: 46.51% (0.465062), test_f1: 41.36% (0.413632)
Epoch 48 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 49
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.009799, val_acc: 44.22% (0.442161), val_f1: 41.34% (0.413418), test_acc: 45.92% (0.459190), test_f1: 42.55% (0.425512)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.006480, val_acc: 36.82% (0.368174), val_f1: 33.35% (0.333457), test_acc: 38.81% (0.388139), test_f1: 35.06% (0.350637)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.000284, val_acc: 41.28% (0.412801), val_f1: 38.37% (0.383721), test_acc: 42.34% (0.423371), test_f1: 38.51% (0.385067)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.995219, val_acc: 41.16% (0.411627), val_f1: 38.42% (0.384190), test_acc: 42.63% (0.426307), test_f1: 39.28% (0.392839)
Epoch 49 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 50
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.018393, val_acc: 44.16% (0.441574), val_f1: 40.87% (0.408746), test_acc: 45.74% (0.457428), test_f1: 42.63% (0.426328)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.073779, val_acc: 41.98% (0.419847), val_f1: 39.00% (0.389957), test_acc: 44.86% (0.448620), test_f1: 41.42% (0.414156)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.004550, val_acc: 45.45% (0.454492), val_f1: 41.19% (0.411856), test_acc: 47.27% (0.472695), test_f1: 42.89% (0.428891)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.966889, val_acc: 41.87% (0.418673), val_f1: 38.67% (0.386706), test_acc: 42.98% (0.429830), test_f1: 38.48% (0.384840)
Epoch 50 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 51
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.039596, val_acc: 44.80% (0.448033), val_f1: 41.03% (0.410295), test_acc: 45.98% (0.459777), test_f1: 42.08% (0.420822)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.001331, val_acc: 41.05% (0.410452), val_f1: 38.54% (0.385423), test_acc: 44.45% (0.444510), test_f1: 41.20% (0.412030)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.053897, val_acc: 41.98% (0.419847), val_f1: 39.46% (0.394644), test_acc: 43.69% (0.436876), test_f1: 40.04% (0.400395)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.000366, val_acc: 43.34% (0.433353), val_f1: 40.27% (0.402671), test_acc: 44.27% (0.442748), test_f1: 40.73% (0.407321)
Epoch 51 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 52
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.038628, val_acc: 43.92% (0.439225), val_f1: 40.95% (0.409486), test_acc: 44.04% (0.440399), test_f1: 40.46% (0.404595)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.073755, val_acc: 45.33% (0.453318), val_f1: 39.01% (0.390112), test_acc: 48.33% (0.483265), test_f1: 42.98% (0.429760)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.053634, val_acc: 40.28% (0.402819), val_f1: 36.99% (0.369863), test_acc: 43.45% (0.434527), test_f1: 39.74% (0.397366)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.030088, val_acc: 37.76% (0.377569), val_f1: 34.36% (0.343624), test_acc: 40.22% (0.402231), test_f1: 36.74% (0.367375)
Epoch 52 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 53
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.958461, val_acc: 39.99% (0.399883), val_f1: 37.96% (0.379617), test_acc: 41.10% (0.411039), test_f1: 38.42% (0.384155)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.035113, val_acc: 40.46% (0.404580), val_f1: 37.08% (0.370794), test_acc: 42.87% (0.428655), test_f1: 38.70% (0.386998)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.967519, val_acc: 39.87% (0.398708), val_f1: 36.53% (0.365294), test_acc: 41.63% (0.416324), test_f1: 38.20% (0.381973)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.996907, val_acc: 41.34% (0.413388), val_f1: 38.53% (0.385325), test_acc: 42.98% (0.429830), test_f1: 39.84% (0.398373)
Epoch 53 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 54
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.047771, val_acc: 43.69% (0.436876), val_f1: 41.33% (0.413348), test_acc: 45.04% (0.450382), test_f1: 42.24% (0.422356)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.066664, val_acc: 42.04% (0.420435), val_f1: 39.35% (0.393519), test_acc: 44.33% (0.443335), test_f1: 40.98% (0.409790)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.985644, val_acc: 43.39% (0.433940), val_f1: 39.88% (0.398817), test_acc: 46.56% (0.465649), test_f1: 43.27% (0.432679)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.039885, val_acc: 37.64% (0.376395), val_f1: 34.52% (0.345231), test_acc: 39.64% (0.396359), test_f1: 36.23% (0.362333)
Epoch 54 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 55
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.071441, val_acc: 43.39% (0.433940), val_f1: 41.57% (0.415681), test_acc: 44.80% (0.448033), test_f1: 42.55% (0.425513)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.004417, val_acc: 46.15% (0.461538), val_f1: 40.90% (0.409016), test_acc: 47.92% (0.479154), test_f1: 42.46% (0.424612)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.006545, val_acc: 43.57% (0.435702), val_f1: 40.37% (0.403687), test_acc: 44.57% (0.445684), test_f1: 40.95% (0.409467)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.923532, val_acc: 44.74% (0.447446), val_f1: 40.94% (0.409376), test_acc: 45.21% (0.452143), test_f1: 41.27% (0.412674)
Epoch 55 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 56
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.040501, val_acc: 45.74% (0.457428), val_f1: 41.85% (0.418462), test_acc: 46.45% (0.464474), test_f1: 42.65% (0.426457)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.964273, val_acc: 38.93% (0.389313), val_f1: 36.25% (0.362493), test_acc: 41.10% (0.411039), test_f1: 38.16% (0.381614)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.051589, val_acc: 44.69% (0.446858), val_f1: 41.80% (0.418047), test_acc: 45.74% (0.457428), test_f1: 42.66% (0.426643)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.025220, val_acc: 43.22% (0.432179), val_f1: 39.62% (0.396190), test_acc: 45.21% (0.452143), test_f1: 41.96% (0.419639)
Epoch 56 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 57
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.048699, val_acc: 45.45% (0.454492), val_f1: 41.14% (0.411433), test_acc: 46.92% (0.469172), test_f1: 42.86% (0.428583)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.017094, val_acc: 42.40% (0.423958), val_f1: 39.95% (0.399460), test_acc: 45.10% (0.450969), test_f1: 42.35% (0.423518)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.960841, val_acc: 44.86% (0.448620), val_f1: 41.33% (0.413263), test_acc: 46.86% (0.468585), test_f1: 43.71% (0.437136)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.048476, val_acc: 44.45% (0.444510), val_f1: 40.86% (0.408627), test_acc: 45.98% (0.459777), test_f1: 42.39% (0.423919)
Epoch 57 completed in 4.32 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 58
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.997839, val_acc: 45.45% (0.454492), val_f1: 41.53% (0.415279), test_acc: 47.03% (0.470346), test_f1: 43.20% (0.431988)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.017240, val_acc: 39.05% (0.390487), val_f1: 37.23% (0.372334), test_acc: 41.22% (0.412214), test_f1: 39.01% (0.390074)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.132992, val_acc: 41.51% (0.415150), val_f1: 37.62% (0.376205), test_acc: 43.10% (0.431004), test_f1: 38.36% (0.383590)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.043610, val_acc: 44.04% (0.440399), val_f1: 39.93% (0.399255), test_acc: 45.92% (0.459190), test_f1: 41.98% (0.419773)
Epoch 58 completed in 4.35 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 59
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.028560, val_acc: 46.39% (0.463887), val_f1: 40.08% (0.400841), test_acc: 47.74% (0.477393), test_f1: 41.36% (0.413563)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.101413, val_acc: 43.69% (0.436876), val_f1: 39.15% (0.391469), test_acc: 45.39% (0.453905), test_f1: 40.77% (0.407670)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.988304, val_acc: 44.63% (0.446271), val_f1: 40.52% (0.405196), test_acc: 45.92% (0.459190), test_f1: 41.52% (0.415172)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.979848, val_acc: 43.69% (0.436876), val_f1: 40.49% (0.404870), test_acc: 44.51% (0.445097), test_f1: 40.60% (0.405972)
Epoch 59 completed in 4.36 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 60
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.155234, val_acc: 43.92% (0.439225), val_f1: 40.94% (0.409423), test_acc: 45.68% (0.456841), test_f1: 41.93% (0.419271)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.003913, val_acc: 45.39% (0.453905), val_f1: 40.11% (0.401094), test_acc: 46.62% (0.466236), test_f1: 41.94% (0.419422)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.020957, val_acc: 39.46% (0.394598), val_f1: 38.06% (0.380602), test_acc: 41.05% (0.410452), test_f1: 39.20% (0.391962)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.940296, val_acc: 44.80% (0.448033), val_f1: 41.89% (0.418917), test_acc: 47.45% (0.474457), test_f1: 44.46% (0.444600)
Epoch 60 completed in 4.39 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 61
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.012657, val_acc: 44.92% (0.449207), val_f1: 41.93% (0.419326), test_acc: 45.68% (0.456841), test_f1: 42.55% (0.425486)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.040068, val_acc: 45.74% (0.457428), val_f1: 40.94% (0.409393), test_acc: 47.33% (0.473282), test_f1: 42.61% (0.426063)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.025933, val_acc: 42.45% (0.424545), val_f1: 41.04% (0.410365), test_acc: 43.75% (0.437463), test_f1: 42.24% (0.422424)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.978562, val_acc: 45.04% (0.450382), val_f1: 40.12% (0.401162), test_acc: 47.27% (0.472695), test_f1: 42.49% (0.424901)
Epoch 61 completed in 4.35 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 62
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.024801, val_acc: 44.69% (0.446858), val_f1: 41.87% (0.418674), test_acc: 46.21% (0.462126), test_f1: 43.22% (0.432156)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.984810, val_acc: 43.98% (0.439812), val_f1: 40.88% (0.408814), test_acc: 43.04% (0.430417), test_f1: 38.82% (0.388207)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.032148, val_acc: 42.69% (0.426894), val_f1: 40.35% (0.403504), test_acc: 42.81% (0.428068), test_f1: 39.73% (0.397274)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.987421, val_acc: 43.69% (0.436876), val_f1: 40.92% (0.409225), test_acc: 45.39% (0.453905), test_f1: 42.35% (0.423513)
Epoch 62 completed in 4.39 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 63
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.967382, val_acc: 41.57% (0.415737), val_f1: 37.55% (0.375534), test_acc: 43.10% (0.431004), test_f1: 39.29% (0.392867)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.998528, val_acc: 36.29% (0.362889), val_f1: 36.31% (0.363118), test_acc: 38.64% (0.386377), test_f1: 38.56% (0.385640)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.078387, val_acc: 43.34% (0.433353), val_f1: 41.24% (0.412364), test_acc: 44.16% (0.441574), test_f1: 41.85% (0.418465)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.037896, val_acc: 36.99% (0.369935), val_f1: 32.46% (0.324596), test_acc: 37.23% (0.372284), test_f1: 32.58% (0.325819)
Epoch 63 completed in 4.36 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 64
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.977077, val_acc: 42.45% (0.424545), val_f1: 40.95% (0.409466), test_acc: 43.16% (0.431591), test_f1: 41.40% (0.413993)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.947331, val_acc: 40.93% (0.409278), val_f1: 37.91% (0.379108), test_acc: 42.40% (0.423958), test_f1: 39.01% (0.390074)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.962866, val_acc: 45.21% (0.452143), val_f1: 41.09% (0.410865), test_acc: 45.80% (0.458015), test_f1: 41.88% (0.418836)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.031121, val_acc: 44.74% (0.447446), val_f1: 41.90% (0.419011), test_acc: 45.98% (0.459777), test_f1: 42.89% (0.428932)
Epoch 64 completed in 4.37 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 65
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.972617, val_acc: 43.57% (0.435702), val_f1: 41.17% (0.411675), test_acc: 45.27% (0.452730), test_f1: 42.62% (0.426250)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.981891, val_acc: 40.87% (0.408691), val_f1: 35.79% (0.357932), test_acc: 42.81% (0.428068), test_f1: 37.14% (0.371379)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.929761, val_acc: 46.04% (0.460364), val_f1: 40.25% (0.402489), test_acc: 47.80% (0.477980), test_f1: 42.23% (0.422331)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.079355, val_acc: 41.46% (0.414563), val_f1: 38.65% (0.386470), test_acc: 42.98% (0.429830), test_f1: 39.26% (0.392634)
Epoch 65 completed in 4.41 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 66
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.966534, val_acc: 41.57% (0.415737), val_f1: 38.83% (0.388292), test_acc: 42.92% (0.429243), test_f1: 39.83% (0.398350)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.954042, val_acc: 40.34% (0.403406), val_f1: 37.27% (0.372680), test_acc: 42.34% (0.423371), test_f1: 38.66% (0.386583)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.977687, val_acc: 45.74% (0.457428), val_f1: 41.04% (0.410383), test_acc: 45.92% (0.459190), test_f1: 40.82% (0.408171)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.038882, val_acc: 38.99% (0.389900), val_f1: 36.15% (0.361479), test_acc: 40.52% (0.405167), test_f1: 37.72% (0.377180)
Epoch 66 completed in 4.38 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 67
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.099620, val_acc: 41.22% (0.412214), val_f1: 39.21% (0.392132), test_acc: 43.16% (0.431591), test_f1: 41.07% (0.410684)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.062236, val_acc: 41.51% (0.415150), val_f1: 38.68% (0.386849), test_acc: 42.57% (0.425719), test_f1: 39.06% (0.390636)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.972989, val_acc: 45.16% (0.451556), val_f1: 40.99% (0.409901), test_acc: 45.39% (0.453905), test_f1: 40.79% (0.407908)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.068473, val_acc: 39.52% (0.395185), val_f1: 36.20% (0.362043), test_acc: 41.75% (0.417499), test_f1: 38.43% (0.384347)
Epoch 67 completed in 4.39 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 68
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.031209, val_acc: 41.40% (0.413975), val_f1: 37.66% (0.376648), test_acc: 42.92% (0.429243), test_f1: 38.35% (0.383499)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.101114, val_acc: 44.27% (0.442748), val_f1: 40.68% (0.406827), test_acc: 44.33% (0.443335), test_f1: 39.87% (0.398711)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.019706, val_acc: 45.04% (0.450382), val_f1: 39.33% (0.393254), test_acc: 46.56% (0.465649), test_f1: 40.51% (0.405091)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.941003, val_acc: 41.40% (0.413975), val_f1: 38.17% (0.381716), test_acc: 42.40% (0.423958), test_f1: 38.69% (0.386947)
Epoch 68 completed in 4.41 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 69
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.881630, val_acc: 44.10% (0.440986), val_f1: 39.58% (0.395752), test_acc: 45.33% (0.453318), test_f1: 40.52% (0.405210)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.946984, val_acc: 45.57% (0.455666), val_f1: 40.94% (0.409443), test_acc: 45.04% (0.450382), test_f1: 40.22% (0.402171)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.084484, val_acc: 38.05% (0.380505), val_f1: 36.13% (0.361292), test_acc: 40.75% (0.407516), test_f1: 38.55% (0.385512)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.053020, val_acc: 43.45% (0.434527), val_f1: 39.84% (0.398394), test_acc: 44.27% (0.442748), test_f1: 40.35% (0.403487)
Epoch 69 completed in 4.40 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 70
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.929443, val_acc: 41.28% (0.412801), val_f1: 38.05% (0.380547), test_acc: 41.46% (0.414563), test_f1: 37.99% (0.379882)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.012740, val_acc: 37.40% (0.374046), val_f1: 34.22% (0.342174), test_acc: 39.40% (0.394011), test_f1: 36.48% (0.364844)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.999165, val_acc: 43.39% (0.433940), val_f1: 39.98% (0.399775), test_acc: 44.10% (0.440986), test_f1: 40.21% (0.402098)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.994020, val_acc: 41.40% (0.413975), val_f1: 38.90% (0.388956), test_acc: 42.45% (0.424545), test_f1: 39.62% (0.396221)
Epoch 70 completed in 4.45 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 71
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.058083, val_acc: 41.75% (0.417499), val_f1: 39.15% (0.391515), test_acc: 43.34% (0.433353), test_f1: 40.81% (0.408145)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.911219, val_acc: 41.93% (0.419260), val_f1: 37.87% (0.378727), test_acc: 43.04% (0.430417), test_f1: 38.43% (0.384327)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.042525, val_acc: 46.10% (0.460951), val_f1: 40.61% (0.406079), test_acc: 47.27% (0.472695), test_f1: 42.38% (0.423800)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.971641, val_acc: 45.45% (0.454492), val_f1: 41.15% (0.411510), test_acc: 45.57% (0.455666), test_f1: 41.18% (0.411806)
Epoch 71 completed in 4.39 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 72
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.139524, val_acc: 42.92% (0.429243), val_f1: 40.23% (0.402348), test_acc: 44.80% (0.448033), test_f1: 41.44% (0.414398)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.983943, val_acc: 36.93% (0.369348), val_f1: 34.53% (0.345310), test_acc: 39.40% (0.394011), test_f1: 37.29% (0.372949)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.061197, val_acc: 42.16% (0.421609), val_f1: 39.68% (0.396819), test_acc: 43.45% (0.434527), test_f1: 41.26% (0.412630)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.932477, val_acc: 39.22% (0.392249), val_f1: 36.59% (0.365933), test_acc: 41.10% (0.411039), test_f1: 38.17% (0.381698)
Epoch 72 completed in 4.38 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 73
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.020165, val_acc: 43.04% (0.430417), val_f1: 39.30% (0.392998), test_acc: 42.98% (0.429830), test_f1: 38.40% (0.383964)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.979648, val_acc: 42.51% (0.425132), val_f1: 40.06% (0.400563), test_acc: 43.57% (0.435702), test_f1: 40.76% (0.407552)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.921505, val_acc: 43.04% (0.430417), val_f1: 39.37% (0.393655), test_acc: 43.57% (0.435702), test_f1: 39.15% (0.391462)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.041481, val_acc: 37.99% (0.379918), val_f1: 34.09% (0.340880), test_acc: 40.52% (0.405167), test_f1: 36.20% (0.361983)
Epoch 73 completed in 4.35 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 74
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.055863, val_acc: 43.69% (0.436876), val_f1: 39.95% (0.399528), test_acc: 43.57% (0.435702), test_f1: 38.67% (0.386672)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.909430, val_acc: 40.87% (0.408691), val_f1: 37.86% (0.378565), test_acc: 42.10% (0.421022), test_f1: 38.15% (0.381536)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.021264, val_acc: 46.92% (0.469172), val_f1: 40.62% (0.406187), test_acc: 47.80% (0.477980), test_f1: 41.48% (0.414832)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.969487, val_acc: 42.69% (0.426894), val_f1: 40.05% (0.400507), test_acc: 43.86% (0.438638), test_f1: 40.79% (0.407879)
Epoch 74 completed in 4.46 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 75
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.911827, val_acc: 43.86% (0.438638), val_f1: 40.88% (0.408800), test_acc: 43.10% (0.431004), test_f1: 39.91% (0.399065)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.041199, val_acc: 43.81% (0.438050), val_f1: 40.78% (0.407766), test_acc: 44.04% (0.440399), test_f1: 40.87% (0.408715)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.094094, val_acc: 45.51% (0.455079), val_f1: 40.56% (0.405602), test_acc: 46.33% (0.463300), test_f1: 41.01% (0.410090)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.044265, val_acc: 40.93% (0.409278), val_f1: 38.97% (0.389715), test_acc: 41.87% (0.418673), test_f1: 39.63% (0.396266)
Epoch 75 completed in 4.39 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 76
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.995221, val_acc: 41.40% (0.413975), val_f1: 39.31% (0.393139), test_acc: 43.10% (0.431004), test_f1: 40.78% (0.407847)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.972172, val_acc: 39.75% (0.397534), val_f1: 37.19% (0.371873), test_acc: 40.99% (0.409865), test_f1: 38.40% (0.383968)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.989902, val_acc: 43.22% (0.432179), val_f1: 39.54% (0.395447), test_acc: 43.51% (0.435115), test_f1: 40.63% (0.406256)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.906919, val_acc: 43.98% (0.439812), val_f1: 40.10% (0.400998), test_acc: 46.15% (0.461538), test_f1: 41.97% (0.419686)
Epoch 76 completed in 4.47 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 77
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.998644, val_acc: 37.46% (0.374633), val_f1: 35.72% (0.357211), test_acc: 39.52% (0.395185), test_f1: 37.97% (0.379667)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.959534, val_acc: 41.81% (0.418086), val_f1: 40.58% (0.405791), test_acc: 43.45% (0.434527), test_f1: 42.33% (0.423256)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.018556, val_acc: 40.93% (0.409278), val_f1: 37.37% (0.373653), test_acc: 41.75% (0.417499), test_f1: 38.55% (0.385532)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.042188, val_acc: 36.00% (0.359953), val_f1: 35.12% (0.351212), test_acc: 38.70% (0.386964), test_f1: 37.88% (0.378817)
Epoch 77 completed in 4.40 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 78
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.007089, val_acc: 43.10% (0.431004), val_f1: 40.17% (0.401673), test_acc: 45.68% (0.456841), test_f1: 42.43% (0.424346)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.987566, val_acc: 44.63% (0.446271), val_f1: 41.50% (0.415046), test_acc: 43.63% (0.436289), test_f1: 40.36% (0.403580)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.038463, val_acc: 44.92% (0.449207), val_f1: 40.81% (0.408067), test_acc: 44.86% (0.448620), test_f1: 40.34% (0.403357)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.982639, val_acc: 40.99% (0.409865), val_f1: 38.42% (0.384250), test_acc: 42.28% (0.422783), test_f1: 39.26% (0.392554)
Epoch 78 completed in 4.42 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 79
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.944621, val_acc: 41.93% (0.419260), val_f1: 39.05% (0.390534), test_acc: 43.22% (0.432179), test_f1: 40.01% (0.400086)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.998794, val_acc: 39.11% (0.391075), val_f1: 37.46% (0.374622), test_acc: 41.34% (0.413388), test_f1: 40.03% (0.400330)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.905149, val_acc: 41.75% (0.417499), val_f1: 38.97% (0.389673), test_acc: 42.40% (0.423958), test_f1: 38.63% (0.386321)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.947096, val_acc: 41.16% (0.411627), val_f1: 39.45% (0.394541), test_acc: 42.10% (0.421022), test_f1: 40.62% (0.406213)
Epoch 79 completed in 4.37 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 80
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.995231, val_acc: 39.28% (0.392836), val_f1: 38.78% (0.387826), test_acc: 43.28% (0.432766), test_f1: 42.75% (0.427453)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.980860, val_acc: 40.05% (0.400470), val_f1: 39.21% (0.392127), test_acc: 42.57% (0.425719), test_f1: 41.71% (0.417150)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.998192, val_acc: 36.23% (0.362302), val_f1: 36.37% (0.363710), test_acc: 38.46% (0.384615), test_f1: 38.69% (0.386909)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.978406, val_acc: 39.99% (0.399883), val_f1: 39.58% (0.395832), test_acc: 40.52% (0.405167), test_f1: 39.98% (0.399774)
Epoch 80 completed in 4.44 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 81
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.036919, val_acc: 44.04% (0.440399), val_f1: 40.39% (0.403879), test_acc: 44.98% (0.449794), test_f1: 41.55% (0.415467)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.019410, val_acc: 40.46% (0.404580), val_f1: 38.71% (0.387064), test_acc: 41.51% (0.415150), test_f1: 39.80% (0.398005)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.970558, val_acc: 40.52% (0.405167), val_f1: 39.04% (0.390403), test_acc: 42.04% (0.420435), test_f1: 40.80% (0.407973)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.013134, val_acc: 40.81% (0.408103), val_f1: 39.81% (0.398089), test_acc: 41.57% (0.415737), test_f1: 40.38% (0.403780)
Epoch 81 completed in 4.54 seconds (0.08 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 82
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.944462, val_acc: 43.16% (0.431591), val_f1: 40.22% (0.402248), test_acc: 43.57% (0.435702), test_f1: 40.68% (0.406759)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.961610, val_acc: 41.93% (0.419260), val_f1: 39.73% (0.397253), test_acc: 42.63% (0.426307), test_f1: 40.58% (0.405800)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.991372, val_acc: 42.45% (0.424545), val_f1: 39.92% (0.399177), test_acc: 42.87% (0.428655), test_f1: 40.41% (0.404095)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.949153, val_acc: 41.98% (0.419847), val_f1: 39.78% (0.397830), test_acc: 43.34% (0.433353), test_f1: 40.93% (0.409258)
Epoch 82 completed in 4.38 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 83
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.981597, val_acc: 39.46% (0.394598), val_f1: 38.93% (0.389306), test_acc: 40.87% (0.408691), test_f1: 40.26% (0.402562)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.985469, val_acc: 42.45% (0.424545), val_f1: 40.20% (0.401990), test_acc: 42.45% (0.424545), test_f1: 40.34% (0.403449)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.030862, val_acc: 35.53% (0.355255), val_f1: 35.61% (0.356055), test_acc: 38.93% (0.389313), test_f1: 39.01% (0.390057)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.994911, val_acc: 41.16% (0.411627), val_f1: 37.74% (0.377363), test_acc: 40.52% (0.405167), test_f1: 37.27% (0.372728)
Epoch 83 completed in 4.41 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 84
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.057614, val_acc: 38.05% (0.380505), val_f1: 36.34% (0.363408), test_acc: 40.11% (0.401057), test_f1: 38.92% (0.389163)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.936752, val_acc: 38.52% (0.385203), val_f1: 38.13% (0.381267), test_acc: 40.63% (0.406342), test_f1: 40.27% (0.402690)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.926682, val_acc: 43.39% (0.433940), val_f1: 40.68% (0.406751), test_acc: 43.98% (0.439812), test_f1: 41.54% (0.415427)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.897495, val_acc: 42.75% (0.427481), val_f1: 39.72% (0.397177), test_acc: 44.16% (0.441574), test_f1: 41.24% (0.412387)
Epoch 84 completed in 4.41 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 85
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.978543, val_acc: 38.93% (0.389313), val_f1: 38.44% (0.384398), test_acc: 41.05% (0.410452), test_f1: 40.30% (0.403010)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.009537, val_acc: 42.57% (0.425719), val_f1: 38.75% (0.387454), test_acc: 42.57% (0.425719), test_f1: 39.02% (0.390198)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.093517, val_acc: 42.34% (0.423371), val_f1: 40.05% (0.400461), test_acc: 42.28% (0.422783), test_f1: 40.20% (0.402001)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.931627, val_acc: 36.35% (0.363476), val_f1: 35.39% (0.353881), test_acc: 38.64% (0.386377), test_f1: 38.18% (0.381763)
Epoch 85 completed in 4.39 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 86
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.982786, val_acc: 42.16% (0.421609), val_f1: 40.51% (0.405103), test_acc: 43.34% (0.433353), test_f1: 41.88% (0.418826)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.978920, val_acc: 37.64% (0.376395), val_f1: 36.96% (0.369551), test_acc: 39.93% (0.399295), test_f1: 39.54% (0.395449)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.008971, val_acc: 40.11% (0.401057), val_f1: 38.13% (0.381277), test_acc: 41.05% (0.410452), test_f1: 39.53% (0.395334)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.824204, val_acc: 40.28% (0.402819), val_f1: 39.13% (0.391291), test_acc: 41.63% (0.416324), test_f1: 40.12% (0.401186)
Epoch 86 completed in 4.36 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 87
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.050115, val_acc: 37.05% (0.370523), val_f1: 36.97% (0.369659), test_acc: 40.16% (0.401644), test_f1: 39.95% (0.399515)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.918472, val_acc: 39.93% (0.399295), val_f1: 39.22% (0.392241), test_acc: 41.05% (0.410452), test_f1: 40.43% (0.404253)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.994071, val_acc: 38.76% (0.387551), val_f1: 37.75% (0.377545), test_acc: 40.58% (0.405755), test_f1: 39.80% (0.398005)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.107863, val_acc: 40.58% (0.405755), val_f1: 38.61% (0.386087), test_acc: 41.40% (0.413975), test_f1: 39.68% (0.396778)
Epoch 87 completed in 4.36 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 88
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.011284, val_acc: 42.10% (0.421022), val_f1: 37.69% (0.376945), test_acc: 41.93% (0.419260), test_f1: 38.46% (0.384576)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.941033, val_acc: 39.87% (0.398708), val_f1: 38.45% (0.384521), test_acc: 41.69% (0.416911), test_f1: 40.46% (0.404639)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.952772, val_acc: 39.46% (0.394598), val_f1: 37.08% (0.370838), test_acc: 40.69% (0.406929), test_f1: 38.84% (0.388367)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.876365, val_acc: 41.05% (0.410452), val_f1: 39.24% (0.392396), test_acc: 42.04% (0.420435), test_f1: 40.35% (0.403484)
Epoch 88 completed in 4.68 seconds (0.08 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 89
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.932267, val_acc: 40.99% (0.409865), val_f1: 39.69% (0.396914), test_acc: 41.22% (0.412214), test_f1: 39.96% (0.399641)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.918107, val_acc: 42.87% (0.428655), val_f1: 40.38% (0.403830), test_acc: 42.98% (0.429830), test_f1: 40.45% (0.404509)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.995958, val_acc: 36.64% (0.366412), val_f1: 35.96% (0.359631), test_acc: 39.52% (0.395185), test_f1: 39.10% (0.391011)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.923873, val_acc: 41.05% (0.410452), val_f1: 39.82% (0.398178), test_acc: 41.81% (0.418086), test_f1: 40.58% (0.405840)
Epoch 89 completed in 4.42 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 90
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.963130, val_acc: 36.64% (0.366412), val_f1: 36.78% (0.367762), test_acc: 38.11% (0.381092), test_f1: 38.24% (0.382437)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.042674, val_acc: 38.64% (0.386377), val_f1: 38.27% (0.382667), test_acc: 41.28% (0.412801), test_f1: 40.91% (0.409144)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.009380, val_acc: 39.87% (0.398708), val_f1: 37.69% (0.376906), test_acc: 40.63% (0.406342), test_f1: 38.33% (0.383277)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.947905, val_acc: 37.76% (0.377569), val_f1: 35.65% (0.356516), test_acc: 39.05% (0.390487), test_f1: 37.60% (0.376013)
Epoch 90 completed in 4.41 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 91
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.977191, val_acc: 39.46% (0.394598), val_f1: 38.35% (0.383548), test_acc: 41.51% (0.415150), test_f1: 40.77% (0.407716)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.033449, val_acc: 40.87% (0.408691), val_f1: 39.43% (0.394282), test_acc: 42.28% (0.422783), test_f1: 40.83% (0.408297)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.975715, val_acc: 43.51% (0.435115), val_f1: 38.65% (0.386525), test_acc: 43.34% (0.433353), test_f1: 39.36% (0.393570)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.895838, val_acc: 40.87% (0.408691), val_f1: 39.43% (0.394326), test_acc: 42.69% (0.426894), test_f1: 41.39% (0.413946)
Epoch 91 completed in 4.51 seconds (0.08 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 92
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.982478, val_acc: 40.81% (0.408103), val_f1: 39.34% (0.393359), test_acc: 42.04% (0.420435), test_f1: 40.70% (0.406989)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.870209, val_acc: 42.10% (0.421022), val_f1: 39.33% (0.393335), test_acc: 43.16% (0.431591), test_f1: 40.74% (0.407381)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.998051, val_acc: 40.46% (0.404580), val_f1: 39.08% (0.390830), test_acc: 41.87% (0.418673), test_f1: 40.83% (0.408270)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.959192, val_acc: 39.40% (0.394011), val_f1: 37.72% (0.377168), test_acc: 40.75% (0.407516), test_f1: 39.34% (0.393422)
Epoch 92 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 93
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.044209, val_acc: 39.34% (0.393423), val_f1: 38.01% (0.380103), test_acc: 41.28% (0.412801), test_f1: 40.43% (0.404336)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.980296, val_acc: 43.45% (0.434527), val_f1: 38.49% (0.384950), test_acc: 45.16% (0.451556), test_f1: 40.19% (0.401915)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.946990, val_acc: 40.52% (0.405167), val_f1: 39.46% (0.394645), test_acc: 41.93% (0.419260), test_f1: 40.77% (0.407689)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.904365, val_acc: 33.76% (0.337639), val_f1: 33.46% (0.334597), test_acc: 36.11% (0.361127), test_f1: 35.99% (0.359877)
Epoch 93 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 94
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.970431, val_acc: 37.05% (0.370523), val_f1: 34.04% (0.340366), test_acc: 38.46% (0.384615), test_f1: 35.99% (0.359923)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.840556, val_acc: 42.16% (0.421609), val_f1: 39.49% (0.394926), test_acc: 43.75% (0.437463), test_f1: 41.72% (0.417212)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.958397, val_acc: 38.17% (0.381679), val_f1: 37.97% (0.379701), test_acc: 40.34% (0.403406), test_f1: 39.94% (0.399393)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.016447, val_acc: 42.75% (0.427481), val_f1: 39.13% (0.391305), test_acc: 43.86% (0.438638), test_f1: 40.85% (0.408464)
Epoch 94 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 95
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.017653, val_acc: 41.98% (0.419847), val_f1: 39.92% (0.399201), test_acc: 43.22% (0.432179), test_f1: 41.40% (0.414040)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.017977, val_acc: 39.40% (0.394011), val_f1: 38.09% (0.380942), test_acc: 40.52% (0.405167), test_f1: 39.36% (0.393574)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.929956, val_acc: 43.45% (0.434527), val_f1: 39.22% (0.392178), test_acc: 44.27% (0.442748), test_f1: 40.39% (0.403909)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.980145, val_acc: 36.29% (0.362889), val_f1: 36.20% (0.362016), test_acc: 39.11% (0.391075), test_f1: 39.10% (0.391048)
Epoch 95 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 96
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.895001, val_acc: 38.17% (0.381679), val_f1: 37.02% (0.370170), test_acc: 40.69% (0.406929), test_f1: 39.95% (0.399481)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.940839, val_acc: 38.23% (0.382267), val_f1: 38.07% (0.380733), test_acc: 40.28% (0.402819), test_f1: 40.05% (0.400531)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.931861, val_acc: 39.58% (0.395772), val_f1: 38.11% (0.381101), test_acc: 41.69% (0.416911), test_f1: 40.37% (0.403738)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.085437, val_acc: 44.16% (0.441574), val_f1: 40.49% (0.404929), test_acc: 44.45% (0.444510), test_f1: 40.49% (0.404920)
Epoch 96 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 97
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.986916, val_acc: 42.63% (0.426307), val_f1: 38.55% (0.385451), test_acc: 43.63% (0.436289), test_f1: 40.30% (0.402985)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.961183, val_acc: 43.86% (0.438638), val_f1: 40.29% (0.402858), test_acc: 44.51% (0.445097), test_f1: 41.28% (0.412784)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.970609, val_acc: 43.04% (0.430417), val_f1: 39.34% (0.393439), test_acc: 44.10% (0.440986), test_f1: 40.95% (0.409500)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.920651, val_acc: 38.34% (0.383441), val_f1: 37.81% (0.378100), test_acc: 40.87% (0.408691), test_f1: 40.47% (0.404674)
Epoch 97 completed in 4.30 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 98
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.962274, val_acc: 41.22% (0.412214), val_f1: 39.85% (0.398450), test_acc: 41.93% (0.419260), test_f1: 40.55% (0.405472)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.933487, val_acc: 39.17% (0.391662), val_f1: 37.71% (0.377136), test_acc: 40.87% (0.408691), test_f1: 39.70% (0.396981)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.169640, val_acc: 41.34% (0.413388), val_f1: 39.96% (0.399558), test_acc: 42.16% (0.421609), test_f1: 40.17% (0.401705)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.961098, val_acc: 38.52% (0.385203), val_f1: 37.19% (0.371942), test_acc: 40.28% (0.402819), test_f1: 39.54% (0.395445)
Epoch 98 completed in 4.29 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 99
[DEBUG] Sample predictions in evaluate:  tensor([0, 0, 0, 2, 2, 1, 1, 2, 1, 0], device='cuda:0')
Batch 0 completed in 0.02 seconds (0.00 minutes)
loss: 0.988398, val_acc: 39.64% (0.396359), val_f1: 38.72% (0.387207), test_acc: 40.87% (0.408691), test_f1: 40.07% (0.400679)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.956112, val_acc: 38.34% (0.383441), val_f1: 37.22% (0.372227), test_acc: 39.75% (0.397534), test_f1: 38.78% (0.387761)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.912539, val_acc: 36.35% (0.363476), val_f1: 35.24% (0.352382), test_acc: 37.58% (0.375807), test_f1: 36.84% (0.368431)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.945077, val_acc: 40.63% (0.406342), val_f1: 37.91% (0.379095), test_acc: 42.69% (0.426894), test_f1: 40.22% (0.402217)
Epoch 99 completed in 4.30 seconds (0.07 minutes)
RESULT: Max Val F1: 0.421918, Max Test F1: 0.422680
Training complete. Generating confusion matrix on the test set.
Confusion matrix saved to /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/SIMPLE-T/2025-02-11/sub-1/014_Feb-11-2025_09:06_PM/confusion_matrix.png
Reading TensorBoard loss at each epoch:
Available tags: {'images': [], 'audio': [], 'histograms': [], 'scalars': ['Loss/train_batch', 'Loss/val_log_step', 'Loss/train_epoch', 'Loss/val_epoch'], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}
Output File: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/SIMPLE-T/2025-02-11/sub-1/014_Feb-11-2025_09:06_PM/trainval_loss_curves.png
Training and validation loss curves saved to /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/SIMPLE-T/2025-02-11/sub-1/014_Feb-11-2025_09:06_PM/trainval_loss_curves.png
Total Completion Time: 7.84 minutes. (0.13 hours) 
