SLURM Job ID: 19847650
Number of GPUs available: 1
Python PATH: ['/home/rgg2706/Multimodal-Sentiment-Analysis/Models/SIMPLE-T/src', '/home/rgg2706/Multimodal-Sentiment-Analysis', '/Models/SIMPLE-T/src', '/.autofs/tools/spack/var/spack/environments/default-nlp-x86_64-24072401/.spack-env/._view/wtxwc3mlkmzy7fbaxlum2674jarpitc2/lib/python311.zip', '/.autofs/tools/spack/var/spack/environments/default-nlp-x86_64-24072401/.spack-env/._view/wtxwc3mlkmzy7fbaxlum2674jarpitc2/lib/python3.11', '/.autofs/tools/spack/var/spack/environments/default-nlp-x86_64-24072401/.spack-env/._view/wtxwc3mlkmzy7fbaxlum2674jarpitc2/lib/python3.11/lib-dynload', '/.autofs/tools/spack/var/spack/environments/default-nlp-x86_64-24072401/.spack-env/._view/wtxwc3mlkmzy7fbaxlum2674jarpitc2/lib/python3.11/site-packages']
Logs directory: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/SIMPLE-T/2025-02-11/sub-1/010_Feb-11-2025_08:41_PM
> training arguments:
>>> rand_seed: 8
>>> model_fusion: simpletextatt
>>> dataset: mvsa-mts-v3
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> initializer: <function xavier_uniform_ at 0x7fee664d1440>
>>> learning_rate: 0.001
>>> dropout_rate: 0.5
>>> weight_decay: 0.0
>>> num_layers: 3
>>> num_epoch: 100
>>> batch_size: 64
>>> log_step: 60
>>> max_seq_len: 20
>>> polarities_dim: 3
>>> clip_grad: 5.0
>>> path_image: ./images
>>> crop_size: 224
>>> n_head: 8
>>> hidden_dim: 256
>>> num_classes: 3
>>> log_dir: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/SIMPLE-T/2025-02-11/sub-1/010_Feb-11-2025_08:41_PM
>>> counter: 0
>>> model_class: <class 'models.simpletextatt.SimpleTextAtt'>
Loading dataset 'mvsa-mts-v3':
  Train path: Datasets/MVSA-MTS/mvsa-mts-v3/train.tsv
	Validation path: Datasets/MVSA-MTS/mvsa-mts-v3/val.tsv
  Test path: Datasets//MVSA-MTS/mvsa-mts-v3/test.tsv
loading word vectors...
building embedding_matrix: 200_glove_embedding_matrix.dat
-------------- Loading Datasets/MVSA-MTS/mvsa-mts-v3/train.tsv ---------------
[DEBUG] index: 706
[DEBUG] raw_text: PC Party, #Youth, #Education,Opportunity,#RenewableResources, Proudly #Canada's PCs #elxn42 http://t.co/NSwTddCHS8
[DEBUG] processed_str: pc party opportunity proudly pcs
[DEBUG] text_indices: [2 3 4 5 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[DEBUG] polarity: 2
[DEBUG] index: 2699
[DEBUG] raw_text: Running through the 6 wit my woes #ComeTogether #BlueJays #inthe6
[DEBUG] processed_str: running through the wit my woes
[DEBUG] text_indices: [ 7  8  9 10 11 12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
[DEBUG] polarity: 1
[DEBUG] index: 15657
[DEBUG] raw_text: #TruckTuesday | | support@innovativeautoworx.com | 403.242.2767 | #Trucks #YYC #Calgary | http://t.co/ruwEqCd3LT
[DEBUG] processed_str: support
[DEBUG] text_indices: [13  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
[DEBUG] polarity: 0
[DEBUG] index: 13219
[DEBUG] raw_text: I dont even care how ridiculous this looks #OTRAToronto is officially tomorrow and I am more than ready @onedirection
[DEBUG] processed_str: i dont even care how ridiculous this looks is officially tomorrow and i am more than ready onedirection
[DEBUG] text_indices: [14 15 16 17 18 19 20 21 22 23 24 25 14 26 27 28 29 30  0  0]
[DEBUG] polarity: 0
Time taken to load Datasets/MVSA-MTS/mvsa-mts-v3/train.tsv: 2.81 seconds(0.05 minutes)
Train classes: [0, 1, 2], count=3
[DEBUG] Train label distribution:
{0: 3522, 1: 3468, 2: 6631}
-------------- Loading Datasets/MVSA-MTS/mvsa-mts-v3/val.tsv ---------------
[DEBUG] index: 18681
[DEBUG] raw_text: ***Steven thinking about the life he just left behind with his beloved, Sam. Should he have stayed?...to be continued
[DEBUG] processed_str: steven thinking about the life he just left behind with his beloved sam should he have stayed to be continued
[DEBUG] text_indices: [ 5180  1615   777     9   825   594   339  1227   652   143   220  9092
  5368   562   594   428 15160    90    50 14601]
[DEBUG] polarity: 2
[DEBUG] index: 16242
[DEBUG] raw_text: Thanks for an amazing summer #yyc,53 organizations engaged youth in 350 projects to contribute 20000 volunteer hours!
[DEBUG] processed_str: thanks for an amazing summer organizations engaged youth in projects to contribute volunteer hours
[DEBUG] text_indices: [  567    39   498    80   655 10273 14072  2198    42  3274    90 16735
  5282  1135     0     0     0     0     0     0]
[DEBUG] polarity: 2
[DEBUG] index: 9628
[DEBUG] raw_text: HSR fares go up on Tuesday. Tickets (new issue) are $2.15. Don't be overcharged! #HamOnt #HSR https://t.co/zBxyTmcy1o
[DEBUG] processed_str: hsr fares go up on tuesday tickets new issue are do be overcharged
[DEBUG] text_indices: [ 9340 14360   413   197    69   656   528    76  1129   582   489    50
 16736     0     0     0     0     0     0     0]
[DEBUG] polarity: 1
[DEBUG] index: 6350
[DEBUG] raw_text: @Calum5SOS just saw this on my Instagram feed and instantly thought of you #JetBlackHeart #ShesKindaHotVMA
[DEBUG] processed_str: just saw this on my instagram feed and instantly thought of you
[DEBUG] text_indices: [  339  1395    20    69    11  1661  3485    25 16017   385   102   283
     0     0     0     0     0     0     0     0]
[DEBUG] polarity: 0
Time taken to load Datasets/MVSA-MTS/mvsa-mts-v3/val.tsv: 0.36 seconds(0.01 minutes)
Val classes: [0, 1, 2], count=3
[DEBUG] Train label distribution:
{0: 436, 1: 442, 2: 825}
[DEBUG] Computed class_weights = [1.2891349792480469, 1.3092080354690552, 0.6847132444381714]
-------------- Loading Datasets//MVSA-MTS/mvsa-mts-v3/test.tsv ---------------
[DEBUG] index: 14949
[DEBUG] raw_text: Candid shot at #Montreal @FetishWeekend. #smile latex: @HWD_Latex #iLoveBiancaMondays http://t.co/eDaoHprlRP
[DEBUG] processed_str: candid shot at fetishweekend latex
[DEBUG] text_indices: [17897   297    75  7826 17898     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0]
[DEBUG] polarity: 2
[DEBUG] index: 9542
[DEBUG] raw_text: #hamont stop by and help out Lynwood Hall raise some founds with a car wash! #lynwoodhallcarwash #machealth
[DEBUG] processed_str: stop by and help out lynwood hall raise some founds with a car wash
[DEBUG] text_indices: [  394   243    25  1455    54 17899   723  2788    85 17900   143   139
   933  5787     0     0     0     0     0     0]
[DEBUG] polarity: 0
[DEBUG] index: 6309
[DEBUG] raw_text: EVEN MY NEICE WANTS TO VOTE #ShesKindaHotVMA
[DEBUG] processed_str: even my neice wants to vote
[DEBUG] text_indices: [   16    11 17901  3469    90   752     0     0     0     0     0     0
     0     0     0     0     0     0     0     0]
[DEBUG] polarity: 2
[DEBUG] index: 17974
[DEBUG] raw_text: Looks like I'm going alone ????
[DEBUG] processed_str: looks like i going alone
[DEBUG] text_indices: [  21  138   14  375 1954    0    0    0    0    0    0    0    0    0
    0    0    0    0    0    0]
[DEBUG] polarity: 2
Time taken to load Datasets//MVSA-MTS/mvsa-mts-v3/test.tsv: 0.36 seconds(0.01 minutes)
Test classes: [0, 1, 2], count=3
[DEBUG] Train label distribution:
{0: 450, 1: 431, 2: 822}
[DEBUG] 95th percentile sequence length across all splits: 17.00
Total Training Samples: 17027
Number of Training Samples: 13621
Number of Validation Samples: 1703
Number of Test Samples: 1703
Number of unique sentiment classes: 3
Building model
1
n_trainable_params: 7539971, n_nontrainable_params: 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
[DEBUG] text_indices.shape: torch.Size([64, 20])
[DEBUG] embedded_text.shape: torch.Size([64, 20, 200])
[DEBUG] lstm_output.shape: torch.Size([64, 20, 1536])
[DEBUG] h_n.shape: torch.Size([6, 64, 768])
[DEBUG] c_n.shape: torch.Size([6, 64, 768])
[DEBUG] text_features.shape: torch.Size([64, 1536])
[DEBUG] Sample predictions in evaluate:  tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')
[DEBUG] outputs.shape: torch.Size([64, 3])
[DEBUG] Sample of raw logits (first 5):
tensor([[-1.6029, -0.0272, -2.2284],
        [-1.5216, -0.3506, -0.4008],
        [-1.9400,  0.1780, -0.7932],
        [-0.6588,  0.6627, -1.9626],
        [-0.5506, -1.9561, -1.1064]], device='cuda:0',
       grad_fn=<SliceBackward0>)
[DEBUG] Sample of predicted probabilities (first 5):
tensor([[0.1570, 0.7590, 0.0840],
        [0.1371, 0.4423, 0.4206],
        [0.0802, 0.6672, 0.2526],
        [0.1992, 0.7467, 0.0541],
        [0.5498, 0.1348, 0.3154]], device='cuda:0', grad_fn=<SliceBackward0>)
Batch 0 completed in 0.37 seconds (0.01 minutes)
New best val_f1: 0.241513 (previous best: 0.000000)
loss: 1.230950, val_acc: 48.03% (0.480329), val_f1: 24.15% (0.241513), test_acc: 47.62% (0.476218), test_f1: 23.57% (0.235697)
Batch 60 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.318763 (previous best: 0.241513)
loss: 1.140229, val_acc: 45.80% (0.458015), val_f1: 31.88% (0.318763), test_acc: 46.33% (0.463300), test_f1: 32.94% (0.329400)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.097333, val_acc: 48.03% (0.480329), val_f1: 24.94% (0.249447), test_acc: 47.62% (0.476218), test_f1: 24.14% (0.241416)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.136278, val_acc: 26.07% (0.260716), val_f1: 14.29% (0.142858), test_acc: 26.66% (0.266588), test_f1: 14.41% (0.144058)
Epoch 0 completed in 4.45 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.853492, val_acc: 26.95% (0.269524), val_f1: 15.86% (0.158592), test_acc: 27.54% (0.275396), test_f1: 16.03% (0.160294)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.084513, val_acc: 31.59% (0.315913), val_f1: 30.80% (0.308033), test_acc: 31.83% (0.318262), test_f1: 31.14% (0.311448)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.071823, val_acc: 48.44% (0.484439), val_f1: 21.76% (0.217563), test_acc: 48.27% (0.482678), test_f1: 21.70% (0.217030)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.081605, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Epoch 1 completed in 3.99 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.861680, val_acc: 27.83% (0.278332), val_f1: 19.10% (0.190994), test_acc: 28.19% (0.281856), test_f1: 19.10% (0.190960)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.127190, val_acc: 32.00% (0.320023), val_f1: 23.12% (0.231170), test_acc: 31.71% (0.317087), test_f1: 22.88% (0.228824)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.115299, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.089268, val_acc: 45.68% (0.456841), val_f1: 31.32% (0.313233), test_acc: 48.97% (0.489724), test_f1: 34.14% (0.341446)
Epoch 2 completed in 3.99 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.723585, val_acc: 48.44% (0.484439), val_f1: 21.76% (0.217563), test_acc: 48.27% (0.482678), test_f1: 21.70% (0.217030)
Batch 60 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.326975 (previous best: 0.318763)
loss: 1.069285, val_acc: 41.69% (0.416911), val_f1: 32.70% (0.326975), test_acc: 44.33% (0.443335), test_f1: 33.85% (0.338482)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.067102, val_acc: 26.83% (0.268350), val_f1: 16.36% (0.163609), test_acc: 27.36% (0.273635), test_f1: 16.01% (0.160096)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.104779, val_acc: 25.66% (0.256606), val_f1: 13.79% (0.137941), test_acc: 26.48% (0.264827), test_f1: 14.14% (0.141379)
Epoch 3 completed in 3.99 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.209911, val_acc: 41.63% (0.416324), val_f1: 31.64% (0.316403), test_acc: 43.75% (0.437463), test_f1: 33.16% (0.331581)
Batch 60 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.334658 (previous best: 0.326975)
loss: 1.092429, val_acc: 46.33% (0.463300), val_f1: 33.47% (0.334658), test_acc: 49.56% (0.495596), test_f1: 35.46% (0.354575)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.094066, val_acc: 39.64% (0.396359), val_f1: 30.46% (0.304566), test_acc: 39.64% (0.396359), test_f1: 30.26% (0.302605)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.324588, val_acc: 26.78% (0.267763), val_f1: 18.39% (0.183853), test_acc: 27.83% (0.278332), test_f1: 18.83% (0.188308)
Epoch 4 completed in 4.00 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.243601, val_acc: 47.21% (0.472108), val_f1: 25.24% (0.252388), test_acc: 48.09% (0.480916), test_f1: 26.26% (0.262595)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.095135, val_acc: 26.25% (0.262478), val_f1: 14.17% (0.141711), test_acc: 25.66% (0.256606), test_f1: 14.05% (0.140461)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.067228, val_acc: 25.90% (0.258955), val_f1: 14.02% (0.140223), test_acc: 26.66% (0.266588), test_f1: 14.29% (0.142889)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.100373, val_acc: 26.54% (0.265414), val_f1: 14.99% (0.149895), test_acc: 27.48% (0.274809), test_f1: 15.49% (0.154948)
Epoch 5 completed in 4.00 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.847638, val_acc: 30.36% (0.303582), val_f1: 22.31% (0.223094), test_acc: 31.24% (0.312390), test_f1: 23.04% (0.230372)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.092922, val_acc: 25.90% (0.258955), val_f1: 14.02% (0.140223), test_acc: 26.83% (0.268350), test_f1: 14.55% (0.145483)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.064798, val_acc: 26.37% (0.263652), val_f1: 14.74% (0.147375), test_acc: 27.42% (0.274222), test_f1: 15.41% (0.154107)
Batch 180 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.360971 (previous best: 0.334658)
loss: 1.035941, val_acc: 36.41% (0.364063), val_f1: 36.10% (0.360971), test_acc: 36.52% (0.365238), test_f1: 36.33% (0.363316)
Epoch 6 completed in 4.00 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
Batch 0 completed in 0.01 seconds (0.00 minutes)
New best val_f1: 0.388579 (previous best: 0.360971)
loss: 2.219967, val_acc: 39.93% (0.399295), val_f1: 38.86% (0.388579), test_acc: 40.11% (0.401057), test_f1: 38.97% (0.389689)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.060322, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.079677, val_acc: 25.95% (0.259542), val_f1: 14.12% (0.141201), test_acc: 27.01% (0.270112), test_f1: 14.81% (0.148059)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.088964, val_acc: 25.90% (0.258955), val_f1: 14.02% (0.140223), test_acc: 26.60% (0.266001), test_f1: 14.20% (0.142021)
Epoch 7 completed in 4.01 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.461700, val_acc: 25.90% (0.258955), val_f1: 14.03% (0.140342), test_acc: 26.95% (0.269524), test_f1: 14.72% (0.147202)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.060888, val_acc: 25.90% (0.258955), val_f1: 14.02% (0.140223), test_acc: 26.54% (0.265414), test_f1: 14.12% (0.141151)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.998855, val_acc: 25.90% (0.258955), val_f1: 14.02% (0.140223), test_acc: 26.66% (0.266588), test_f1: 14.30% (0.142954)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.111211, val_acc: 25.84% (0.258368), val_f1: 13.94% (0.139424), test_acc: 26.60% (0.266001), test_f1: 14.23% (0.142281)
Epoch 8 completed in 4.00 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.558641, val_acc: 25.72% (0.257193), val_f1: 13.78% (0.137756), test_acc: 26.54% (0.265414), test_f1: 14.19% (0.141885)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.077582, val_acc: 25.90% (0.258955), val_f1: 14.02% (0.140223), test_acc: 26.72% (0.267176), test_f1: 14.38% (0.143756)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.067761, val_acc: 25.78% (0.257780), val_f1: 13.86% (0.138559), test_acc: 26.48% (0.264827), test_f1: 14.03% (0.140280)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.033582, val_acc: 27.13% (0.271286), val_f1: 18.55% (0.185472), test_acc: 27.60% (0.275984), test_f1: 18.34% (0.183404)
Epoch 9 completed in 4.01 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 10
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.450468, val_acc: 28.83% (0.288315), val_f1: 19.59% (0.195899), test_acc: 29.36% (0.293600), test_f1: 19.76% (0.197589)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.043574, val_acc: 26.37% (0.263652), val_f1: 16.16% (0.161559), test_acc: 26.60% (0.266001), test_f1: 15.66% (0.156596)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.052079, val_acc: 25.90% (0.258955), val_f1: 14.03% (0.140287), test_acc: 26.83% (0.268350), test_f1: 14.55% (0.145548)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.062765, val_acc: 25.84% (0.258368), val_f1: 13.95% (0.139488), test_acc: 26.66% (0.266588), test_f1: 14.37% (0.143705)
Epoch 10 completed in 4.01 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 11
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.486449, val_acc: 25.84% (0.258368), val_f1: 13.94% (0.139360), test_acc: 26.54% (0.265414), test_f1: 14.12% (0.141153)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.055554, val_acc: 25.84% (0.258368), val_f1: 13.94% (0.139360), test_acc: 26.60% (0.266001), test_f1: 14.28% (0.142819)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.012387, val_acc: 25.84% (0.258368), val_f1: 13.94% (0.139360), test_acc: 26.60% (0.266001), test_f1: 14.21% (0.142086)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.055231, val_acc: 25.84% (0.258368), val_f1: 13.94% (0.139360), test_acc: 26.54% (0.265414), test_f1: 14.13% (0.141281)
Epoch 11 completed in 4.00 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 12
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.515837, val_acc: 25.84% (0.258368), val_f1: 13.94% (0.139424), test_acc: 26.54% (0.265414), test_f1: 14.28% (0.142755)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.063219, val_acc: 25.84% (0.258368), val_f1: 13.94% (0.139360), test_acc: 26.60% (0.266001), test_f1: 14.28% (0.142758)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.043072, val_acc: 25.66% (0.256606), val_f1: 13.70% (0.136951), test_acc: 26.48% (0.264827), test_f1: 14.12% (0.141194)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.083411, val_acc: 25.78% (0.257780), val_f1: 13.86% (0.138559), test_acc: 26.60% (0.266001), test_f1: 14.27% (0.142696)
Epoch 12 completed in 4.01 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 13
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.369891, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.049890, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136080), test_acc: 26.48% (0.264827), test_f1: 14.11% (0.141071)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.135568, val_acc: 25.66% (0.256606), val_f1: 13.70% (0.136951), test_acc: 26.48% (0.264827), test_f1: 14.12% (0.141194)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.089284, val_acc: 25.60% (0.256019), val_f1: 13.75% (0.137507), test_acc: 26.48% (0.264827), test_f1: 14.26% (0.142587)
Epoch 13 completed in 4.01 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 14
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.131493, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.020573, val_acc: 26.48% (0.264827), val_f1: 15.99% (0.159919), test_acc: 26.72% (0.267176), test_f1: 15.23% (0.152252)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.027843, val_acc: 25.78% (0.257780), val_f1: 13.86% (0.138559), test_acc: 26.48% (0.264827), test_f1: 14.19% (0.141943)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.055920, val_acc: 26.01% (0.260129), val_f1: 14.36% (0.143641), test_acc: 26.83% (0.268350), test_f1: 14.64% (0.146351)
Epoch 14 completed in 4.01 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 15
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.489575, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.127348, val_acc: 26.07% (0.260716), val_f1: 14.61% (0.146055), test_acc: 26.60% (0.266001), test_f1: 14.45% (0.144452)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.040208, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.057644, val_acc: 25.66% (0.256606), val_f1: 13.70% (0.136951), test_acc: 26.48% (0.264827), test_f1: 14.12% (0.141194)
Epoch 15 completed in 4.01 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 16
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.696178, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136080), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.025343, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.036744, val_acc: 25.84% (0.258368), val_f1: 13.94% (0.139424), test_acc: 26.54% (0.265414), test_f1: 14.15% (0.141476)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.044574, val_acc: 25.66% (0.256606), val_f1: 13.69% (0.136887), test_acc: 26.54% (0.265414), test_f1: 14.19% (0.141885)
Epoch 16 completed in 4.01 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 17
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.436891, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136080), test_acc: 26.48% (0.264827), test_f1: 14.03% (0.140280)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.017639, val_acc: 25.84% (0.258368), val_f1: 13.94% (0.139360), test_acc: 26.60% (0.266001), test_f1: 14.22% (0.142216)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.000737, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136080), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.012071, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 17 completed in 4.02 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 18
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.171950, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.047558, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136080), test_acc: 26.42% (0.264240), test_f1: 13.95% (0.139470)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.084314, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.998320, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136080), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Epoch 18 completed in 4.02 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 19
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.042724, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136080), test_acc: 26.42% (0.264240), test_f1: 13.95% (0.139470)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.059545, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.067487, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.095569, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136144), test_acc: 26.42% (0.264240), test_f1: 14.10% (0.141006)
Epoch 19 completed in 4.02 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 20
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.985436, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136144), test_acc: 26.42% (0.264240), test_f1: 13.95% (0.139535)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.043802, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136080), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.054470, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.021667, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 20 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 21
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.165961, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.007540, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.022400, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.094461, val_acc: 24.78% (0.247798), val_f1: 15.49% (0.154887), test_acc: 25.84% (0.258368), test_f1: 15.60% (0.156027)
Epoch 21 completed in 4.02 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 22
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.179120, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.126816, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.002502, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.051149, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 22 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 23
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.261059, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.053849, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.106004, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.062150, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 23 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 24
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.997645, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.976470, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.039370, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.043965, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 24 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 25
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.959299, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136080), test_acc: 26.48% (0.264827), test_f1: 14.03% (0.140280)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.065421, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.126347, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.050461, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Epoch 25 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 26
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.218371, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.995512, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.023531, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.056396, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Epoch 26 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 27
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.113591, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136080), test_acc: 26.42% (0.264240), test_f1: 13.95% (0.139470)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.086222, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.065646, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.114057, val_acc: 26.48% (0.264827), val_f1: 16.23% (0.162287), test_acc: 25.84% (0.258368), test_f1: 15.66% (0.156605)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Epoch 27 completed in 4.02 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 28
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 2.285517, val_acc: 28.95% (0.289489), val_f1: 25.64% (0.256417), test_acc: 28.71% (0.287140), test_f1: 25.42% (0.254240)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.109814, val_acc: 25.60% (0.256019), val_f1: 13.65% (0.136463), test_acc: 26.48% (0.264827), test_f1: 14.14% (0.141379)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.026143, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.086647, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Epoch 28 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 29
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.505852, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.005893, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.48% (0.264827), test_f1: 14.02% (0.140215)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.094915, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.104100, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Epoch 29 completed in 4.02 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 30
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.377527, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.081824, val_acc: 26.07% (0.260716), val_f1: 13.91% (0.139114), test_acc: 25.37% (0.253670), test_f1: 13.55% (0.135519)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.098792, val_acc: 25.90% (0.258955), val_f1: 13.71% (0.137127), test_acc: 25.25% (0.252496), test_f1: 13.44% (0.134396)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.082143, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Epoch 30 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 31
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.050920, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.097550, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.103503, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.100092, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Epoch 31 completed in 4.01 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 32
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.377212, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.098076, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.098943, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.098697, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Epoch 32 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 33
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.047808, val_acc: 48.44% (0.484439), val_f1: 21.76% (0.217563), test_acc: 48.27% (0.482678), test_f1: 21.70% (0.217030)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.098853, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
[DEBUG][WARNING] attention.w_kx gradient is all zero!
[DEBUG][WARNING] attention.w_qx gradient is all zero!
[DEBUG][WARNING] attention.proj.weight gradient is all zero!
[DEBUG][WARNING] attention.proj.bias gradient is all zero!
[DEBUG][WARNING] post_att_proj.weight gradient is all zero!
[DEBUG][WARNING] post_att_proj.bias gradient is all zero!
[DEBUG][WARNING] batch_norm.weight gradient is all zero!
[DEBUG][WARNING] batch_norm.bias gradient is all zero!
[DEBUG][WARNING] hidden_layer.weight gradient is all zero!
[DEBUG][WARNING] hidden_layer.bias gradient is all zero!
[DEBUG][WARNING] classifier.weight gradient is all zero!
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.029187, val_acc: 25.72% (0.257193), val_f1: 13.76% (0.137628), test_acc: 26.48% (0.264827), test_f1: 14.02% (0.140215)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.020040, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 33 completed in 4.02 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 34
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.166065, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.091154, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.042805, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.036679, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 34 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 35
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.047864, val_acc: 25.66% (0.256606), val_f1: 13.69% (0.136887), test_acc: 26.48% (0.264827), test_f1: 14.02% (0.140215)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.020127, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.068020, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.997191, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 35 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 36
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.083279, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.018922, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.028870, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.081910, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 36 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 37
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.075699, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.025992, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.028829, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.040788, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 37 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 38
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.113128, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.098403, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.013443, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.970301, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Epoch 38 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 39
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.050361, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.051406, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.998437, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.157677, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 39 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 40
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.080317, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.063497, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.042741, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.033343, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 40 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 41
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.079036, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.026994, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.012265, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.059476, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 41 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 42
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.000031, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.058686, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.040282, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.010651, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Epoch 42 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 43
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.097883, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.030602, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.068695, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.088633, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 43 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 44
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.083789, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.999245, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.062148, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.969954, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 44 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 45
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.056227, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.085358, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.045199, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.007259, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 45 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 46
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.031396, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.041266, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.068720, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.040638, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 46 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 47
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.067053, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.028434, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.058493, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.008102, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 47 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 48
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.048391, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.998990, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.95% (0.139470)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.043783, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.961572, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 48 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 49
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.009799, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.006480, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.000284, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.995219, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 49 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 50
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.018393, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.073779, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.004550, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.966889, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 50 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 51
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.039596, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.001331, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.053897, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.000366, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 51 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 52
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.038628, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.073755, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.053634, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.030088, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 52 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 53
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.958461, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.035113, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.967519, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.996907, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 53 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 54
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.047771, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.066664, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.985644, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.039885, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 54 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 55
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.071441, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.004417, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.006545, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.923532, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 55 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 56
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.040501, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.964273, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.051589, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.95% (0.139535)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.025220, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 56 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 57
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.048699, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.017094, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.960841, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.048476, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Epoch 57 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 58
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.997839, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.017240, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.132992, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.043610, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 58 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 59
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.028560, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.101413, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.988304, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.979848, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 59 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 60
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.155234, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.003913, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.020957, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.940296, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 60 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 61
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.012657, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.040068, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.025933, val_acc: 25.66% (0.256606), val_f1: 13.81% (0.138063), test_acc: 26.19% (0.261891), test_f1: 13.92% (0.139201)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.978562, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Epoch 61 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 62
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.024801, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.984810, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.032148, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.987421, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.95% (0.139470)
Epoch 62 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 63
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.967382, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.95% (0.139470)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.998528, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136080), test_acc: 26.37% (0.263652), test_f1: 13.92% (0.139225)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.078387, val_acc: 25.72% (0.257193), val_f1: 14.15% (0.141519), test_acc: 26.19% (0.261891), test_f1: 14.36% (0.143619)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.037896, val_acc: 26.60% (0.266001), val_f1: 16.88% (0.168817), test_acc: 26.42% (0.264240), test_f1: 16.08% (0.160839)
Epoch 63 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 64
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.977077, val_acc: 25.60% (0.256019), val_f1: 13.63% (0.136271), test_acc: 26.31% (0.263065), test_f1: 13.91% (0.139109)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.947331, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.962866, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.031121, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 64 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 65
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.972617, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.981891, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.929761, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.079355, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 65 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 66
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.966534, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.954042, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.977687, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.038882, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 66 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 67
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.099620, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.062236, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.972989, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.068473, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 67 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 68
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.031209, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.101114, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.019706, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.941003, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 68 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 69
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.881630, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.946984, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.084484, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.053020, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 69 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 70
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.929443, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.012740, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.999165, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.994020, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 70 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 71
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.058083, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.911219, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.042525, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.971641, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 71 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 72
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.139524, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.983943, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.061197, val_acc: 26.95% (0.269524), val_f1: 17.73% (0.177256), test_acc: 26.25% (0.262478), test_f1: 16.32% (0.163206)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.932477, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 72 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 73
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.020165, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.979648, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.921505, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.041481, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 73 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 74
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.055863, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.135953), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.909430, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.021264, val_acc: 25.60% (0.256019), val_f1: 13.60% (0.136016), test_acc: 26.42% (0.264240), test_f1: 13.94% (0.139405)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.969487, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 74 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 75
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.911827, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.041199, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.094094, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.044265, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 75 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 76
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.995221, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.972172, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.989902, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.906919, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 76 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 77
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.998644, val_acc: 27.42% (0.274222), val_f1: 20.94% (0.209394), test_acc: 27.07% (0.270699), test_f1: 20.12% (0.201210)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.959534, val_acc: 27.30% (0.273048), val_f1: 18.83% (0.188253), test_acc: 26.60% (0.266001), test_f1: 17.21% (0.172140)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.018556, val_acc: 27.19% (0.271873), val_f1: 18.75% (0.187544), test_acc: 26.42% (0.264240), test_f1: 17.56% (0.175617)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.042188, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Epoch 77 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 78
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.007089, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.987566, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.038463, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.982639, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Epoch 78 completed in 4.04 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 79
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.944621, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.998794, val_acc: 26.07% (0.260716), val_f1: 14.33% (0.143321), test_acc: 25.48% (0.254844), test_f1: 14.06% (0.140610)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.905149, val_acc: 25.60% (0.256019), val_f1: 13.59% (0.135889), test_acc: 26.42% (0.264240), test_f1: 13.93% (0.139340)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.947096, val_acc: 28.42% (0.284204), val_f1: 25.32% (0.253207), test_acc: 29.95% (0.299472), test_f1: 26.61% (0.266120)
Epoch 79 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 80
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.995231, val_acc: 27.42% (0.274222), val_f1: 20.59% (0.205866), test_acc: 27.13% (0.271286), test_f1: 19.99% (0.199934)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.980860, val_acc: 26.72% (0.267176), val_f1: 17.15% (0.171463), test_acc: 26.31% (0.263065), test_f1: 16.13% (0.161301)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.998192, val_acc: 26.37% (0.263652), val_f1: 16.36% (0.163623), test_acc: 26.37% (0.263652), test_f1: 15.61% (0.156082)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.978406, val_acc: 27.48% (0.274809), val_f1: 20.81% (0.208109), test_acc: 26.31% (0.263065), test_f1: 20.24% (0.202406)
Epoch 80 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 81
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.036919, val_acc: 28.54% (0.285379), val_f1: 24.39% (0.243879), test_acc: 28.89% (0.288902), test_f1: 25.15% (0.251518)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.019410, val_acc: 26.19% (0.261891), val_f1: 15.64% (0.156449), test_acc: 26.07% (0.260716), test_f1: 16.43% (0.164308)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.970558, val_acc: 28.71% (0.287140), val_f1: 25.02% (0.250222), test_acc: 29.07% (0.290664), test_f1: 25.28% (0.252807)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.013134, val_acc: 25.95% (0.259542), val_f1: 13.87% (0.138713), test_acc: 25.37% (0.253670), test_f1: 13.62% (0.136247)
Epoch 81 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 82
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.944462, val_acc: 28.19% (0.281856), val_f1: 24.62% (0.246233), test_acc: 29.30% (0.293012), test_f1: 25.71% (0.257138)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.961610, val_acc: 26.07% (0.260716), val_f1: 15.10% (0.151039), test_acc: 25.72% (0.257193), test_f1: 15.27% (0.152720)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.991372, val_acc: 28.30% (0.283030), val_f1: 25.06% (0.250623), test_acc: 29.83% (0.298297), test_f1: 26.31% (0.263102)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.949153, val_acc: 27.42% (0.274222), val_f1: 20.74% (0.207401), test_acc: 27.13% (0.271286), test_f1: 20.10% (0.201023)
Epoch 82 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 83
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.981597, val_acc: 27.60% (0.275984), val_f1: 21.61% (0.216056), test_acc: 26.48% (0.264827), test_f1: 20.68% (0.206804)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.985469, val_acc: 28.77% (0.287728), val_f1: 25.17% (0.251656), test_acc: 29.71% (0.297123), test_f1: 26.36% (0.263604)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.030862, val_acc: 28.60% (0.285966), val_f1: 24.81% (0.248058), test_acc: 28.95% (0.289489), test_f1: 25.12% (0.251153)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.994911, val_acc: 27.72% (0.277158), val_f1: 21.14% (0.211377), test_acc: 26.37% (0.263652), test_f1: 20.12% (0.201185)
Epoch 83 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 84
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.057614, val_acc: 28.19% (0.281856), val_f1: 25.08% (0.250792), test_acc: 30.24% (0.302408), test_f1: 26.94% (0.269441)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.936752, val_acc: 28.89% (0.288902), val_f1: 24.27% (0.242712), test_acc: 27.13% (0.271286), test_f1: 22.56% (0.225631)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.926682, val_acc: 28.71% (0.287140), val_f1: 24.74% (0.247425), test_acc: 28.71% (0.287140), test_f1: 25.17% (0.251722)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.897495, val_acc: 27.07% (0.270699), val_f1: 18.63% (0.186332), test_acc: 25.95% (0.259542), test_f1: 18.25% (0.182461)
Epoch 84 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 85
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.978543, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.009537, val_acc: 26.07% (0.260716), val_f1: 14.20% (0.141972), test_acc: 25.43% (0.254257), test_f1: 13.90% (0.139044)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.093517, val_acc: 26.01% (0.260129), val_f1: 15.33% (0.153278), test_acc: 25.95% (0.259542), test_f1: 16.37% (0.163690)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.931627, val_acc: 28.66% (0.286553), val_f1: 25.40% (0.254045), test_acc: 30.01% (0.300059), test_f1: 26.77% (0.267689)
Epoch 85 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 86
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.982786, val_acc: 26.37% (0.263652), val_f1: 17.59% (0.175925), test_acc: 26.31% (0.263065), test_f1: 17.89% (0.178891)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.978920, val_acc: 27.95% (0.279507), val_f1: 24.85% (0.248510), test_acc: 30.06% (0.300646), test_f1: 26.73% (0.267279)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.008971, val_acc: 25.95% (0.259542), val_f1: 13.87% (0.138713), test_acc: 25.43% (0.254257), test_f1: 13.78% (0.137778)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.824204, val_acc: 26.01% (0.260129), val_f1: 14.83% (0.148283), test_acc: 25.84% (0.258368), test_f1: 14.98% (0.149754)
Epoch 86 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 87
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.050115, val_acc: 25.60% (0.256019), val_f1: 13.61% (0.136080), test_acc: 26.42% (0.264240), test_f1: 13.95% (0.139535)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.918472, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.994071, val_acc: 27.25% (0.272460), val_f1: 21.36% (0.213599), test_acc: 26.54% (0.265414), test_f1: 21.16% (0.211565)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.107863, val_acc: 26.01% (0.260129), val_f1: 16.11% (0.161070), test_acc: 26.19% (0.261891), test_f1: 16.90% (0.168992)
Epoch 87 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 88
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.011284, val_acc: 26.01% (0.260129), val_f1: 13.90% (0.138963), test_acc: 25.37% (0.253670), test_f1: 13.62% (0.136247)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.941033, val_acc: 28.24% (0.282443), val_f1: 24.30% (0.243008), test_acc: 29.01% (0.290076), test_f1: 25.43% (0.254288)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.952772, val_acc: 26.07% (0.260716), val_f1: 14.20% (0.141972), test_acc: 25.43% (0.254257), test_f1: 13.90% (0.139044)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.876365, val_acc: 26.07% (0.260716), val_f1: 14.35% (0.143474), test_acc: 25.43% (0.254257), test_f1: 13.92% (0.139151)
Epoch 88 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 89
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.932267, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.918107, val_acc: 25.90% (0.258955), val_f1: 13.99% (0.139898), test_acc: 25.48% (0.254844), test_f1: 14.08% (0.140814)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.995958, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.923873, val_acc: 27.25% (0.272460), val_f1: 20.21% (0.202076), test_acc: 26.54% (0.265414), test_f1: 19.75% (0.197527)
Epoch 89 completed in 4.02 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 90
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.963130, val_acc: 27.89% (0.278920), val_f1: 22.93% (0.229323), test_acc: 27.60% (0.275984), test_f1: 22.51% (0.225080)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.042674, val_acc: 26.66% (0.266588), val_f1: 18.22% (0.182151), test_acc: 26.42% (0.264240), test_f1: 18.53% (0.185274)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.009380, val_acc: 26.01% (0.260129), val_f1: 14.83% (0.148322), test_acc: 25.90% (0.258955), test_f1: 15.36% (0.153644)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.947905, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Epoch 90 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 91
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.977191, val_acc: 27.07% (0.270699), val_f1: 19.95% (0.199524), test_acc: 26.37% (0.263652), test_f1: 19.48% (0.194826)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.033449, val_acc: 27.01% (0.270112), val_f1: 19.64% (0.196362), test_acc: 26.07% (0.260716), test_f1: 18.82% (0.188167)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.975715, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.47% (0.134709)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.895838, val_acc: 26.01% (0.260129), val_f1: 15.20% (0.152000), test_acc: 25.95% (0.259542), test_f1: 16.06% (0.160569)
Epoch 91 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 92
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.982478, val_acc: 27.60% (0.275984), val_f1: 22.16% (0.221598), test_acc: 26.83% (0.268350), test_f1: 21.82% (0.218222)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.870209, val_acc: 25.95% (0.259542), val_f1: 13.87% (0.138713), test_acc: 25.37% (0.253670), test_f1: 13.62% (0.136247)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.998051, val_acc: 26.37% (0.263652), val_f1: 18.37% (0.183748), test_acc: 26.01% (0.260129), test_f1: 18.35% (0.183537)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.959192, val_acc: 26.07% (0.260716), val_f1: 14.34% (0.143423), test_acc: 25.43% (0.254257), test_f1: 13.92% (0.139151)
Epoch 92 completed in 4.02 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 93
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.044209, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.980296, val_acc: 26.01% (0.260129), val_f1: 14.04% (0.140407), test_acc: 25.43% (0.254257), test_f1: 13.91% (0.139098)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.946990, val_acc: 26.54% (0.265414), val_f1: 18.24% (0.182392), test_acc: 26.19% (0.261891), test_f1: 18.06% (0.180612)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.904365, val_acc: 28.77% (0.287728), val_f1: 24.66% (0.246610), test_acc: 28.54% (0.285379), test_f1: 24.83% (0.248300)
Epoch 93 completed in 4.02 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 94
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.970431, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.47% (0.134709)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.840556, val_acc: 26.48% (0.264827), val_f1: 17.94% (0.179393), test_acc: 26.25% (0.262478), test_f1: 18.02% (0.180151)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.958397, val_acc: 28.19% (0.281856), val_f1: 25.09% (0.250882), test_acc: 30.01% (0.300059), test_f1: 26.70% (0.267020)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.016447, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Epoch 94 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 95
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 1.017653, val_acc: 26.95% (0.269524), val_f1: 18.92% (0.189200), test_acc: 26.60% (0.266001), test_f1: 18.96% (0.189581)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 1.017977, val_acc: 26.01% (0.260129), val_f1: 14.03% (0.140349), test_acc: 25.43% (0.254257), test_f1: 13.78% (0.137778)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.929956, val_acc: 27.19% (0.271873), val_f1: 21.27% (0.212663), test_acc: 26.60% (0.266001), test_f1: 20.92% (0.209230)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.980145, val_acc: 26.01% (0.260129), val_f1: 14.03% (0.140292), test_acc: 25.37% (0.253670), test_f1: 13.62% (0.136247)
Epoch 95 completed in 4.02 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 96
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.895001, val_acc: 26.01% (0.260129), val_f1: 14.18% (0.141823), test_acc: 25.43% (0.254257), test_f1: 13.91% (0.139098)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.940839, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.931861, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 1.085437, val_acc: 25.95% (0.259542), val_f1: 13.87% (0.138713), test_acc: 25.31% (0.253083), test_f1: 13.47% (0.134709)
Epoch 96 completed in 4.03 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 97
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.986916, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.961183, val_acc: 26.07% (0.260716), val_f1: 14.20% (0.142026), test_acc: 25.43% (0.254257), test_f1: 13.78% (0.137778)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.970609, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.920651, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Epoch 97 completed in 4.02 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 98
Batch 0 completed in 0.01 seconds (0.00 minutes)
loss: 0.962274, val_acc: 25.90% (0.258955), val_f1: 13.71% (0.137127), test_acc: 25.37% (0.253670), test_f1: 13.62% (0.136247)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.933487, val_acc: 25.95% (0.259542), val_f1: 13.87% (0.138713), test_acc: 25.31% (0.253083), test_f1: 13.47% (0.134709)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 1.169640, val_acc: 26.01% (0.260129), val_f1: 14.46% (0.144576), test_acc: 25.60% (0.256019), test_f1: 14.62% (0.146242)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.961098, val_acc: 26.37% (0.263652), val_f1: 17.02% (0.170164), test_acc: 26.37% (0.263652), test_f1: 17.66% (0.176598)
Epoch 98 completed in 4.02 seconds (0.07 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 99
[DEBUG] Sample predictions in evaluate:  tensor([0, 0, 0, 2, 2, 1, 1, 2, 1, 0], device='cuda:0')
Batch 0 completed in 0.02 seconds (0.00 minutes)
loss: 0.988398, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Batch 60 completed in 0.01 seconds (0.00 minutes)
loss: 0.956112, val_acc: 25.95% (0.259542), val_f1: 13.87% (0.138713), test_acc: 25.31% (0.253083), test_f1: 13.47% (0.134709)
Batch 120 completed in 0.01 seconds (0.00 minutes)
loss: 0.912539, val_acc: 25.95% (0.259542), val_f1: 13.74% (0.137374), test_acc: 25.31% (0.253083), test_f1: 13.46% (0.134645)
Batch 180 completed in 0.01 seconds (0.00 minutes)
loss: 0.945077, val_acc: 25.60% (0.256019), val_f1: 13.62% (0.136207), test_acc: 26.25% (0.262478), test_f1: 13.90% (0.138993)
Epoch 99 completed in 4.03 seconds (0.07 minutes)
RESULT: Max Val F1: 0.388579, Max Test F1: 0.389689
Training complete. Generating confusion matrix on the test set.
Confusion matrix saved to /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/SIMPLE-T/2025-02-11/sub-1/010_Feb-11-2025_08:41_PM/confusion_matrix.png
Reading TensorBoard loss at each epoch:
Available tags: {'images': [], 'audio': [], 'histograms': [], 'scalars': ['Loss/train_batch', 'Loss/val_log_step', 'Loss/train_epoch', 'Loss/val_epoch'], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}
Output File: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/SIMPLE-T/2025-02-11/sub-1/010_Feb-11-2025_08:41_PM/trainval_loss_curves.png
Training and validation loss curves saved to /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/SIMPLE-T/2025-02-11/sub-1/010_Feb-11-2025_08:41_PM/trainval_loss_curves.png
Total Completion Time: 7.31 minutes. (0.12 hours) 
