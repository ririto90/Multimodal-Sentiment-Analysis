SLURM Job ID: 19686139
Number of GPUs available: 2
Logs directory: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/DMLAN/2025-01-25/sub-1/042_Jan-25-2025_02:38_PM
> training arguments:
>>> rand_seed: 8
>>> model_fusion: dmlanfusion
>>> dataset: mvsa-mts-v3
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> initializer: <function xavier_uniform_ at 0x7fa79b631440>
>>> learning_rate: 0.001
>>> dropout_rate: 0.5
>>> weight_decay: 0.0
>>> num_layers: 3
>>> num_epoch: 70
>>> batch_size: 64
>>> log_step: 60
>>> max_seq_len: 64
>>> polarities_dim: 3
>>> clip_grad: 5.0
>>> path_image: ./images
>>> crop_size: 224
>>> n_head: 8
>>> hidden_dim: 768
>>> num_classes: 3
>>> log_dir: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/DMLAN/2025-01-25/sub-1/042_Jan-25-2025_02:38_PM
>>> counter: 0
>>> model_class: <class 'models.dmlanfusion.DMLANFUSION'>
Preparing mvsa-mts-v3 dataset...
loading word vectors...
building embedding_matrix: 200_glove_embedding_matrix.dat
-------------- Loading Datasets/MVSA-MTS/mvsa-mts-v3/train.tsv ---------------
Time taken to load Datasets/MVSA-MTS/mvsa-mts-v3/train.tsv: 438.89 seconds (7.31 minutes)
The number of problematic samples: 2
-------------- Loading Datasets/MVSA-MTS/mvsa-mts-v3/train.tsv ---------------
Time taken to load Datasets/MVSA-MTS/mvsa-mts-v3/train.tsv: 88.49 seconds (1.47 minutes)
The number of problematic samples: 2
-------------- Loading Datasets/MVSA-MTS/mvsa-mts-v3/train.tsv ---------------
Time taken to load Datasets/MVSA-MTS/mvsa-mts-v3/train.tsv: 91.19 seconds (1.52 minutes)
The number of problematic samples: 2
Total Training Samples: 40863
Number of Training Samples: 13621
Number of Validation Samples: 13621
Number of Test Samples: 13621
Number of unique sentiment classes: 3
Building model
Using 2 GPUs
n_trainable_params: 2233831, n_nontrainable_params: 0
No weight decay
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
targets.shape: torch.Size([64]) targets.dtype: torch.int64
0
text_features: torch.Size([32, 64, 1536]) image_features: torch.Size([16, 2048, 8, 8])
batch_size: 16 C: 2048 H: 8 W: 8
avg_pool: torch.Size([16, 2048]) max_pool: torch.Size([16, 2048])
0
text_features: torch.Size([32, 64, 1536]) image_features: torch.Size([16, 2048, 8, 8])
batch_size: 16 C: 2048 H: 8 W: 8
avg_pool: torch.Size([16, 2048]) max_pool: torch.Size([16, 2048])
avg_pool_proj: torch.Size([16, 2048]) max_pool_proj: torch.Size([16, 2048])
channel_attention: torch.Size([16, 2048, 1, 1])
channel_refined_feature: torch.Size([16, 2048, 8, 8])
avg_pool_proj: torch.Size([16, 2048]) max_pool_proj: torch.Size([16, 2048])
channel_attention: torch.Size([16, 2048, 1, 1])
channel_refined_feature: torch.Size([16, 2048, 8, 8])
