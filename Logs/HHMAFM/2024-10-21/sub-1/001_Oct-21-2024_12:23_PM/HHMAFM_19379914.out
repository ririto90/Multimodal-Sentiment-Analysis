Number of GPUs available: 1
Logs directory: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/HHMAFM/2024-10-21/sub-1/001_Oct-21-2024_12:23_PM
/home/rgg2706/Multimodal-Sentiment-Analysis/Logs/HHMAFM/2024-10-21/sub-1/001_Oct-21-2024_12:23_PM
> training arguments:
>>> rand_seed: 8
>>> model_name: mmfusion
>>> dataset: mvsa-mts-1000
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> initializer: <function xavier_uniform_ at 0x7f9e137b1b20>
>>> learning_rate: 0.001
>>> dropout_rate: 0.5
>>> num_epoch: 10
>>> batch_size: 64
>>> log_step: 30
>>> max_seq_len: 64
>>> polarities_dim: 3
>>> clip_grad: 5.0
>>> path_image: ./Datasets/MVSA-MTS/images-indexed
>>> crop_size: 224
>>> n_head: 8
>>> roberta_text_feature_dim: 768
>>> roberta_topic_feature_dim: 50
>>> resnet_feature_dim: 2048
>>> densenet_feature_dim: 1024
>>> common_dim: 512
>>> num_classes: 3
>>> model_class: <class 'models.mmfusion.MMFUSION'>
Preparing mvsa-mts-1000 dataset...
-------------- Loading Datasets/MVSA-MTS/mvsa-mts-1000/train.tsv ---------------
Time taken to load Datasets/MVSA-MTS/mvsa-mts-1000/train.tsv: 47.33 seconds (0.79 minutes)
The number of problematic samples: 69
-------------- Loading Datasets/MVSA-MTS/mvsa-mts-1000/val.tsv ---------------
Time taken to load Datasets/MVSA-MTS/mvsa-mts-1000/val.tsv: 12.03 seconds (0.20 minutes)
The number of problematic samples: 10
-------------- Loading Datasets/MVSA-MTS/mvsa-mts-1000/test.tsv ---------------
Time taken to load Datasets/MVSA-MTS/mvsa-mts-1000/test.tsv: 13.63 seconds (0.23 minutes)
The number of problematic samples: 6
Total Training Samples: 1000
Number of Training Samples: 800
Number of Development Samples: 100
Number of Test Samples: 100
Number of unique sentiment classes: 3
building model
n_trainable_params: 43766787, n_nontrainable_params: 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
Batch 0 completed in 2.52 seconds (0.04 minutes)
max_dev_f1: 0, max_test_f1: 0
loss: 1.116304, dev_acc: 42.00% (0.420000), dev_f1: 19.72% (0.197183), test_acc: 48.00% (0.480000), test_f1: 21.62% (0.216216)
Epoch 0 completed in 11.48 seconds (0.19 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
Batch 0 completed in 0.56 seconds (0.01 minutes)
max_dev_f1: 0.19718309859154928, max_test_f1: 0.21621621621621623
loss: 1.121555, dev_acc: 45.00% (0.450000), dev_f1: 29.23% (0.292348), test_acc: 50.00% (0.500000), test_f1: 27.25% (0.272511)
Epoch 1 completed in 7.98 seconds (0.13 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
Batch 0 completed in 0.57 seconds (0.01 minutes)
max_dev_f1: 0.2923479398299542, max_test_f1: 0.27251141552511415
loss: 0.850972, dev_acc: 45.00% (0.450000), dev_f1: 34.91% (0.349063), test_acc: 48.00% (0.480000), test_f1: 31.43% (0.314296)
Epoch 2 completed in 8.02 seconds (0.13 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
Batch 0 completed in 0.57 seconds (0.01 minutes)
max_dev_f1: 0.34906299195617957, max_test_f1: 0.31429624714296245
loss: 0.545598, dev_acc: 53.00% (0.530000), dev_f1: 46.58% (0.465828), test_acc: 50.00% (0.500000), test_f1: 38.09% (0.380853)
Epoch 3 completed in 8.04 seconds (0.13 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
Batch 0 completed in 0.57 seconds (0.01 minutes)
loss: 0.301015, dev_acc: 42.00% (0.420000), dev_f1: 35.16% (0.351624), test_acc: 33.00% (0.330000), test_f1: 25.52% (0.255231)
Epoch 4 completed in 8.03 seconds (0.13 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
Batch 0 completed in 0.57 seconds (0.01 minutes)
loss: 0.303014, dev_acc: 45.00% (0.450000), dev_f1: 40.75% (0.407524), test_acc: 44.00% (0.440000), test_f1: 38.06% (0.380562)
Epoch 5 completed in 7.97 seconds (0.13 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
Batch 0 completed in 0.57 seconds (0.01 minutes)
loss: 0.358622, dev_acc: 43.00% (0.430000), dev_f1: 39.80% (0.398006), test_acc: 34.00% (0.340000), test_f1: 26.76% (0.267644)
Epoch 6 completed in 8.01 seconds (0.13 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
Batch 0 completed in 0.57 seconds (0.01 minutes)
max_dev_f1: 0.4658278471892931, max_test_f1: 0.38085304455210406
loss: 0.226390, dev_acc: 55.00% (0.550000), dev_f1: 54.22% (0.542177), test_acc: 33.00% (0.330000), test_f1: 26.78% (0.267795)
Epoch 7 completed in 8.12 seconds (0.14 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
Batch 0 completed in 0.57 seconds (0.01 minutes)
loss: 0.233845, dev_acc: 45.00% (0.450000), dev_f1: 42.20% (0.422015), test_acc: 37.00% (0.370000), test_f1: 29.81% (0.298056)
Epoch 8 completed in 8.11 seconds (0.14 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
Batch 0 completed in 0.58 seconds (0.01 minutes)
loss: 0.158425, dev_acc: 53.00% (0.530000), dev_f1: 50.40% (0.503968), test_acc: 49.00% (0.490000), test_f1: 42.21% (0.422101)
Epoch 9 completed in 8.03 seconds (0.13 minutes)
Max dev F1: 54.22% (0.542177), Max test F1: 26.78% (0.267795)
Reading TensorBoard loss at each epoch:
Available tags: {'images': [], 'audio': [], 'histograms': [], 'scalars': ['Loss/train', 'Loss/val'], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}
Step: 1, Loss: 1.1163043975830078
Step: 2, Loss: 24.18794059753418
Step: 3, Loss: 26.22214126586914
Step: 4, Loss: 9.952237129211426
Step: 5, Loss: 5.990220069885254
Step: 6, Loss: 9.814908027648926
Step: 7, Loss: 3.7257165908813477
Step: 8, Loss: 1.7792980670928955
Step: 9, Loss: 5.017425060272217
Step: 10, Loss: 2.5023138523101807
Step: 11, Loss: 1.4130476713180542
Step: 12, Loss: 1.4725916385650635
Step: 13, Loss: 1.387998342514038
Step: 14, Loss: 1.1215546131134033
Step: 15, Loss: 1.292523980140686
Step: 16, Loss: 0.8986297845840454
Step: 17, Loss: 0.9678246378898621
Step: 18, Loss: 0.9385924935340881
Step: 19, Loss: 0.9277528524398804
Step: 20, Loss: 1.1672919988632202
Step: 21, Loss: 0.849677324295044
Step: 22, Loss: 1.0545425415039062
Step: 23, Loss: 1.114125370979309
Step: 24, Loss: 0.9228934645652771
Step: 25, Loss: 0.91185063123703
Step: 26, Loss: 0.8613698482513428
Step: 27, Loss: 0.8509724736213684
Step: 28, Loss: 0.7587093114852905
Step: 29, Loss: 0.6755572557449341
Step: 30, Loss: 0.7752745151519775
Step: 31, Loss: 0.6782063245773315
Step: 32, Loss: 0.6715921759605408
Step: 33, Loss: 0.6567221879959106
Step: 34, Loss: 0.9736025333404541
Step: 35, Loss: 0.7514464259147644
Step: 36, Loss: 0.7406731843948364
Step: 37, Loss: 0.705127477645874
Step: 38, Loss: 0.6825255751609802
Step: 39, Loss: 0.7321400046348572
Step: 40, Loss: 0.5455982089042664
Step: 41, Loss: 0.528530478477478
Step: 42, Loss: 0.565942108631134
Step: 43, Loss: 0.38816016912460327
Step: 44, Loss: 0.49440905451774597
Step: 45, Loss: 0.5773328542709351
Step: 46, Loss: 0.5216418504714966
Step: 47, Loss: 0.4822612702846527
Step: 48, Loss: 0.4759733974933624
Step: 49, Loss: 0.5178267359733582
Step: 50, Loss: 0.6412929892539978
Step: 51, Loss: 0.7153326272964478
Step: 52, Loss: 0.5584213137626648
Step: 53, Loss: 0.3010152578353882
Step: 54, Loss: 0.2987079620361328
Step: 55, Loss: 0.4580039978027344
Step: 56, Loss: 0.3927004337310791
Step: 57, Loss: 0.5595704913139343
Step: 58, Loss: 0.37777987122535706
Step: 59, Loss: 0.35091689229011536
Step: 60, Loss: 0.33967018127441406
Step: 61, Loss: 0.3986680507659912
Step: 62, Loss: 0.6223357915878296
Step: 63, Loss: 0.5516930818557739
Step: 64, Loss: 0.3058137893676758
Step: 65, Loss: 0.7923296093940735
Step: 66, Loss: 0.3030143976211548
Step: 67, Loss: 0.36398056149482727
Step: 68, Loss: 0.20836693048477173
Step: 69, Loss: 0.30500948429107666
Step: 70, Loss: 0.2518232464790344
Step: 71, Loss: 0.35923442244529724
Step: 72, Loss: 0.2621780335903168
Step: 73, Loss: 0.3601130545139313
Step: 74, Loss: 0.3676658868789673
Step: 75, Loss: 0.472308486700058
Step: 76, Loss: 0.49888181686401367
Step: 77, Loss: 0.5218010544776917
Step: 78, Loss: 0.5722932815551758
Step: 79, Loss: 0.35862210392951965
Step: 80, Loss: 0.2147536277770996
Step: 81, Loss: 0.3396846652030945
Step: 82, Loss: 0.27906113862991333
Step: 83, Loss: 0.5469191670417786
Step: 84, Loss: 0.5085853338241577
Step: 85, Loss: 0.44684267044067383
Step: 86, Loss: 0.5277198553085327
Step: 87, Loss: 0.3775932192802429
Step: 88, Loss: 0.7831592559814453
Step: 89, Loss: 0.6343134045600891
Step: 90, Loss: 0.38932448625564575
Step: 91, Loss: 0.2298562228679657
Step: 92, Loss: 0.22638975083827972
Step: 93, Loss: 0.3449455201625824
Step: 94, Loss: 0.25651639699935913
Step: 95, Loss: 0.6080840229988098
Step: 96, Loss: 0.2700844407081604
Step: 97, Loss: 0.43767088651657104
Step: 98, Loss: 0.5920224785804749
Step: 99, Loss: 0.4192068874835968
Step: 100, Loss: 0.26912984251976013
Step: 101, Loss: 0.42645758390426636
Step: 102, Loss: 0.26626357436180115
Step: 103, Loss: 0.5771487355232239
Step: 104, Loss: 0.338847279548645
Step: 105, Loss: 0.23384496569633484
Step: 106, Loss: 0.44174280762672424
Step: 107, Loss: 0.308216392993927
Step: 108, Loss: 0.42788952589035034
Step: 109, Loss: 0.13979458808898926
Step: 110, Loss: 0.2540203928947449
Step: 111, Loss: 0.33156609535217285
Step: 112, Loss: 0.4120533764362335
Step: 113, Loss: 0.22375957667827606
Step: 114, Loss: 0.45991504192352295
Step: 115, Loss: 0.25779226422309875
Step: 116, Loss: 0.3506219685077667
Step: 117, Loss: 0.5255563259124756
Step: 118, Loss: 0.15842494368553162
Step: 119, Loss: 0.2746478319168091
Step: 120, Loss: 0.15251289308071136
Step: 121, Loss: 0.24954669177532196
Step: 122, Loss: 0.40558770298957825
Step: 123, Loss: 0.15774843096733093
Step: 124, Loss: 0.21095789968967438
Step: 125, Loss: 0.19207489490509033
Step: 126, Loss: 0.20046861469745636
Step: 127, Loss: 0.18317626416683197
Step: 128, Loss: 0.2182561457157135
Step: 129, Loss: 0.22821559011936188
Step: 130, Loss: 0.5027462244033813
Output File: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/HHMAFM/2024-10-21/sub-1/001_Oct-21-2024_12:23_PM/trainval_loss_curves.png
Training and validation loss curves saved to /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/HHMAFM/2024-10-21/sub-1/001_Oct-21-2024_12:23_PM/trainval_loss_curves.png
Total Completion Time: 3.03 minutes. (0.05 hours) 
