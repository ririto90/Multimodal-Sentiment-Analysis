Number of GPUs available: 2
Logs directory: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/HHMAFM/2024-10-19/029_Oct-19-2024_04:25_PM
/home/rgg2706/Multimodal-Sentiment-Analysis/Logs/HHMAFM/2024-10-19/029_Oct-19-2024_04:25_PM
> training arguments:
>>> rand_seed: 8
>>> model_name: cmhafusion
>>> dataset: mvsa-mts
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> initializer: <function xavier_uniform_ at 0x7f5056685a80>
>>> learning_rate: 0.001
>>> dropout_rate: 0.5
>>> num_epoch: 10
>>> batch_size: 128
>>> log_step: 10
>>> max_seq_len: 64
>>> polarities_dim: 3
>>> clip_grad: 5.0
>>> path_image: ./Datasets/MVSA-MTS/images-indexed
>>> crop_size: 224
>>> roberta_text_feature_dim: 768
>>> roberta_topic_feature_dim: 50
>>> resnet_feature_dim: 2048
>>> densenet_feature_dim: 1024
>>> common_dim: 512
>>> num_classes: 3
>>> model_class: <class 'models.cmhafusion.CMHAFUSION'>
Preparing mvsa-mts dataset...
-------------- Loading Datasets/MVSA-MTS/mvsa-mts/train.tsv ---------------
Time taken to load Datasets/MVSA-MTS/mvsa-mts/train.tsv: 46.26 seconds (0.77 minutes)
The number of problematic samples: 402
-------------- Loading Datasets/MVSA-MTS/mvsa-mts/val.tsv ---------------
Time taken to load Datasets/MVSA-MTS/mvsa-mts/val.tsv: 15.29 seconds (0.25 minutes)
The number of problematic samples: 136
-------------- Loading Datasets/MVSA-MTS/mvsa-mts/test.tsv ---------------
Time taken to load Datasets/MVSA-MTS/mvsa-mts/test.tsv: 15.51 seconds (0.26 minutes)
The number of problematic samples: 142
Total Training Samples: 19600
Number of Training Samples: 11760
Number of Development Samples: 3920
Number of Test Samples: 3920
Number of unique sentiment classes: 3
building model
Using 2 GPUs
n_trainable_params: 5228035, n_nontrainable_params: 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
Batch 0 completed in 3.51 seconds (0.06 minutes)
max_dev_f1: 0, max_test_f1: 0
loss: 1.117018, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 10 completed in 1.22 seconds (0.02 minutes)
loss: 1.020195, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 20 completed in 1.22 seconds (0.02 minutes)
loss: 1.067070, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 30 completed in 1.22 seconds (0.02 minutes)
loss: 1.059257, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 40 completed in 1.22 seconds (0.02 minutes)
loss: 1.012382, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 50 completed in 1.21 seconds (0.02 minutes)
loss: 1.074882, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 60 completed in 1.21 seconds (0.02 minutes)
loss: 1.059257, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 70 completed in 1.22 seconds (0.02 minutes)
loss: 1.028007, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 80 completed in 1.21 seconds (0.02 minutes)
loss: 0.981132, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 90 completed in 1.21 seconds (0.02 minutes)
loss: 0.988945, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Epoch 0 completed in 592.65 seconds (9.88 minutes), avg loss: 1.019153
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
Batch 0 completed in 1.22 seconds (0.02 minutes)
loss: 1.043632, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 10 completed in 1.21 seconds (0.02 minutes)
loss: 1.020195, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 20 completed in 1.21 seconds (0.02 minutes)
loss: 1.004570, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 30 completed in 1.21 seconds (0.02 minutes)
loss: 0.981132, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 40 completed in 1.22 seconds (0.02 minutes)
loss: 0.949882, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 50 completed in 1.21 seconds (0.02 minutes)
loss: 1.051445, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 60 completed in 1.55 seconds (0.03 minutes)
loss: 1.090507, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 70 completed in 1.64 seconds (0.03 minutes)
loss: 1.020195, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 80 completed in 1.22 seconds (0.02 minutes)
loss: 1.028007, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 90 completed in 1.21 seconds (0.02 minutes)
loss: 1.051445, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Epoch 1 completed in 594.31 seconds (9.91 minutes), avg loss: 1.017393
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
Batch 0 completed in 1.22 seconds (0.02 minutes)
loss: 1.051445, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 10 completed in 1.22 seconds (0.02 minutes)
loss: 1.059257, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 20 completed in 1.22 seconds (0.02 minutes)
loss: 0.965507, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 30 completed in 1.21 seconds (0.02 minutes)
loss: 0.988945, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 40 completed in 1.22 seconds (0.02 minutes)
loss: 1.020195, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 50 completed in 1.21 seconds (0.02 minutes)
loss: 1.004570, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 60 completed in 1.21 seconds (0.02 minutes)
loss: 0.988945, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 70 completed in 1.21 seconds (0.02 minutes)
loss: 1.051445, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 80 completed in 1.22 seconds (0.02 minutes)
loss: 0.996757, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 90 completed in 1.21 seconds (0.02 minutes)
loss: 1.043632, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Epoch 2 completed in 594.99 seconds (9.92 minutes), avg loss: 1.017296
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
Batch 0 completed in 1.22 seconds (0.02 minutes)
loss: 1.074882, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 10 completed in 1.21 seconds (0.02 minutes)
loss: 0.988945, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 20 completed in 1.21 seconds (0.02 minutes)
loss: 1.106132, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 30 completed in 1.21 seconds (0.02 minutes)
loss: 0.957695, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 40 completed in 1.21 seconds (0.02 minutes)
loss: 0.981132, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 50 completed in 1.21 seconds (0.02 minutes)
loss: 0.981132, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 60 completed in 1.22 seconds (0.02 minutes)
loss: 1.067070, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 70 completed in 1.21 seconds (0.02 minutes)
loss: 1.043632, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 80 completed in 1.21 seconds (0.02 minutes)
loss: 1.090507, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 90 completed in 1.21 seconds (0.02 minutes)
loss: 0.988945, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Epoch 3 completed in 592.83 seconds (9.88 minutes), avg loss: 1.017441
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
Batch 0 completed in 1.22 seconds (0.02 minutes)
loss: 0.934257, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 10 completed in 1.21 seconds (0.02 minutes)
loss: 1.090507, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 20 completed in 1.21 seconds (0.02 minutes)
loss: 0.957695, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 30 completed in 1.21 seconds (0.02 minutes)
loss: 1.035820, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 40 completed in 1.65 seconds (0.03 minutes)
loss: 0.965507, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 50 completed in 1.22 seconds (0.02 minutes)
loss: 1.051445, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 60 completed in 1.22 seconds (0.02 minutes)
loss: 0.981132, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 70 completed in 1.21 seconds (0.02 minutes)
loss: 1.004570, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 80 completed in 1.21 seconds (0.02 minutes)
loss: 1.098320, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 90 completed in 1.21 seconds (0.02 minutes)
loss: 1.035820, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Epoch 4 completed in 593.94 seconds (9.90 minutes), avg loss: 1.017259
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
Batch 0 completed in 1.22 seconds (0.02 minutes)
loss: 1.004570, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 10 completed in 1.21 seconds (0.02 minutes)
loss: 0.973320, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 20 completed in 1.21 seconds (0.02 minutes)
loss: 0.949882, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 30 completed in 1.22 seconds (0.02 minutes)
loss: 0.988945, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 40 completed in 1.22 seconds (0.02 minutes)
loss: 1.004570, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 50 completed in 1.21 seconds (0.02 minutes)
loss: 0.957695, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 60 completed in 1.21 seconds (0.02 minutes)
loss: 1.020195, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 70 completed in 1.21 seconds (0.02 minutes)
loss: 0.981132, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 80 completed in 1.21 seconds (0.02 minutes)
loss: 0.949882, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 90 completed in 1.21 seconds (0.02 minutes)
loss: 1.035820, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Epoch 5 completed in 594.99 seconds (9.92 minutes), avg loss: 1.017320
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
Batch 0 completed in 1.22 seconds (0.02 minutes)
loss: 1.043632, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 10 completed in 1.21 seconds (0.02 minutes)
loss: 0.981132, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 20 completed in 1.21 seconds (0.02 minutes)
loss: 1.035820, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 30 completed in 1.21 seconds (0.02 minutes)
loss: 0.996757, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 40 completed in 1.22 seconds (0.02 minutes)
loss: 0.973320, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 50 completed in 1.22 seconds (0.02 minutes)
loss: 1.020195, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 60 completed in 1.21 seconds (0.02 minutes)
loss: 1.004570, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 70 completed in 1.21 seconds (0.02 minutes)
loss: 1.043632, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 80 completed in 1.21 seconds (0.02 minutes)
loss: 1.051445, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 90 completed in 1.21 seconds (0.02 minutes)
loss: 0.957695, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Epoch 6 completed in 593.64 seconds (9.89 minutes), avg loss: 1.017186
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
Batch 0 completed in 1.22 seconds (0.02 minutes)
loss: 0.942070, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 10 completed in 1.22 seconds (0.02 minutes)
loss: 1.113945, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 20 completed in 1.21 seconds (0.02 minutes)
loss: 0.988945, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 30 completed in 1.21 seconds (0.02 minutes)
loss: 1.043632, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 40 completed in 1.21 seconds (0.02 minutes)
loss: 1.067070, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 50 completed in 1.21 seconds (0.02 minutes)
loss: 1.059257, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 60 completed in 1.21 seconds (0.02 minutes)
loss: 1.043632, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 70 completed in 1.21 seconds (0.02 minutes)
loss: 0.981132, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 80 completed in 1.21 seconds (0.02 minutes)
loss: 1.035820, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 90 completed in 1.21 seconds (0.02 minutes)
loss: 1.028007, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Epoch 7 completed in 594.58 seconds (9.91 minutes), avg loss: 1.017271
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
Batch 0 completed in 1.21 seconds (0.02 minutes)
loss: 1.067070, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 10 completed in 1.22 seconds (0.02 minutes)
loss: 1.074882, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 20 completed in 1.21 seconds (0.02 minutes)
loss: 1.051445, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 30 completed in 1.21 seconds (0.02 minutes)
loss: 1.035820, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 40 completed in 1.21 seconds (0.02 minutes)
loss: 1.082695, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 50 completed in 1.22 seconds (0.02 minutes)
loss: 1.020195, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 60 completed in 1.21 seconds (0.02 minutes)
loss: 1.043632, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 70 completed in 1.21 seconds (0.02 minutes)
loss: 1.012382, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 80 completed in 1.21 seconds (0.02 minutes)
loss: 1.020195, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 90 completed in 1.21 seconds (0.02 minutes)
loss: 1.043632, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Epoch 8 completed in 592.12 seconds (9.87 minutes), avg loss: 1.017368
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
Batch 0 completed in 1.22 seconds (0.02 minutes)
loss: 0.965507, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 10 completed in 1.22 seconds (0.02 minutes)
loss: 1.035820, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 20 completed in 1.21 seconds (0.02 minutes)
loss: 0.934257, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 30 completed in 1.21 seconds (0.02 minutes)
loss: 1.012382, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 40 completed in 1.21 seconds (0.02 minutes)
loss: 1.035820, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 50 completed in 1.21 seconds (0.02 minutes)
loss: 1.004570, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 60 completed in 1.21 seconds (0.02 minutes)
loss: 0.988945, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 70 completed in 1.21 seconds (0.02 minutes)
loss: 0.988945, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 80 completed in 1.21 seconds (0.02 minutes)
loss: 0.988945, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Batch 90 completed in 1.21 seconds (0.02 minutes)
loss: 0.965507, dev_acc: 52.91% (0.529082), dev_f1: 23.07% (0.230675), test_acc: 53.78% (0.537755), test_f1: 23.31% (0.233134)
Epoch 9 completed in 592.52 seconds (9.88 minutes), avg loss: 1.017332
Max dev F1: 23.07% (0.230675), Max test F1: 23.31% (0.233134)
Reading TensorBoard loss at each epoch:
Available tags: {'images': [], 'audio': [], 'histograms': [], 'scalars': ['Loss/train', 'Loss/epoch'], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}
Step: 0, Loss: 1.1170178651809692
Step: 1, Loss: 1.0713282823562622
Step: 2, Loss: 1.0380606651306152
Step: 3, Loss: 0.9181194305419922
Step: 4, Loss: 1.0358141660690308
Step: 5, Loss: 1.0201947689056396
Step: 6, Loss: 1.0045698881149292
Step: 7, Loss: 1.0045698881149292
Step: 8, Loss: 0.965507447719574
Step: 9, Loss: 1.0123823881149292
Step: 10, Loss: 1.0201948881149292
Step: 11, Loss: 1.0280073881149292
Step: 12, Loss: 0.996757447719574
Step: 13, Loss: 1.0592573881149292
Step: 14, Loss: 0.996757447719574
Step: 15, Loss: 1.0514448881149292
Step: 16, Loss: 1.0045698881149292
Step: 17, Loss: 0.981132447719574
Step: 18, Loss: 1.0201948881149292
Step: 19, Loss: 1.0201948881149292
Step: 20, Loss: 1.0670698881149292
Step: 21, Loss: 0.996757447719574
Step: 22, Loss: 0.965507447719574
Step: 23, Loss: 1.0123823881149292
Step: 24, Loss: 1.0514448881149292
Step: 25, Loss: 1.0358198881149292
Step: 26, Loss: 1.0436323881149292
Step: 27, Loss: 1.0280073881149292
Step: 28, Loss: 1.0201948881149292
Step: 29, Loss: 0.965507447719574
Step: 30, Loss: 1.0592573881149292
Step: 31, Loss: 1.0123823881149292
Step: 32, Loss: 1.0358198881149292
Step: 33, Loss: 1.0358198881149292
Step: 34, Loss: 0.926444947719574
Step: 35, Loss: 1.0436323881149292
Step: 36, Loss: 1.0045698881149292
Step: 37, Loss: 1.0592573881149292
Step: 38, Loss: 1.0280073881149292
Step: 39, Loss: 1.0045698881149292
Step: 40, Loss: 1.0123823881149292
Step: 41, Loss: 0.942069947719574
Step: 42, Loss: 1.0748823881149292
Step: 43, Loss: 1.0592573881149292
Step: 44, Loss: 1.0905073881149292
Step: 45, Loss: 1.0905073881149292
Step: 46, Loss: 1.0280073881149292
Step: 47, Loss: 0.957694947719574
Step: 48, Loss: 1.0280073881149292
Step: 49, Loss: 1.0514448881149292
Step: 50, Loss: 1.0748823881149292
Step: 51, Loss: 1.0123823881149292
Step: 52, Loss: 1.0358198881149292
Step: 53, Loss: 1.0436323881149292
Step: 54, Loss: 0.965507447719574
Step: 55, Loss: 0.965507447719574
Step: 56, Loss: 0.981132447719574
Step: 57, Loss: 1.0280073881149292
Step: 58, Loss: 0.942069947719574
Step: 59, Loss: 1.0436323881149292
Step: 60, Loss: 1.0592573881149292
Step: 61, Loss: 0.981132447719574
Step: 62, Loss: 0.949882447719574
Step: 63, Loss: 0.981132447719574
Step: 64, Loss: 0.965507447719574
Step: 65, Loss: 1.0201948881149292
Step: 66, Loss: 1.0358198881149292
Step: 67, Loss: 1.0280073881149292
Step: 68, Loss: 1.0826948881149292
Step: 69, Loss: 1.0045698881149292
Step: 70, Loss: 1.0280073881149292
Step: 71, Loss: 1.0123823881149292
Step: 72, Loss: 1.0201948881149292
Step: 73, Loss: 1.0592573881149292
Step: 74, Loss: 1.0514448881149292
Step: 75, Loss: 0.988944947719574
Step: 76, Loss: 1.0201948881149292
Step: 77, Loss: 0.996757447719574
Step: 78, Loss: 0.934257447719574
Step: 79, Loss: 0.988944947719574
Step: 80, Loss: 0.981132447719574
Step: 81, Loss: 1.0826948881149292
Step: 82, Loss: 1.0045698881149292
Step: 83, Loss: 1.0748823881149292
Step: 84, Loss: 0.965507447719574
Step: 85, Loss: 0.981132447719574
Step: 86, Loss: 1.0826948881149292
Step: 87, Loss: 1.0436323881149292
Step: 88, Loss: 1.0670698881149292
Step: 89, Loss: 1.0670698881149292
Step: 90, Loss: 0.988944947719574
Step: 91, Loss: 1.024659276008606
Step: 92, Loss: 1.0436323881149292
Step: 93, Loss: 0.996757447719574
Step: 94, Loss: 1.0123823881149292
Step: 95, Loss: 1.0045698881149292
Step: 96, Loss: 0.981132447719574
Step: 97, Loss: 1.0280073881149292
Step: 98, Loss: 1.0358198881149292
Step: 99, Loss: 1.0514448881149292
Step: 100, Loss: 0.965507447719574
Step: 101, Loss: 1.0201948881149292
Step: 102, Loss: 1.0201948881149292
Step: 103, Loss: 0.988944947719574
Step: 104, Loss: 1.0748823881149292
Step: 105, Loss: 0.981132447719574
Step: 106, Loss: 0.973319947719574
Step: 107, Loss: 1.0201948881149292
Step: 108, Loss: 1.0358198881149292
Step: 109, Loss: 0.926444947719574
Step: 110, Loss: 0.988944947719574
Step: 111, Loss: 0.996757447719574
Step: 112, Loss: 1.0045698881149292
Step: 113, Loss: 1.0358198881149292
Step: 114, Loss: 1.0905073881149292
Step: 115, Loss: 0.996757447719574
Step: 116, Loss: 0.965507447719574
Step: 117, Loss: 1.0592573881149292
Step: 118, Loss: 1.0358198881149292
Step: 119, Loss: 1.0123823881149292
Step: 120, Loss: 0.942069947719574
Step: 121, Loss: 1.0983198881149292
Step: 122, Loss: 0.981132447719574
Step: 123, Loss: 0.934257447719574
Step: 124, Loss: 1.0201948881149292
Step: 125, Loss: 0.996757447719574
Step: 126, Loss: 1.0514448881149292
Step: 127, Loss: 1.0358198881149292
Step: 128, Loss: 1.0826948881149292
Step: 129, Loss: 1.0358198881149292
Step: 130, Loss: 1.0592573881149292
Step: 131, Loss: 0.949882447719574
Step: 132, Loss: 0.949882447719574
Step: 133, Loss: 1.0045698881149292
Step: 134, Loss: 1.0358198881149292
Step: 135, Loss: 1.0670698881149292
Step: 136, Loss: 0.996757447719574
Step: 137, Loss: 1.0201948881149292
Step: 138, Loss: 1.0983198881149292
Step: 139, Loss: 1.0358198881149292
Step: 140, Loss: 1.0358198881149292
Step: 141, Loss: 1.0045698881149292
Step: 142, Loss: 1.0514448881149292
Step: 143, Loss: 1.0514448881149292
Step: 144, Loss: 0.981132447719574
Step: 145, Loss: 1.0123823881149292
Step: 146, Loss: 1.0592573881149292
Step: 147, Loss: 1.0280073881149292
Step: 148, Loss: 1.0201948881149292
Step: 149, Loss: 1.0592573881149292
Step: 150, Loss: 1.0045698881149292
Step: 151, Loss: 1.0358198881149292
Step: 152, Loss: 1.0905073881149292
Step: 153, Loss: 1.0436323881149292
Step: 154, Loss: 1.0201948881149292
Step: 155, Loss: 0.965507447719574
Step: 156, Loss: 0.988944947719574
Step: 157, Loss: 1.0358198881149292
Step: 158, Loss: 0.981132447719574
Step: 159, Loss: 0.996757447719574
Step: 160, Loss: 1.0045698881149292
Step: 161, Loss: 1.0592573881149292
Step: 162, Loss: 1.0201948881149292
Step: 163, Loss: 1.0514448881149292
Step: 164, Loss: 1.0201948881149292
Step: 165, Loss: 1.0201948881149292
Step: 166, Loss: 0.949882447719574
Step: 167, Loss: 0.996757447719574
Step: 168, Loss: 1.0045698881149292
Step: 169, Loss: 0.996757447719574
Step: 170, Loss: 1.0358198881149292
Step: 171, Loss: 1.0748823881149292
Step: 172, Loss: 1.0280073881149292
Step: 173, Loss: 1.0280073881149292
Step: 174, Loss: 0.973319947719574
Step: 175, Loss: 1.0592573881149292
Step: 176, Loss: 0.981132447719574
Step: 177, Loss: 0.988944947719574
Step: 178, Loss: 0.949882447719574
Step: 179, Loss: 1.0358198881149292
Step: 180, Loss: 0.988944947719574
Step: 181, Loss: 1.0201948881149292
Step: 182, Loss: 1.0514448881149292
Step: 183, Loss: 1.0514448881149292
Step: 184, Loss: 1.0514448881149292
Step: 185, Loss: 0.996757447719574
Step: 186, Loss: 1.0358198881149292
Step: 187, Loss: 1.0592573881149292
Step: 188, Loss: 1.0123823881149292
Step: 189, Loss: 1.0045698881149292
Step: 190, Loss: 0.965507447719574
Step: 191, Loss: 1.0670698881149292
Step: 192, Loss: 0.957694947719574
Step: 193, Loss: 0.973319947719574
Step: 194, Loss: 1.0592573881149292
Step: 195, Loss: 1.0045698881149292
Step: 196, Loss: 1.0436323881149292
Step: 197, Loss: 1.0670698881149292
Step: 198, Loss: 1.0123823881149292
Step: 199, Loss: 1.0670698881149292
Step: 200, Loss: 1.0514448881149292
Step: 201, Loss: 1.0983198881149292
Step: 202, Loss: 1.0045698881149292
Step: 203, Loss: 0.957694947719574
Step: 204, Loss: 0.965507447719574
Step: 205, Loss: 0.910819947719574
Step: 206, Loss: 0.965507447719574
Step: 207, Loss: 1.0123823881149292
Step: 208, Loss: 0.996757447719574
Step: 209, Loss: 1.0201948881149292
Step: 210, Loss: 1.0123823881149292
Step: 211, Loss: 0.965507447719574
Step: 212, Loss: 0.973319947719574
Step: 213, Loss: 1.0280073881149292
Step: 214, Loss: 0.988944947719574
Step: 215, Loss: 1.0280073881149292
Step: 216, Loss: 1.0514448881149292
Step: 217, Loss: 0.981132447719574
Step: 218, Loss: 1.0123823881149292
Step: 219, Loss: 1.0201948881149292
Step: 220, Loss: 1.0436323881149292
Step: 221, Loss: 1.0905073881149292
Step: 222, Loss: 1.0436323881149292
Step: 223, Loss: 0.981132447719574
Step: 224, Loss: 1.0201948881149292
Step: 225, Loss: 0.973319947719574
Step: 226, Loss: 0.988944947719574
Step: 227, Loss: 1.0592573881149292
Step: 228, Loss: 1.0358198881149292
Step: 229, Loss: 1.0045698881149292
Step: 230, Loss: 1.0358198881149292
Step: 231, Loss: 0.996757447719574
Step: 232, Loss: 0.973319947719574
Step: 233, Loss: 1.0905073881149292
Step: 234, Loss: 1.0045698881149292
Step: 235, Loss: 1.0123823881149292
Step: 236, Loss: 1.0280073881149292
Step: 237, Loss: 0.910819947719574
Step: 238, Loss: 1.0670698881149292
Step: 239, Loss: 0.957694947719574
Step: 240, Loss: 1.0201948881149292
Step: 241, Loss: 1.0670698881149292
Step: 242, Loss: 1.1139448881149292
Step: 243, Loss: 1.0592573881149292
Step: 244, Loss: 0.988944947719574
Step: 245, Loss: 1.0280073881149292
Step: 246, Loss: 1.0436323881149292
Step: 247, Loss: 0.957694947719574
Step: 248, Loss: 1.0201948881149292
Step: 249, Loss: 1.0670698881149292
Step: 250, Loss: 1.0358198881149292
Step: 251, Loss: 1.0358198881149292
Step: 252, Loss: 0.996757447719574
Step: 253, Loss: 0.965507447719574
Step: 254, Loss: 1.0514448881149292
Step: 255, Loss: 0.957694947719574
Step: 256, Loss: 1.0514448881149292
Step: 257, Loss: 1.0280073881149292
Step: 258, Loss: 0.988944947719574
Step: 259, Loss: 0.996757447719574
Step: 260, Loss: 1.0514448881149292
Step: 261, Loss: 1.0123823881149292
Step: 262, Loss: 1.0905073881149292
Step: 263, Loss: 0.965507447719574
Step: 264, Loss: 0.996757447719574
Step: 265, Loss: 1.0358198881149292
Step: 266, Loss: 1.0358198881149292
Step: 267, Loss: 1.0905073881149292
Step: 268, Loss: 0.910819947719574
Step: 269, Loss: 1.0670698881149292
Step: 270, Loss: 1.0592573881149292
Step: 271, Loss: 1.0280073881149292
Step: 272, Loss: 0.988944947719574
Step: 273, Loss: 1.0201948881149292
Step: 274, Loss: 1.0436323881149292
Step: 275, Loss: 0.9800163507461548
Step: 276, Loss: 1.0748823881149292
Step: 277, Loss: 1.0201948881149292
Step: 278, Loss: 1.0514448881149292
Step: 279, Loss: 1.0514448881149292
Step: 280, Loss: 1.0358198881149292
Step: 281, Loss: 1.0592573881149292
Step: 282, Loss: 1.0436323881149292
Step: 283, Loss: 1.0358198881149292
Step: 284, Loss: 1.0280073881149292
Step: 285, Loss: 1.0358198881149292
Step: 286, Loss: 0.988944947719574
Step: 287, Loss: 1.0514448881149292
Step: 288, Loss: 1.0358198881149292
Step: 289, Loss: 1.0045698881149292
Step: 290, Loss: 1.0123823881149292
Step: 291, Loss: 1.0123823881149292
Step: 292, Loss: 1.0201948881149292
Step: 293, Loss: 0.973319947719574
Step: 294, Loss: 1.0201948881149292
Step: 295, Loss: 1.0123823881149292
Step: 296, Loss: 1.1061323881149292
Step: 297, Loss: 1.1139448881149292
Step: 298, Loss: 0.934257447719574
Step: 299, Loss: 1.0905073881149292
Step: 300, Loss: 0.957694947719574
Step: 301, Loss: 1.0358198881149292
Step: 302, Loss: 1.0358198881149292
Step: 303, Loss: 1.0358198881149292
Step: 304, Loss: 1.0045698881149292
Step: 305, Loss: 1.0826948881149292
Step: 306, Loss: 0.957694947719574
Step: 307, Loss: 1.0826948881149292
Step: 308, Loss: 1.0123823881149292
Step: 309, Loss: 0.949882447719574
Step: 310, Loss: 1.0045698881149292
Step: 311, Loss: 0.988944947719574
Step: 312, Loss: 1.0514448881149292
Step: 313, Loss: 0.934257447719574
Step: 314, Loss: 1.0045698881149292
Step: 315, Loss: 1.0592573881149292
Step: 316, Loss: 0.981132447719574
Step: 317, Loss: 1.0358198881149292
Step: 318, Loss: 0.981132447719574
Step: 319, Loss: 0.981132447719574
Step: 320, Loss: 0.996757447719574
Step: 321, Loss: 1.0045698881149292
Step: 322, Loss: 1.0592573881149292
Step: 323, Loss: 1.0201948881149292
Step: 324, Loss: 1.0123823881149292
Step: 325, Loss: 0.988944947719574
Step: 326, Loss: 0.981132447719574
Step: 327, Loss: 1.0045698881149292
Step: 328, Loss: 0.988944947719574
Step: 329, Loss: 0.973319947719574
Step: 330, Loss: 0.926444947719574
Step: 331, Loss: 0.965507447719574
Step: 332, Loss: 1.0592573881149292
Step: 333, Loss: 1.0201948881149292
Step: 334, Loss: 1.0280073881149292
Step: 335, Loss: 1.0358198881149292
Step: 336, Loss: 1.0670698881149292
Step: 337, Loss: 1.0201948881149292
Step: 338, Loss: 1.0280073881149292
Step: 339, Loss: 1.0514448881149292
Step: 340, Loss: 0.996757447719574
Step: 341, Loss: 1.0436323881149292
Step: 342, Loss: 1.0280073881149292
Step: 343, Loss: 1.0045698881149292
Step: 344, Loss: 0.942069947719574
Step: 345, Loss: 0.973319947719574
Step: 346, Loss: 1.0436323881149292
Step: 347, Loss: 0.988944947719574
Step: 348, Loss: 0.973319947719574
Step: 349, Loss: 1.0201948881149292
Step: 350, Loss: 1.0436323881149292
Step: 351, Loss: 0.957694947719574
Step: 352, Loss: 0.981132447719574
Step: 353, Loss: 0.957694947719574
Step: 354, Loss: 1.0592573881149292
Step: 355, Loss: 1.0514448881149292
Step: 356, Loss: 1.0905073881149292
Step: 357, Loss: 1.0280073881149292
Step: 358, Loss: 1.0670698881149292
Step: 359, Loss: 0.973319947719574
Step: 360, Loss: 0.973319947719574
Step: 361, Loss: 1.0670698881149292
Step: 362, Loss: 0.957694947719574
Step: 363, Loss: 1.0670698881149292
Step: 364, Loss: 1.0201948881149292
Step: 365, Loss: 0.996757447719574
Step: 366, Loss: 0.988944947719574
Step: 367, Loss: 1.087159276008606
Step: 368, Loss: 0.934257447719574
Step: 369, Loss: 1.0436323881149292
Step: 370, Loss: 0.949882447719574
Step: 371, Loss: 0.988944947719574
Step: 372, Loss: 1.0748823881149292
Step: 373, Loss: 0.981132447719574
Step: 374, Loss: 1.0514448881149292
Step: 375, Loss: 1.0280073881149292
Step: 376, Loss: 1.0045698881149292
Step: 377, Loss: 1.0201948881149292
Step: 378, Loss: 1.0905073881149292
Step: 379, Loss: 1.0826948881149292
Step: 380, Loss: 1.0748823881149292
Step: 381, Loss: 1.0201948881149292
Step: 382, Loss: 0.965507447719574
Step: 383, Loss: 1.0201948881149292
Step: 384, Loss: 1.0436323881149292
Step: 385, Loss: 1.0983198881149292
Step: 386, Loss: 1.0436323881149292
Step: 387, Loss: 0.988944947719574
Step: 388, Loss: 0.957694947719574
Step: 389, Loss: 1.0514448881149292
Step: 390, Loss: 0.934257447719574
Step: 391, Loss: 1.0280073881149292
Step: 392, Loss: 1.0201948881149292
Step: 393, Loss: 0.965507447719574
Step: 394, Loss: 1.0045698881149292
Step: 395, Loss: 1.0201948881149292
Step: 396, Loss: 0.996757447719574
Step: 397, Loss: 1.0905073881149292
Step: 398, Loss: 1.0358198881149292
Step: 399, Loss: 1.0436323881149292
Step: 400, Loss: 0.949882447719574
Step: 401, Loss: 0.973319947719574
Step: 402, Loss: 0.988944947719574
Step: 403, Loss: 0.988944947719574
Step: 404, Loss: 1.0436323881149292
Step: 405, Loss: 1.0358198881149292
Step: 406, Loss: 0.926444947719574
Step: 407, Loss: 1.0280073881149292
Step: 408, Loss: 0.965507447719574
Step: 409, Loss: 0.988944947719574
Step: 410, Loss: 1.0670698881149292
Step: 411, Loss: 0.957694947719574
Step: 412, Loss: 0.996757447719574
Step: 413, Loss: 0.981132447719574
Step: 414, Loss: 0.903007447719574
Step: 415, Loss: 0.988944947719574
Step: 416, Loss: 1.0358198881149292
Step: 417, Loss: 0.965507447719574
Step: 418, Loss: 1.0514448881149292
Step: 419, Loss: 1.0201948881149292
Step: 420, Loss: 1.0280073881149292
Step: 421, Loss: 1.0358198881149292
Step: 422, Loss: 1.0826948881149292
Step: 423, Loss: 0.996757447719574
Step: 424, Loss: 1.0826948881149292
Step: 425, Loss: 0.973319947719574
Step: 426, Loss: 1.0748823881149292
Step: 427, Loss: 1.0358198881149292
Step: 428, Loss: 0.981132447719574
Step: 429, Loss: 1.0201948881149292
Step: 430, Loss: 0.996757447719574
Step: 431, Loss: 0.988944947719574
Step: 432, Loss: 0.957694947719574
Step: 433, Loss: 0.942069947719574
Step: 434, Loss: 1.0045698881149292
Step: 435, Loss: 1.1061323881149292
Step: 436, Loss: 0.988944947719574
Step: 437, Loss: 1.0280073881149292
Step: 438, Loss: 1.0045698881149292
Step: 439, Loss: 0.957694947719574
Step: 440, Loss: 1.0826948881149292
Step: 441, Loss: 1.0045698881149292
Step: 442, Loss: 1.0983198881149292
Step: 443, Loss: 1.0358198881149292
Step: 444, Loss: 1.0358198881149292
Step: 445, Loss: 1.0670698881149292
Step: 446, Loss: 0.996757447719574
Step: 447, Loss: 0.918632447719574
Step: 448, Loss: 1.0983198881149292
Step: 449, Loss: 1.0748823881149292
Step: 450, Loss: 1.0045698881149292
Step: 451, Loss: 1.0201948881149292
Step: 452, Loss: 1.0592573881149292
Step: 453, Loss: 1.0905073881149292
Step: 454, Loss: 1.0514448881149292
Step: 455, Loss: 1.0436323881149292
Step: 456, Loss: 1.0514448881149292
Step: 457, Loss: 1.0670698881149292
Step: 458, Loss: 1.0358198881149292
Step: 459, Loss: 0.9532306790351868
Step: 460, Loss: 1.0045698881149292
Step: 461, Loss: 0.957694947719574
Step: 462, Loss: 1.0123823881149292
Step: 463, Loss: 0.949882447719574
Step: 464, Loss: 0.895194947719574
Step: 465, Loss: 1.0045698881149292
Step: 466, Loss: 0.988944947719574
Step: 467, Loss: 1.0670698881149292
Step: 468, Loss: 0.981132447719574
Step: 469, Loss: 0.934257447719574
Step: 470, Loss: 0.973319947719574
Step: 471, Loss: 1.0905073881149292
Step: 472, Loss: 1.0280073881149292
Step: 473, Loss: 0.996757447719574
Step: 474, Loss: 0.988944947719574
Step: 475, Loss: 1.0280073881149292
Step: 476, Loss: 1.0201948881149292
Step: 477, Loss: 1.0670698881149292
Step: 478, Loss: 0.988944947719574
Step: 479, Loss: 1.0670698881149292
Step: 480, Loss: 0.949882447719574
Step: 481, Loss: 1.0280073881149292
Step: 482, Loss: 1.0748823881149292
Step: 483, Loss: 1.0123823881149292
Step: 484, Loss: 1.0436323881149292
Step: 485, Loss: 1.0514448881149292
Step: 486, Loss: 1.0045698881149292
Step: 487, Loss: 0.996757447719574
Step: 488, Loss: 1.0592573881149292
Step: 489, Loss: 1.0670698881149292
Step: 490, Loss: 0.988944947719574
Step: 491, Loss: 1.0826948881149292
Step: 492, Loss: 1.0748823881149292
Step: 493, Loss: 1.0436323881149292
Step: 494, Loss: 1.0201948881149292
Step: 495, Loss: 1.0592573881149292
Step: 496, Loss: 1.0436323881149292
Step: 497, Loss: 1.0436323881149292
Step: 498, Loss: 0.988944947719574
Step: 499, Loss: 1.0436323881149292
Step: 500, Loss: 1.0045698881149292
Step: 501, Loss: 1.0670698881149292
Step: 502, Loss: 0.934257447719574
Step: 503, Loss: 1.0670698881149292
Step: 504, Loss: 1.0045698881149292
Step: 505, Loss: 1.0748823881149292
Step: 506, Loss: 0.934257447719574
Step: 507, Loss: 0.879569947719574
Step: 508, Loss: 1.0358198881149292
Step: 509, Loss: 1.0514448881149292
Step: 510, Loss: 0.957694947719574
Step: 511, Loss: 1.0748823881149292
Step: 512, Loss: 1.0358198881149292
Step: 513, Loss: 0.957694947719574
Step: 514, Loss: 1.0670698881149292
Step: 515, Loss: 1.0123823881149292
Step: 516, Loss: 1.0592573881149292
Step: 517, Loss: 1.0826948881149292
Step: 518, Loss: 0.957694947719574
Step: 519, Loss: 1.0592573881149292
Step: 520, Loss: 1.0201948881149292
Step: 521, Loss: 1.0201948881149292
Step: 522, Loss: 1.0045698881149292
Step: 523, Loss: 0.957694947719574
Step: 524, Loss: 0.996757447719574
Step: 525, Loss: 1.0514448881149292
Step: 526, Loss: 0.996757447719574
Step: 527, Loss: 0.973319947719574
Step: 528, Loss: 1.0905073881149292
Step: 529, Loss: 1.0123823881149292
Step: 530, Loss: 0.981132447719574
Step: 531, Loss: 1.0201948881149292
Step: 532, Loss: 1.0592573881149292
Step: 533, Loss: 1.0670698881149292
Step: 534, Loss: 1.0280073881149292
Step: 535, Loss: 1.0514448881149292
Step: 536, Loss: 1.0280073881149292
Step: 537, Loss: 0.996757447719574
Step: 538, Loss: 1.0201948881149292
Step: 539, Loss: 0.981132447719574
Step: 540, Loss: 0.949882447719574
Step: 541, Loss: 0.965507447719574
Step: 542, Loss: 1.0280073881149292
Step: 543, Loss: 1.0358198881149292
Step: 544, Loss: 1.0748823881149292
Step: 545, Loss: 0.996757447719574
Step: 546, Loss: 1.0436323881149292
Step: 547, Loss: 1.0045698881149292
Step: 548, Loss: 1.0514448881149292
Step: 549, Loss: 1.0123823881149292
Step: 550, Loss: 1.0358198881149292
Step: 551, Loss: 0.9978734850883484
Step: 552, Loss: 1.0436323881149292
Step: 553, Loss: 1.0592573881149292
Step: 554, Loss: 1.0280073881149292
Step: 555, Loss: 1.0358198881149292
Step: 556, Loss: 0.957694947719574
Step: 557, Loss: 1.0123823881149292
Step: 558, Loss: 1.0280073881149292
Step: 559, Loss: 0.934257447719574
Step: 560, Loss: 0.957694947719574
Step: 561, Loss: 1.0670698881149292
Step: 562, Loss: 0.981132447719574
Step: 563, Loss: 0.996757447719574
Step: 564, Loss: 0.988944947719574
Step: 565, Loss: 1.0592573881149292
Step: 566, Loss: 0.926444947719574
Step: 567, Loss: 1.0670698881149292
Step: 568, Loss: 0.942069947719574
Step: 569, Loss: 1.0826948881149292
Step: 570, Loss: 0.965507447719574
Step: 571, Loss: 1.0436323881149292
Step: 572, Loss: 1.0358198881149292
Step: 573, Loss: 1.0123823881149292
Step: 574, Loss: 1.0670698881149292
Step: 575, Loss: 1.1373823881149292
Step: 576, Loss: 1.0514448881149292
Step: 577, Loss: 1.1295698881149292
Step: 578, Loss: 1.0123823881149292
Step: 579, Loss: 1.0358198881149292
Step: 580, Loss: 1.0748823881149292
Step: 581, Loss: 1.0514448881149292
Step: 582, Loss: 0.996757447719574
Step: 583, Loss: 1.0123823881149292
Step: 584, Loss: 1.0201948881149292
Step: 585, Loss: 1.0436323881149292
Step: 586, Loss: 1.0280073881149292
Step: 587, Loss: 1.0201948881149292
Step: 588, Loss: 1.0358198881149292
Step: 589, Loss: 0.973319947719574
Step: 590, Loss: 0.942069947719574
Step: 591, Loss: 1.0436323881149292
Step: 592, Loss: 0.973319947719574
Step: 593, Loss: 1.0358198881149292
Step: 594, Loss: 1.0201948881149292
Step: 595, Loss: 1.0201948881149292
Step: 596, Loss: 1.0201948881149292
Step: 597, Loss: 1.0045698881149292
Step: 598, Loss: 1.0826948881149292
Step: 599, Loss: 1.0436323881149292
Step: 600, Loss: 0.895194947719574
Step: 601, Loss: 1.1061323881149292
Step: 602, Loss: 1.0201948881149292
Step: 603, Loss: 0.988944947719574
Step: 604, Loss: 0.988944947719574
Step: 605, Loss: 1.0045698881149292
Step: 606, Loss: 1.0826948881149292
Step: 607, Loss: 0.949882447719574
Step: 608, Loss: 0.981132447719574
Step: 609, Loss: 0.988944947719574
Step: 610, Loss: 1.0123823881149292
Step: 611, Loss: 1.0436323881149292
Step: 612, Loss: 1.0045698881149292
Step: 613, Loss: 1.0436323881149292
Step: 614, Loss: 1.0592573881149292
Step: 615, Loss: 1.0045698881149292
Step: 616, Loss: 0.934257447719574
Step: 617, Loss: 1.0045698881149292
Step: 618, Loss: 0.981132447719574
Step: 619, Loss: 1.0201948881149292
Step: 620, Loss: 1.0280073881149292
Step: 621, Loss: 0.895194947719574
Step: 622, Loss: 1.0436323881149292
Step: 623, Loss: 1.0201948881149292
Step: 624, Loss: 1.1061323881149292
Step: 625, Loss: 0.996757447719574
Step: 626, Loss: 0.965507447719574
Step: 627, Loss: 1.0514448881149292
Step: 628, Loss: 1.0123823881149292
Step: 629, Loss: 0.981132447719574
Step: 630, Loss: 1.0436323881149292
Step: 631, Loss: 0.996757447719574
Step: 632, Loss: 1.0514448881149292
Step: 633, Loss: 1.0280073881149292
Step: 634, Loss: 1.1139448881149292
Step: 635, Loss: 1.0045698881149292
Step: 636, Loss: 1.0280073881149292
Step: 637, Loss: 1.0201948881149292
Step: 638, Loss: 1.0983198881149292
Step: 639, Loss: 0.957694947719574
Step: 640, Loss: 0.949882447719574
Step: 641, Loss: 1.1139448881149292
Step: 642, Loss: 0.957694947719574
Step: 643, Loss: 0.8996592164039612
Step: 644, Loss: 0.942069947719574
Step: 645, Loss: 1.0280073881149292
Step: 646, Loss: 1.0201948881149292
Step: 647, Loss: 1.0514448881149292
Step: 648, Loss: 0.981132447719574
Step: 649, Loss: 1.0201948881149292
Step: 650, Loss: 0.973319947719574
Step: 651, Loss: 1.0436323881149292
Step: 652, Loss: 1.0748823881149292
Step: 653, Loss: 1.0905073881149292
Step: 654, Loss: 1.1139448881149292
Step: 655, Loss: 1.0514448881149292
Step: 656, Loss: 0.957694947719574
Step: 657, Loss: 1.0201948881149292
Step: 658, Loss: 1.0592573881149292
Step: 659, Loss: 1.0358198881149292
Step: 660, Loss: 0.965507447719574
Step: 661, Loss: 1.0280073881149292
Step: 662, Loss: 0.996757447719574
Step: 663, Loss: 1.0592573881149292
Step: 664, Loss: 0.988944947719574
Step: 665, Loss: 1.0280073881149292
Step: 666, Loss: 1.0123823881149292
Step: 667, Loss: 1.0045698881149292
Step: 668, Loss: 1.0826948881149292
Step: 669, Loss: 0.949882447719574
Step: 670, Loss: 1.0280073881149292
Step: 671, Loss: 0.988944947719574
Step: 672, Loss: 0.988944947719574
Step: 673, Loss: 1.0436323881149292
Step: 674, Loss: 1.0436323881149292
Step: 675, Loss: 0.957694947719574
Step: 676, Loss: 0.965507447719574
Step: 677, Loss: 1.1061323881149292
Step: 678, Loss: 1.0123823881149292
Step: 679, Loss: 1.0592573881149292
Step: 680, Loss: 0.942069947719574
Step: 681, Loss: 1.0436323881149292
Step: 682, Loss: 1.0123823881149292
Step: 683, Loss: 0.996757447719574
Step: 684, Loss: 1.0670698881149292
Step: 685, Loss: 1.0748823881149292
Step: 686, Loss: 0.981132447719574
Step: 687, Loss: 0.973319947719574
Step: 688, Loss: 0.981132447719574
Step: 689, Loss: 1.0436323881149292
Step: 690, Loss: 0.965507447719574
Step: 691, Loss: 0.942069947719574
Step: 692, Loss: 1.0592573881149292
Step: 693, Loss: 0.949882447719574
Step: 694, Loss: 1.0592573881149292
Step: 695, Loss: 0.996757447719574
Step: 696, Loss: 1.0201948881149292
Step: 697, Loss: 0.988944947719574
Step: 698, Loss: 1.0983198881149292
Step: 699, Loss: 1.0123823881149292
Step: 700, Loss: 1.0748823881149292
Step: 701, Loss: 0.934257447719574
Step: 702, Loss: 1.0592573881149292
Step: 703, Loss: 1.0514448881149292
Step: 704, Loss: 1.0436323881149292
Step: 705, Loss: 1.0123823881149292
Step: 706, Loss: 1.0592573881149292
Step: 707, Loss: 1.0514448881149292
Step: 708, Loss: 1.0358198881149292
Step: 709, Loss: 0.988944947719574
Step: 710, Loss: 1.0123823881149292
Step: 711, Loss: 1.0592573881149292
Step: 712, Loss: 0.996757447719574
Step: 713, Loss: 0.965507447719574
Step: 714, Loss: 0.981132447719574
Step: 715, Loss: 0.988944947719574
Step: 716, Loss: 1.0358198881149292
Step: 717, Loss: 1.1061323881149292
Step: 718, Loss: 1.0358198881149292
Step: 719, Loss: 1.0123823881149292
Step: 720, Loss: 1.0045698881149292
Step: 721, Loss: 1.0201948881149292
Step: 722, Loss: 0.988944947719574
Step: 723, Loss: 0.981132447719574
Step: 724, Loss: 1.0358198881149292
Step: 725, Loss: 1.0045698881149292
Step: 726, Loss: 0.942069947719574
Step: 727, Loss: 0.942069947719574
Step: 728, Loss: 0.988944947719574
Step: 729, Loss: 0.981132447719574
Step: 730, Loss: 1.0436323881149292
Step: 731, Loss: 1.0670698881149292
Step: 732, Loss: 1.0514448881149292
Step: 733, Loss: 1.0592573881149292
Step: 734, Loss: 1.0280073881149292
Step: 735, Loss: 0.9621592164039612
Step: 736, Loss: 1.0670698881149292
Step: 737, Loss: 0.996757447719574
Step: 738, Loss: 0.910819947719574
Step: 739, Loss: 1.0592573881149292
Step: 740, Loss: 1.0280073881149292
Step: 741, Loss: 1.0123823881149292
Step: 742, Loss: 0.988944947719574
Step: 743, Loss: 0.981132447719574
Step: 744, Loss: 1.0826948881149292
Step: 745, Loss: 0.996757447719574
Step: 746, Loss: 1.0748823881149292
Step: 747, Loss: 0.973319947719574
Step: 748, Loss: 1.0280073881149292
Step: 749, Loss: 1.0670698881149292
Step: 750, Loss: 1.0436323881149292
Step: 751, Loss: 0.973319947719574
Step: 752, Loss: 1.0123823881149292
Step: 753, Loss: 0.988944947719574
Step: 754, Loss: 0.996757447719574
Step: 755, Loss: 1.0670698881149292
Step: 756, Loss: 1.0514448881149292
Step: 757, Loss: 0.996757447719574
Step: 758, Loss: 1.0592573881149292
Step: 759, Loss: 0.996757447719574
Step: 760, Loss: 1.1217573881149292
Step: 761, Loss: 0.996757447719574
Step: 762, Loss: 1.0436323881149292
Step: 763, Loss: 1.0826948881149292
Step: 764, Loss: 1.0045698881149292
Step: 765, Loss: 1.0748823881149292
Step: 766, Loss: 1.0358198881149292
Step: 767, Loss: 1.0670698881149292
Step: 768, Loss: 1.0826948881149292
Step: 769, Loss: 1.1061323881149292
Step: 770, Loss: 0.965507447719574
Step: 771, Loss: 1.0436323881149292
Step: 772, Loss: 1.0045698881149292
Step: 773, Loss: 0.981132447719574
Step: 774, Loss: 1.0358198881149292
Step: 775, Loss: 0.996757447719574
Step: 776, Loss: 1.0826948881149292
Step: 777, Loss: 1.0280073881149292
Step: 778, Loss: 0.926444947719574
Step: 779, Loss: 1.0045698881149292
Step: 780, Loss: 0.973319947719574
Step: 781, Loss: 0.996757447719574
Step: 782, Loss: 1.0123823881149292
Step: 783, Loss: 0.981132447719574
Step: 784, Loss: 1.0045698881149292
Step: 785, Loss: 0.973319947719574
Step: 786, Loss: 1.0201948881149292
Step: 787, Loss: 0.910819947719574
Step: 788, Loss: 1.0436323881149292
Step: 789, Loss: 1.0045698881149292
Step: 790, Loss: 0.996757447719574
Step: 791, Loss: 0.965507447719574
Step: 792, Loss: 1.0045698881149292
Step: 793, Loss: 1.0358198881149292
Step: 794, Loss: 1.0358198881149292
Step: 795, Loss: 1.0514448881149292
Step: 796, Loss: 1.0436323881149292
Step: 797, Loss: 0.981132447719574
Step: 798, Loss: 1.0045698881149292
Step: 799, Loss: 1.0045698881149292
Step: 800, Loss: 1.0592573881149292
Step: 801, Loss: 1.0280073881149292
Step: 802, Loss: 1.0123823881149292
Step: 803, Loss: 1.0436323881149292
Step: 804, Loss: 1.0045698881149292
Step: 805, Loss: 1.0670698881149292
Step: 806, Loss: 1.0123823881149292
Step: 807, Loss: 1.1061323881149292
Step: 808, Loss: 0.981132447719574
Step: 809, Loss: 0.949882447719574
Step: 810, Loss: 1.0358198881149292
Step: 811, Loss: 0.957694947719574
Step: 812, Loss: 1.0123823881149292
Step: 813, Loss: 0.981132447719574
Step: 814, Loss: 0.965507447719574
Step: 815, Loss: 1.0670698881149292
Step: 816, Loss: 1.0201948881149292
Step: 817, Loss: 0.973319947719574
Step: 818, Loss: 1.0358198881149292
Step: 819, Loss: 1.0045698881149292
Step: 820, Loss: 1.0280073881149292
Step: 821, Loss: 0.996757447719574
Step: 822, Loss: 0.996757447719574
Step: 823, Loss: 1.0045698881149292
Step: 824, Loss: 0.965507447719574
Step: 825, Loss: 1.0280073881149292
Step: 826, Loss: 1.0436323881149292
Step: 827, Loss: 1.0335878133773804
Step: 828, Loss: 0.965507447719574
Step: 829, Loss: 0.996757447719574
Step: 830, Loss: 1.0280073881149292
Step: 831, Loss: 1.0045698881149292
Step: 832, Loss: 1.0358198881149292
Step: 833, Loss: 0.973319947719574
Step: 834, Loss: 1.0358198881149292
Step: 835, Loss: 1.0123823881149292
Step: 836, Loss: 1.0436323881149292
Step: 837, Loss: 0.973319947719574
Step: 838, Loss: 1.0358198881149292
Step: 839, Loss: 0.973319947719574
Step: 840, Loss: 1.0436323881149292
Step: 841, Loss: 1.0045698881149292
Step: 842, Loss: 1.0123823881149292
Step: 843, Loss: 1.0123823881149292
Step: 844, Loss: 1.0123823881149292
Step: 845, Loss: 0.957694947719574
Step: 846, Loss: 1.0514448881149292
Step: 847, Loss: 1.0123823881149292
Step: 848, Loss: 0.934257447719574
Step: 849, Loss: 1.0905073881149292
Step: 850, Loss: 1.0201948881149292
Step: 851, Loss: 1.0436323881149292
Step: 852, Loss: 0.949882447719574
Step: 853, Loss: 0.996757447719574
Step: 854, Loss: 1.0826948881149292
Step: 855, Loss: 1.0201948881149292
Step: 856, Loss: 0.981132447719574
Step: 857, Loss: 1.0436323881149292
Step: 858, Loss: 1.0123823881149292
Step: 859, Loss: 1.0201948881149292
Step: 860, Loss: 1.0123823881149292
Step: 861, Loss: 1.0748823881149292
Step: 862, Loss: 1.0748823881149292
Step: 863, Loss: 0.996757447719574
Step: 864, Loss: 1.0280073881149292
Step: 865, Loss: 0.965507447719574
Step: 866, Loss: 1.0983198881149292
Step: 867, Loss: 1.0358198881149292
Step: 868, Loss: 1.0358198881149292
Step: 869, Loss: 0.942069947719574
Step: 870, Loss: 1.0592573881149292
Step: 871, Loss: 1.0201948881149292
Step: 872, Loss: 0.981132447719574
Step: 873, Loss: 1.0358198881149292
Step: 874, Loss: 0.973319947719574
Step: 875, Loss: 1.0045698881149292
Step: 876, Loss: 0.996757447719574
Step: 877, Loss: 1.0748823881149292
Step: 878, Loss: 1.0045698881149292
Step: 879, Loss: 1.0201948881149292
Step: 880, Loss: 0.981132447719574
Step: 881, Loss: 1.1217573881149292
Step: 882, Loss: 0.965507447719574
Step: 883, Loss: 1.0592573881149292
Step: 884, Loss: 1.0045698881149292
Step: 885, Loss: 1.0045698881149292
Step: 886, Loss: 0.996757447719574
Step: 887, Loss: 1.0358198881149292
Step: 888, Loss: 0.988944947719574
Step: 889, Loss: 0.942069947719574
Step: 890, Loss: 1.0123823881149292
Step: 891, Loss: 0.957694947719574
Step: 892, Loss: 1.0592573881149292
Step: 893, Loss: 0.996757447719574
Step: 894, Loss: 1.0358198881149292
Step: 895, Loss: 1.0280073881149292
Step: 896, Loss: 0.988944947719574
Step: 897, Loss: 1.0670698881149292
Step: 898, Loss: 0.988944947719574
Step: 899, Loss: 1.0670698881149292
Step: 900, Loss: 1.0592573881149292
Step: 901, Loss: 0.988944947719574
Step: 902, Loss: 0.996757447719574
Step: 903, Loss: 1.0280073881149292
Step: 904, Loss: 1.0045698881149292
Step: 905, Loss: 1.0592573881149292
Step: 906, Loss: 1.0748823881149292
Step: 907, Loss: 0.973319947719574
Step: 908, Loss: 0.988944947719574
Step: 909, Loss: 1.0983198881149292
Step: 910, Loss: 1.0514448881149292
Step: 911, Loss: 1.0748823881149292
Step: 912, Loss: 1.0358198881149292
Step: 913, Loss: 1.0201948881149292
Step: 914, Loss: 0.988944947719574
Step: 915, Loss: 1.0358198881149292
Step: 916, Loss: 0.996757447719574
Step: 917, Loss: 1.0280073881149292
Step: 918, Loss: 0.965507447719574
Step: 919, Loss: 1.0068020820617676
Output File: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/HHMAFM/2024-10-19/029_Oct-19-2024_04:25_PM/training_loss_curve.png
Training loss curve saved to /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/HHMAFM/2024-10-19/029_Oct-19-2024_04:25_PM/training_loss_curve.png
Total Completion Time: 100.46 minutes. (1.67 hours) 
