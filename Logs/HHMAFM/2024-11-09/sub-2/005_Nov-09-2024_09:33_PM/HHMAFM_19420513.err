Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/home/rgg2706/Multimodal-Sentiment-Analysis/Models/HHMAFM/src/train.py", line 78, in <module>
    ins.run()
  File "/home/rgg2706/Multimodal-Sentiment-Analysis/Models/HHMAFM/src/instructor.py", line 143, in run
    outputs = self.model(roberta_text_features, roberta_topic_features, 
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/.autofs/tools/spack/var/spack/environments/default-nlp-x86_64-24072401/.spack-env/view/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/.autofs/tools/spack/var/spack/environments/default-nlp-x86_64-24072401/.spack-env/view/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rgg2706/Multimodal-Sentiment-Analysis/Models/HHMAFM/src/models/mfcchfusion.py", line 104, in forward
    text_self_attended = self.self_attention_text(roberta_text_features.unsqueeze(1)).squeeze(1)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/.autofs/tools/spack/var/spack/environments/default-nlp-x86_64-24072401/.spack-env/view/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/.autofs/tools/spack/var/spack/environments/default-nlp-x86_64-24072401/.spack-env/view/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rgg2706/Multimodal-Sentiment-Analysis/Models/HHMAFM/src/layers/attention.py", line 137, in forward
    return super(SelfAttention, self).forward(k, q)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/rgg2706/Multimodal-Sentiment-Analysis/Models/HHMAFM/src/layers/attention.py", line 43, in forward
    print(f"Batch size: {batch_size}, Sequence length: {seq_len}, Embed dim: {self.embed_dim}")
                                                        ^^^^^^^
UnboundLocalError: cannot access local variable 'seq_len' where it is not associated with a value
