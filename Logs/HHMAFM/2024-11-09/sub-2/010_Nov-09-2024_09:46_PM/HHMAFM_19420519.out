SLURM Job ID: 19420519
Number of GPUs available: 1
Logs directory: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/HHMAFM/2024-11-09/sub-2/010_Nov-09-2024_09:46_PM
> training arguments:
>>> rand_seed: 8
>>> model_name: mfcchfusion
>>> dataset: mvsa-mts-1000
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> initializer: <function xavier_uniform_ at 0x7f5b17409580>
>>> learning_rate: 0.0001
>>> dropout_rate: 0.5
>>> num_epoch: 10
>>> batch_size: 64
>>> log_step: 60
>>> max_seq_len: 64
>>> polarities_dim: 3
>>> clip_grad: 5.0
>>> path_image: ./Datasets/MVSA-MTS/images-indexed
>>> crop_size: 224
>>> n_head: 8
>>> hidden_dim: 1024
>>> num_classes: 3
>>> log_dir: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/HHMAFM/2024-11-09/sub-2/010_Nov-09-2024_09:46_PM
>>> model_class: <class 'models.mfcchfusion.MFCCHFUSION'>
Preparing mvsa-mts-1000 dataset...
-------------- Loading Datasets/MVSA-MTS/mvsa-mts-1000/train.tsv ---------------
Time taken to load Datasets/MVSA-MTS/mvsa-mts-1000/train.tsv: 2.99 seconds (0.05 minutes)
The number of problematic samples: 69
-------------- Loading Datasets/MVSA-MTS/mvsa-mts-1000/val.tsv ---------------
Time taken to load Datasets/MVSA-MTS/mvsa-mts-1000/val.tsv: 0.36 seconds (0.01 minutes)
The number of problematic samples: 10
-------------- Loading Datasets/MVSA-MTS/mvsa-mts-1000/test.tsv ---------------
Time taken to load Datasets/MVSA-MTS/mvsa-mts-1000/test.tsv: 0.37 seconds (0.01 minutes)
The number of problematic samples: 6
Total Training Samples: 1000
Number of Training Samples: 800
Number of Development Samples: 100
Number of Test Samples: 100
Number of unique sentiment classes: 3
Building model
Initialized MFCCHFUSION with dimensions:
text_dim: 768
resnet_dim: 2048
densenet_dim: 1024
n_trainable_params: 360217603, n_nontrainable_params: 0
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 0
text_proj shape: torch.Size([64, 1024])
resnet_proj shape: torch.Size([64, 1024])
topic_proj shape: torch.Size([64, 1024])
densenet_proj shape: torch.Size([64, 1024])
Batch 0 completed in 1.74 seconds (0.03 minutes)
New best dev_f1: 0.197183 (previous best: 0.000000)
loss: 1.100072, dev_acc: 42.00% (0.420000), dev_f1: 19.72% (0.197183), test_acc: 48.00% (0.480000), test_f1: 21.62% (0.216216)
Epoch 0 completed in 10.28 seconds (0.17 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 1
Batch 0 completed in 0.60 seconds (0.01 minutes)
loss: 0.969331, dev_acc: 42.00% (0.420000), dev_f1: 19.72% (0.197183), test_acc: 48.00% (0.480000), test_f1: 21.62% (0.216216)
Epoch 1 completed in 8.68 seconds (0.14 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 2
Batch 0 completed in 0.60 seconds (0.01 minutes)
New best dev_f1: 0.232520 (previous best: 0.197183)
loss: 0.853568, dev_acc: 44.00% (0.440000), dev_f1: 23.25% (0.232520), test_acc: 49.00% (0.490000), test_f1: 27.33% (0.273276)
Epoch 2 completed in 8.59 seconds (0.14 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 3
Batch 0 completed in 0.60 seconds (0.01 minutes)
New best dev_f1: 0.389814 (previous best: 0.232520)
loss: 0.700264, dev_acc: 45.00% (0.450000), dev_f1: 38.98% (0.389814), test_acc: 43.00% (0.430000), test_f1: 35.70% (0.356985)
Epoch 3 completed in 8.58 seconds (0.14 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 4
Batch 0 completed in 0.61 seconds (0.01 minutes)
loss: 0.665318, dev_acc: 46.00% (0.460000), dev_f1: 36.48% (0.364796), test_acc: 38.00% (0.380000), test_f1: 32.70% (0.327004)
Epoch 4 completed in 8.54 seconds (0.14 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 5
Batch 0 completed in 0.60 seconds (0.01 minutes)
New best dev_f1: 0.455301 (previous best: 0.389814)
loss: 0.630144, dev_acc: 47.00% (0.470000), dev_f1: 45.53% (0.455301), test_acc: 44.00% (0.440000), test_f1: 41.26% (0.412572)
Epoch 5 completed in 8.50 seconds (0.14 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 6
Batch 0 completed in 0.61 seconds (0.01 minutes)
loss: 0.623458, dev_acc: 37.00% (0.370000), dev_f1: 28.99% (0.289893), test_acc: 41.00% (0.410000), test_f1: 31.58% (0.315779)
Epoch 6 completed in 8.61 seconds (0.14 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 7
Batch 0 completed in 0.61 seconds (0.01 minutes)
loss: 0.536361, dev_acc: 33.00% (0.330000), dev_f1: 29.62% (0.296162), test_acc: 40.00% (0.400000), test_f1: 34.55% (0.345538)
Epoch 7 completed in 8.62 seconds (0.14 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 8
Batch 0 completed in 0.61 seconds (0.01 minutes)
loss: 0.305148, dev_acc: 46.00% (0.460000), dev_f1: 39.59% (0.395875), test_acc: 46.00% (0.460000), test_f1: 33.46% (0.334643)
Epoch 8 completed in 8.51 seconds (0.14 minutes)
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch: 9
Batch 0 completed in 0.61 seconds (0.01 minutes)
loss: 0.313545, dev_acc: 46.00% (0.460000), dev_f1: 37.32% (0.373210), test_acc: 48.00% (0.480000), test_f1: 34.87% (0.348741)
Epoch 9 completed in 8.53 seconds (0.14 minutes)
RESULT: Max Dev F1: 0.455301, Max Test F1: 0.412572
Reading TensorBoard loss at each epoch:
Available tags: {'images': [], 'audio': [], 'histograms': [], 'scalars': ['Loss/train', 'Loss/val'], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}
Step: 1, Loss: 1.1000720262527466
Step: 2, Loss: 0.9346485733985901
Step: 3, Loss: 2.0732100009918213
Step: 4, Loss: 2.9305949211120605
Step: 5, Loss: 1.5371423959732056
Step: 6, Loss: 1.370755910873413
Step: 7, Loss: 1.2244118452072144
Step: 8, Loss: 0.9592146873474121
Step: 9, Loss: 0.8911821246147156
Step: 10, Loss: 1.0885307788848877
Step: 11, Loss: 0.9894568920135498
Step: 12, Loss: 1.1259843111038208
Step: 13, Loss: 1.0621337890625
Step: 14, Loss: 0.9693306684494019
Step: 15, Loss: 1.0110366344451904
Step: 16, Loss: 0.9440423846244812
Step: 17, Loss: 1.0351688861846924
Step: 18, Loss: 0.9700465798377991
Step: 19, Loss: 0.9846004247665405
Step: 20, Loss: 0.9567618370056152
Step: 21, Loss: 0.9953896999359131
Step: 22, Loss: 0.9699426293373108
Step: 23, Loss: 0.9522630572319031
Step: 24, Loss: 0.9643351435661316
Step: 25, Loss: 0.9376426339149475
Step: 26, Loss: 0.7557246685028076
Step: 27, Loss: 0.8535684943199158
Step: 28, Loss: 0.8118896484375
Step: 29, Loss: 1.0082623958587646
Step: 30, Loss: 0.8718878030776978
Step: 31, Loss: 0.8444381356239319
Step: 32, Loss: 0.8474733233451843
Step: 33, Loss: 0.8406270146369934
Step: 34, Loss: 0.7363050580024719
Step: 35, Loss: 0.8749306797981262
Step: 36, Loss: 1.0139086246490479
Step: 37, Loss: 0.7629401683807373
Step: 38, Loss: 0.7703192234039307
Step: 39, Loss: 0.961679995059967
Step: 40, Loss: 0.7002644538879395
Step: 41, Loss: 0.6774551868438721
Step: 42, Loss: 0.6619868278503418
Step: 43, Loss: 0.6940233111381531
Step: 44, Loss: 0.7205926775932312
Step: 45, Loss: 0.5932736396789551
Step: 46, Loss: 0.7174570560455322
Step: 47, Loss: 0.5830668807029724
Step: 48, Loss: 0.6711529493331909
Step: 49, Loss: 0.7217471599578857
Step: 50, Loss: 0.5628353357315063
Step: 51, Loss: 0.6821022629737854
Step: 52, Loss: 0.5507241487503052
Step: 53, Loss: 0.6653183698654175
Step: 54, Loss: 0.5186748504638672
Step: 55, Loss: 0.6565696597099304
Step: 56, Loss: 1.0794570446014404
Step: 57, Loss: 0.6227882504463196
Step: 58, Loss: 1.6779801845550537
Step: 59, Loss: 0.95447838306427
Step: 60, Loss: 0.6133853793144226
Step: 61, Loss: 0.6509292125701904
Step: 62, Loss: 0.9237510561943054
Step: 63, Loss: 0.7232035994529724
Step: 64, Loss: 0.622269332408905
Step: 65, Loss: 0.8818106055259705
Step: 66, Loss: 0.6301442980766296
Step: 67, Loss: 0.5771373510360718
Step: 68, Loss: 0.6347129940986633
Step: 69, Loss: 0.490284264087677
Step: 70, Loss: 0.3913758397102356
Step: 71, Loss: 0.8843410611152649
Step: 72, Loss: 0.43051692843437195
Step: 73, Loss: 0.5954983234405518
Step: 74, Loss: 0.3027850389480591
Step: 75, Loss: 0.596102774143219
Step: 76, Loss: 0.26021263003349304
Step: 77, Loss: 0.48383209109306335
Step: 78, Loss: 0.7643049359321594
Step: 79, Loss: 0.623457670211792
Step: 80, Loss: 0.3999483287334442
Step: 81, Loss: 0.38808390498161316
Step: 82, Loss: 0.43224892020225525
Step: 83, Loss: 0.36310485005378723
Step: 84, Loss: 0.3721756339073181
Step: 85, Loss: 0.3385079503059387
Step: 86, Loss: 0.7513343095779419
Step: 87, Loss: 0.34163418412208557
Step: 88, Loss: 0.637497067451477
Step: 89, Loss: 0.6832716464996338
Step: 90, Loss: 0.4531362056732178
Step: 91, Loss: 0.4714563488960266
Step: 92, Loss: 0.5363609194755554
Step: 93, Loss: 0.5012233853340149
Step: 94, Loss: 0.46643099188804626
Step: 95, Loss: 0.5022668838500977
Step: 96, Loss: 0.41003087162971497
Step: 97, Loss: 0.3213323652744293
Step: 98, Loss: 0.5095388293266296
Step: 99, Loss: 0.5824878811836243
Step: 100, Loss: 0.3074001967906952
Step: 101, Loss: 0.41977131366729736
Step: 102, Loss: 0.4394517242908478
Step: 103, Loss: 0.22389435768127441
Step: 104, Loss: 0.497888445854187
Step: 105, Loss: 0.30514758825302124
Step: 106, Loss: 0.3615838289260864
Step: 107, Loss: 0.1908939927816391
Step: 108, Loss: 0.12528133392333984
Step: 109, Loss: 0.27329060435295105
Step: 110, Loss: 0.305929034948349
Step: 111, Loss: 0.3733055591583252
Step: 112, Loss: 0.31751444935798645
Step: 113, Loss: 0.22257377207279205
Step: 114, Loss: 0.32814064621925354
Step: 115, Loss: 0.326884388923645
Step: 116, Loss: 0.29903295636177063
Step: 117, Loss: 0.19262605905532837
Step: 118, Loss: 0.3135448694229126
Step: 119, Loss: 0.2048090696334839
Step: 120, Loss: 0.17323799431324005
Step: 121, Loss: 0.25898513197898865
Step: 122, Loss: 0.3459330201148987
Step: 123, Loss: 0.32549989223480225
Step: 124, Loss: 0.4117889404296875
Step: 125, Loss: 0.25552377104759216
Step: 126, Loss: 0.4404867887496948
Step: 127, Loss: 0.5603818893432617
Step: 128, Loss: 0.6193625330924988
Step: 129, Loss: 0.17393867671489716
Step: 130, Loss: 0.3414350152015686
Output File: /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/HHMAFM/2024-11-09/sub-2/010_Nov-09-2024_09:46_PM/trainval_loss_curves.png
Training and validation loss curves saved to /home/rgg2706/Multimodal-Sentiment-Analysis/Logs/HHMAFM/2024-11-09/sub-2/010_Nov-09-2024_09:46_PM/trainval_loss_curves.png
Total Completion Time: 1.79 minutes. (0.03 hours) 
