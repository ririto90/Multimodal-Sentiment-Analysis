PyTorch version: 2.1.1
CUDA available: True
CUDA version: 12.3
True
1
0
NVIDIA A100-PCIE-40GB
************no tfn layer************
> training arguments:
>>> rand_seed: 25
>>> model_name: mmfusion
>>> dataset: mvsa-m
>>> optimizer: <class 'torch.optim.adam.Adam'>
>>> initializer: <function xavier_uniform_ at 0x7f3facb09e40>
>>> learning_rate: 0.001
>>> dropout_rate: 0.5
>>> num_epoch: 100
>>> batch_size: 64
>>> log_step: 50
>>> logdir: log
>>> embed_dim: 100
>>> hidden_dim: 100
>>> max_seq_len: 24
>>> polarities_dim: 3
>>> hops: 3
>>> att_file: ./att_file/
>>> pred_file: ./pred_file/
>>> clip_grad: 5.0
>>> path_image: ../../Datasets/MVSA/mvsa_images
>>> crop_size: 224
>>> fine_tune_cnn: False
>>> att_mode: vis_concat_attimg_gate
>>> resnet_root: ./resnet
>>> checkpoint: ./checkpoint/
>>> load_check_point: False
>>> load_opt: False
>>> tfn: False
>>> model_class: <class 'models.mmfusion.MMFUSION'>
>>> inputs_cols: ['text_left_indicator', 'text_right_indicator', 'aspect_indices']
>>> device: cuda
Before model allocation:
Allocated: 0.00 MB
Cached:    0.00 MB
preparing mvsa-m dataset...
loading word vectors...
building embedding_matrix: 100_mvsa-m_embedding_matrix.dat
--------------./datasets/mvsa-m/train.txt---------------
image has problem!
image has problem!
image has problem!
the number of problematic samples: 3
--------------./datasets/mvsa-m/dev.txt---------------
image has problem!
the number of problematic samples: 1
--------------./datasets/mvsa-m/test.txt---------------
the number of problematic samples: 0
building model
Before absa_dataset allocation:
Allocated: 230.28 MB
Cached:    244.00 MB
After model allocation:
Allocated: 254.87 MB
Cached:    268.00 MB
DataParallel(
  (module): MMFUSION(
    (embed): Embedding(51574, 100)
    (lstm_aspect): DynamicLSTM(
      (RNN): LSTM(100, 100, batch_first=True)
    )
    (lstm_l): DynamicLSTM(
      (RNN): LSTM(100, 100, batch_first=True)
    )
    (lstm_r): DynamicLSTM(
      (RNN): LSTM(100, 100, batch_first=True)
    )
    (attention_l): Attention2(
      (proj): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (attention_r): Attention2(
      (proj): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (visaspect_att_l): MMAttention(
      (proj): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (visaspect_att_r): MMAttention(
      (proj): Linear(in_features=100, out_features=100, bias=True)
      (dropout): Dropout(p=0.5, inplace=False)
    )
    (ltext2hidden): Linear(in_features=100, out_features=100, bias=True)
    (laspect2hidden): Linear(in_features=100, out_features=100, bias=True)
    (rtext2hidden): Linear(in_features=100, out_features=100, bias=True)
    (raspect2hidden): Linear(in_features=100, out_features=100, bias=True)
    (dropout): Dropout(p=0.5, inplace=False)
    (aspect2text): Linear(in_features=100, out_features=100, bias=True)
    (vismap2text): Linear(in_features=2048, out_features=100, bias=True)
    (vis2text): Linear(in_features=2048, out_features=100, bias=True)
    (gate): Linear(in_features=2448, out_features=100, bias=True)
    (madality_attetion): Linear(in_features=100, out_features=1, bias=True)
    (text2hiddenvis): Linear(in_features=400, out_features=100, bias=True)
    (vis2hiddenvis): Linear(in_features=100, out_features=100, bias=True)
    (dense_6): Linear(in_features=600, out_features=3, bias=True)
  )
)
n_trainable_params: 1200104, n_nontrainable_params: 5157400
parameters only include text parts without word embeddings
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  0
loss: 0.9571, acc: 0.5071, dev_acc: 0.5250, test_acc: 0.5036
dev_f1: 0.2887, test_f1: 0.2773
loss: 0.8823, acc: 0.5129, dev_acc: 0.5253, test_acc: 0.5082
dev_f1: 0.3006, test_f1: 0.2917
loss: 0.8063, acc: 0.5342, dev_acc: 0.5413, test_acc: 0.5224
dev_f1: 0.3418, test_f1: 0.3371
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  1
loss: 0.8809, acc: 0.5512, dev_acc: 0.5365, test_acc: 0.5347
dev_f1: 0.3859, test_f1: 0.4026
loss: 0.8904, acc: 0.5600, dev_acc: 0.5253, test_acc: 0.5401
dev_f1: 0.3918, test_f1: 0.4145
loss: 0.8550, acc: 0.5608, dev_acc: 0.5321, test_acc: 0.5426
dev_f1: 0.3717, test_f1: 0.3979
loss: 0.8863, acc: 0.5681, dev_acc: 0.5378, test_acc: 0.5462
dev_f1: 0.3901, test_f1: 0.4182
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  2
loss: 0.9315, acc: 0.5668, dev_acc: 0.5380, test_acc: 0.5464
dev_f1: 0.3742, test_f1: 0.3967
loss: 0.9411, acc: 0.5750, dev_acc: 0.5452, test_acc: 0.5436
dev_f1: 0.4060, test_f1: 0.4183
loss: 0.7691, acc: 0.5760, dev_acc: 0.5383, test_acc: 0.5436
dev_f1: 0.4044, test_f1: 0.4128
loss: 0.9397, acc: 0.5808, dev_acc: 0.5429, test_acc: 0.5490
dev_f1: 0.3809, test_f1: 0.3957
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  3
loss: 0.8028, acc: 0.5866, dev_acc: 0.5360, test_acc: 0.5431
dev_f1: 0.3999, test_f1: 0.4252
loss: 0.9390, acc: 0.5938, dev_acc: 0.5431, test_acc: 0.5500
dev_f1: 0.4247, test_f1: 0.4347
loss: 0.8877, acc: 0.5946, dev_acc: 0.5311, test_acc: 0.5508
dev_f1: 0.3921, test_f1: 0.4251
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  4
loss: 0.7522, acc: 0.6029, dev_acc: 0.5357, test_acc: 0.5526
dev_f1: 0.3978, test_f1: 0.4211
loss: 0.7743, acc: 0.6044, dev_acc: 0.5227, test_acc: 0.5436
dev_f1: 0.3903, test_f1: 0.4178
loss: 0.8122, acc: 0.6180, dev_acc: 0.5258, test_acc: 0.5334
dev_f1: 0.4152, test_f1: 0.4253
loss: 0.7019, acc: 0.6190, dev_acc: 0.5393, test_acc: 0.5480
dev_f1: 0.4182, test_f1: 0.4255
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  5
loss: 0.7289, acc: 0.5966, dev_acc: 0.5321, test_acc: 0.5265
dev_f1: 0.3915, test_f1: 0.3930
loss: 0.8184, acc: 0.6207, dev_acc: 0.5089, test_acc: 0.5362
dev_f1: 0.4186, test_f1: 0.4353
loss: 0.6992, acc: 0.6240, dev_acc: 0.5084, test_acc: 0.5253
dev_f1: 0.4170, test_f1: 0.4351
loss: 0.7906, acc: 0.6225, dev_acc: 0.5288, test_acc: 0.5390
dev_f1: 0.3819, test_f1: 0.3988
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  6
loss: 0.7380, acc: 0.6531, dev_acc: 0.4977, test_acc: 0.5237
dev_f1: 0.4199, test_f1: 0.4341
loss: 0.8062, acc: 0.6521, dev_acc: 0.5268, test_acc: 0.5332
dev_f1: 0.4087, test_f1: 0.4193
loss: 0.7053, acc: 0.6595, dev_acc: 0.5166, test_acc: 0.5327
dev_f1: 0.4149, test_f1: 0.4340
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  7
loss: 0.6422, acc: 0.6782, dev_acc: 0.5212, test_acc: 0.5314
dev_f1: 0.4242, test_f1: 0.4391
loss: 0.7083, acc: 0.6810, dev_acc: 0.4898, test_acc: 0.5028
dev_f1: 0.4209, test_f1: 0.4336
loss: 0.7282, acc: 0.6850, dev_acc: 0.5176, test_acc: 0.5237
dev_f1: 0.4011, test_f1: 0.4092
loss: 0.6334, acc: 0.7080, dev_acc: 0.5166, test_acc: 0.5273
dev_f1: 0.4198, test_f1: 0.4389
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  8
loss: 0.7551, acc: 0.6986, dev_acc: 0.4890, test_acc: 0.4972
dev_f1: 0.4151, test_f1: 0.4172
loss: 0.7638, acc: 0.7313, dev_acc: 0.5026, test_acc: 0.5151
dev_f1: 0.4088, test_f1: 0.4331
loss: 0.6351, acc: 0.7207, dev_acc: 0.4997, test_acc: 0.5125
dev_f1: 0.4011, test_f1: 0.4170
loss: 0.5907, acc: 0.7432, dev_acc: 0.5056, test_acc: 0.5107
dev_f1: 0.4080, test_f1: 0.4257
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  9
loss: 0.6294, acc: 0.7420, dev_acc: 0.5125, test_acc: 0.5168
dev_f1: 0.4039, test_f1: 0.4243
loss: 0.6039, acc: 0.7619, dev_acc: 0.4929, test_acc: 0.5069
dev_f1: 0.3992, test_f1: 0.4200
loss: 0.6168, acc: 0.7706, dev_acc: 0.4967, test_acc: 0.5102
dev_f1: 0.4066, test_f1: 0.4321
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  10
loss: 0.4927, acc: 0.7895, dev_acc: 0.5046, test_acc: 0.5158
dev_f1: 0.4101, test_f1: 0.4300
loss: 0.4942, acc: 0.7832, dev_acc: 0.5130, test_acc: 0.5263
dev_f1: 0.3881, test_f1: 0.4058
loss: 0.5220, acc: 0.8024, dev_acc: 0.5066, test_acc: 0.5130
dev_f1: 0.4130, test_f1: 0.4318
loss: 0.5294, acc: 0.8139, dev_acc: 0.4906, test_acc: 0.5166
dev_f1: 0.4036, test_f1: 0.4366
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  11
loss: 0.5559, acc: 0.8237, dev_acc: 0.5077, test_acc: 0.5145
dev_f1: 0.4153, test_f1: 0.4332
loss: 0.4450, acc: 0.8324, dev_acc: 0.4959, test_acc: 0.5128
dev_f1: 0.4029, test_f1: 0.4372
loss: 0.5606, acc: 0.8389, dev_acc: 0.4801, test_acc: 0.5051
dev_f1: 0.4093, test_f1: 0.4412
loss: 0.6064, acc: 0.8407, dev_acc: 0.4982, test_acc: 0.5038
dev_f1: 0.4054, test_f1: 0.4186
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  12
loss: 0.5353, acc: 0.8573, dev_acc: 0.5048, test_acc: 0.5166
dev_f1: 0.4049, test_f1: 0.4341
loss: 0.4128, acc: 0.8558, dev_acc: 0.4875, test_acc: 0.5026
dev_f1: 0.4085, test_f1: 0.4282
loss: 0.5049, acc: 0.8681, dev_acc: 0.4776, test_acc: 0.4997
dev_f1: 0.3856, test_f1: 0.4230
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  13
loss: 0.3370, acc: 0.8639, dev_acc: 0.5115, test_acc: 0.5189
dev_f1: 0.3999, test_f1: 0.4274
loss: 0.3293, acc: 0.8821, dev_acc: 0.4827, test_acc: 0.4995
dev_f1: 0.3975, test_f1: 0.4259
loss: 0.3731, acc: 0.8815, dev_acc: 0.5003, test_acc: 0.5122
dev_f1: 0.3908, test_f1: 0.4082
loss: 0.3249, acc: 0.8868, dev_acc: 0.4913, test_acc: 0.5041
dev_f1: 0.3895, test_f1: 0.4186
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  14
loss: 0.4665, acc: 0.9016, dev_acc: 0.4849, test_acc: 0.5023
dev_f1: 0.4029, test_f1: 0.4247
loss: 0.3088, acc: 0.9071, dev_acc: 0.4921, test_acc: 0.5036
dev_f1: 0.4008, test_f1: 0.4287
loss: 0.3647, acc: 0.8939, dev_acc: 0.4913, test_acc: 0.5018
dev_f1: 0.3895, test_f1: 0.3964
loss: 0.5506, acc: 0.9060, dev_acc: 0.4870, test_acc: 0.5140
dev_f1: 0.3988, test_f1: 0.4362
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  15
loss: 0.2374, acc: 0.9192, dev_acc: 0.4867, test_acc: 0.5168
dev_f1: 0.3933, test_f1: 0.4408
loss: 0.3775, acc: 0.9158, dev_acc: 0.4990, test_acc: 0.5059
dev_f1: 0.4054, test_f1: 0.4209
loss: 0.2616, acc: 0.9173, dev_acc: 0.4842, test_acc: 0.4908
dev_f1: 0.4006, test_f1: 0.4104
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  16
loss: 0.2850, acc: 0.9060, dev_acc: 0.5061, test_acc: 0.5268
dev_f1: 0.3888, test_f1: 0.4235
loss: 0.3596, acc: 0.9319, dev_acc: 0.4816, test_acc: 0.5003
dev_f1: 0.3931, test_f1: 0.4177
loss: 0.3349, acc: 0.9313, dev_acc: 0.4814, test_acc: 0.4888
dev_f1: 0.3996, test_f1: 0.4170
loss: 0.2914, acc: 0.9276, dev_acc: 0.4992, test_acc: 0.5168
dev_f1: 0.3956, test_f1: 0.4316
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  17
loss: 0.1624, acc: 0.9381, dev_acc: 0.4796, test_acc: 0.4990
dev_f1: 0.4039, test_f1: 0.4266
loss: 0.2724, acc: 0.9373, dev_acc: 0.4735, test_acc: 0.4944
dev_f1: 0.3855, test_f1: 0.4129
loss: 0.2308, acc: 0.9368, dev_acc: 0.4750, test_acc: 0.4944
dev_f1: 0.3935, test_f1: 0.4163
loss: 0.2316, acc: 0.9399, dev_acc: 0.5036, test_acc: 0.5092
dev_f1: 0.3971, test_f1: 0.4159
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  18
loss: 0.2266, acc: 0.9490, dev_acc: 0.5023, test_acc: 0.5179
dev_f1: 0.4061, test_f1: 0.4392
loss: 0.2728, acc: 0.9451, dev_acc: 0.4827, test_acc: 0.4918
dev_f1: 0.3820, test_f1: 0.3980
loss: 0.2127, acc: 0.9523, dev_acc: 0.4804, test_acc: 0.4967
dev_f1: 0.3927, test_f1: 0.4203
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  19
loss: 0.0807, acc: 0.9530, dev_acc: 0.4939, test_acc: 0.4985
dev_f1: 0.3940, test_f1: 0.4112
loss: 0.0938, acc: 0.9571, dev_acc: 0.4957, test_acc: 0.5092
dev_f1: 0.3940, test_f1: 0.4124
loss: 0.2809, acc: 0.9557, dev_acc: 0.5046, test_acc: 0.5023
dev_f1: 0.3979, test_f1: 0.4101
loss: 0.1309, acc: 0.9605, dev_acc: 0.4883, test_acc: 0.5031
dev_f1: 0.4037, test_f1: 0.4267
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
epoch:  20
loss: 0.0999, acc: 0.9565, dev_acc: 0.4753, test_acc: 0.4967
dev_f1: 0.4040, test_f1: 0.4283
